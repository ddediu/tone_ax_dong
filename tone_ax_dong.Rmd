---
title: "Using psychometrics and Signal Detection Theory to investigate tone perception in 490 native speakers of *Kam* (*Dong*)"
subtitle: "Supplementary Materials (full analysis report)"
author: Dan Dediu [ddediu@gmail.com], Luchang Wang, Patrick C. M. Wong, Manxiang Wu
date: "`r date()`"
output:
  html_document: 
    highlight: textmate
    toc: yes
    toc_depth: 6
    toc_float: no
    theme: cerulean
    number_sections: yes
  github_document:
    html_preview: true
    toc: yes
    toc_depth: 6
    dev: jpeg
    fig_width: 11
    fig_height: 6
editor_options:
  chunk_output_type: console
bibliography: bibliography.bib
csl: apa-6th-edition.csl
---

<style>
/* Whole document: */
body{
  font-family: Times New Roman;
  font-size: 10pt;
}

/* Headers */
h1{
  font-size: 16pt;
  color: MidnightBlue;
}
h2{
  font-size: 15pt;
  color: DarkBlue;
}
h3{
  font-size: 14pt;
  color: Blue;
}
h4{
  font-size: 13pt;
  color: MediumSlateBlue;
}
h5{
  font-size: 12pt;
  color: DodgerBlue;
}
h5{
  font-size: 11pt;
  color: DeepSkyBlue;
}

/* Captions: */
caption, .caption {
  color: DarkGray;
  font-size: 90%;
  text-align: left
}

/* Captions: */
code, .code, pre, .pre {
  color: SlateGray;
  font-size: 90%;
}

a[hreflang]:before{}
</style>

```{r license, include=FALSE, eval=FALSE}
    Code and data accompanying the paper "Using psychometrics and Signal 
    Detection Theory to investigate tone perception in 490 native speakers of 
    Kam (Dong)".
    Copyright (C) 2019-2023 Dan Dediu

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
```

```{r setup, echo=F, message=F, warning=FALSE}

## Preliminary check if we can, in fact, run the script:
if( !dir.exists("./input_files/") || !all(file.exists("./input_files/demographics.csv", "./input_files/tone_stimuli.csv", "./input_files/tone_task.csv", "./input_files/wm_task.csv")) ) stop("This Rmarkdown script cannot be run as you do not seem to have the primary data in the ./input_files/ directory: please contact the corresponding author (Manxiang Wu <manxiangwu1022@163.com>), as instructed in the paper, for obtaining them.")


## Load needed packages:

# Rmarkdown:
library(knitr);
library(pander);

# System/parallel processing:
library(parallel);

# Data processing:
library(tidyr);
library(dplyr);
library(reshape2);

# PCA/EFA:
library(factoextra);
library(psych);

# IRT:
library(mokken);

# Signal Detection Theory:
library(psycho);

# Path analysis/SEM:
library(lavaan); # fit models
library(lavaanPlot); # plot models
library(piecewiseSEM); # piecewise SEM

# Plotting:
library(ggplot2);
library(ggrepel); # reduce clutter with text
library(gridExtra); # combine multiple plots
library(gplots); # heatmap.2
library(sjPlot); # effect plots
library(viridis); # color-blind-friendly palette

# Regression and mediation analysis:
library(lmerTest); # ML mixed-effects models
library(glmmTMB); # beta regression
library(performance); # multicollinearity
library(DHARMa); # check dispersion
library(mediation); # mediation

# MD5 checsums:
library(tools);


## Knitting options:
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE,                   # default code chunk options
                      fig.width=11, fig.height=6, fig.align="center", comment=NA, # default figure dimensions
                      fig.path="./figures/",                                      # save images to ./figures/
                      dpi=70, dev="jpeg",                                         # please set dpi=300 and comment out dev="jpeg" for high resolution but very big images
                      cache=TRUE, autodep=TRUE);                                  # cache chunks


##
# Force recomputing everything (including the cached intermediate results)
# This is very computationally expensive (we're talking hours) and needs a powerful machine (I'd recommend >=32Gb RAM and 8 CPU cores) running Linux (for multicore processing) 
# but must be done if the input files have changed (e.g., we're using the actual genotype data instead of the randomized one for development).
# Also, should not be done during knitting the Rmarkdown but outside Rmarkdown (e.g., using 'Run All' from RStudio).

# Force computation manually:
FORCE_COMPUTE_ALL <- FALSE;

# Check if the input files have changed since the last run (if any)
if( !FORCE_COMPUTE_ALL ) # if FORCE_COMPUTE_ALL == TRUE there's no point in checking the md5 checksums
{
  if( !file.exists("./input_files/md5_checksums.csv") )
  {
    warning("This seems to be the first time this is run: forcing recomputing everything...\n");
    FORCE_COMPUTE_ALL <- TRUE; # first time we're running it!
  } else
  {
    # Check the md5 checksums of the input files:
    md5_checksums_prev <- read.csv("./input_files/md5_checksums.csv"); # previously cached MD5 checksums
    md5_checksums_now  <- data.frame("file"=setdiff(list.files("./input_files/", all.files=FALSE, full.names=FALSE, recursive=FALSE, include.dirs=FALSE, no.. = TRUE), "md5_checksums.csv"),
                                     "md5"=NA);
    md5_checksums_now$md5 <- tools::md5sum(paste0("./input_files/", md5_checksums_now$file));
    if( length(setdiff(md5_checksums_prev$file, md5_checksums_now$file)) > 0 )
    {
      warning("Seems like some input files were deleted: forcing recomputing everything...\n");
      FORCE_COMPUTE_ALL <- TRUE; 
    } else if( length(setdiff(md5_checksums_now$file, md5_checksums_prev$file)) > 0 )
    {
      warning("Seems like there are some new input files: forcing recomputing everything...\n");
      FORCE_COMPUTE_ALL <- TRUE; 
    } else
    {
      md5_checksums_prev <- merge(md5_checksums_prev, md5_checksums_now, by="file", suffixes=c("_prev", "_new"), all.x=TRUE, all.y=TRUE);
      if( any(is.na(md5_checksums_prev$md5_prev)) || any(is.na(md5_checksums_prev$md5_new)) )
      {
        warning("There seem to be some files with missing md5 checksum info: forcing recomputing everything...\n");
        FORCE_COMPUTE_ALL <- TRUE; 
      } else if( !all(md5_checksums_prev$md5_prev == md5_checksums_prev$md5_new) )
      {
        warning("There seem to be some files with a different md5 checksum info: forcing recomputing everything...\n");
        FORCE_COMPUTE_ALL <- TRUE; 
      }
    }
  }
  
  # MD5 checksums seem fine...
}

### Uncomment if you need to run the script step by step
#FORCE_COMPUTE_ALL <- FALSE;
###

if( FORCE_COMPUTE_ALL )
{
  if( knitr::is_html_output() || knitr::is_latex_output() ) stop("Recomputing everything should not done during knitting: please run the R code separately, e.g., using 'Run All' from RStudio!\n");
  warning("Recomputing everything is **very computationally expensive** (we're talking 3 hours on an AMD Ryzen 3700X) and needs a powerful machine (I'd recommend >=32Gb RAM and 8 CPU cores running Linux) and should be done outside Rmarkdown (e.g., using 'Run All' from RStudio)...");
  unlink("./cached_results", recursive=TRUE);
  warning(paste0("Starting at: ", (start_time_force_compute_all <- Sys.time()), "...\n"));
}
##


# For parallel processing:
n_cores <- ifelse(Sys.info()['sysname'] == "Windows", 1, max(detectCores(all.tests=TRUE, logical=FALSE)-1, 1, na.rm=TRUE)); # try to use multiple cores, if running on a non-Windows system

## Set seed for reproducibility:
set.seed(42);

## For (g)lmer, use more iterations with bobyqa:
glmer_ctrl <- glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=1e8));
glm_ctrl   <- glm.control(maxit=1e3);
lmer_ctrl  <- lmerControl( optimizer="bobyqa", optCtrl=list(maxfun=1e8));
beta_ctrl  <- glmmTMBControl(optCtrl=list(iter.max=1e8));
alpha_level <- 0.05;


## Various folders:
if( !dir.exists("./figures") ) dir.create("./figures", showWarnings=FALSE); # figures are saved here
if( !dir.exists("./cached_results") ) dir.create("./cached_results", showWarnings=FALSE); # cache expensive results here

supp_mats <- TRUE; # supplementary materials, so figure and table numberign starts with S

## Auxiliary functions:

# Figure and Table caption adapted from https://stackoverflow.com/questions/37116632/rmarkdown-html-number-figures: 
outputFormat = opts_knit$get("rmarkdown.pandoc.to"); # determine the output format of the document
if( is.null(outputFormat) ) outputFormat = ""; # probably not run within knittr
capTabNo = 1; capFigNo = 1; # figure and table caption numbering, for HTML do it manually
#Function to add the Table Number
capTab = function(x, supplementary=supp_mats, id=TRUE)
{
  if(outputFormat == 'html'){
    #x = paste0("<div class='caption'>**Table ",ifelse(supplementary,"S",""),capTabNo,".** ",x,"</div>")
    x = paste0("**Table ",ifelse(supplementary,"S",""),capTabNo,".** ",x,"")
    if( id ) x <- paste0(x, "<a id='Tab",ifelse(supplementary,"S",""),capTabNo,"'/>");
    capTabNo <<- capTabNo + 1
  }; x
}
#Function to add the Figure Number
capFig = function(x, show_R_version=TRUE, show_package_versions=NULL, is_map=FALSE, supplementary=supp_mats, id=TRUE)
{
  if(outputFormat == 'html')
  {
    x <- paste0("**Figure ",ifelse(supplementary,"S",""),capFigNo,".** ",x,"");
    if( show_R_version || (!is.null(show_package_versions) && length(show_package_versions) > 0) )
    {
      x <- paste0(x, " Figure generated using ");
      if( show_R_version ) x <- paste0(x, stringr::str_replace(R.version.string, stringr::fixed("R "), "[`R`](https://www.r-project.org/) "));
      if( !is.null(show_package_versions) && length(show_package_versions) > 0 )
      {
        x <- paste0(x, ifelse( show_R_version, " and ", " "));
        x <- paste0(x, ifelse( length(show_package_versions) > 1, "packages ", "package "));
        x <- paste0(x, paste0(vapply(show_package_versions, function(x) paste0("`",x,"`", " (version ", packageVersion(x),")"), character(1)), collapse=", "), ".");
      }
      if( is_map ) x <- paste0(x, " Maps are using public domain data from the [Natural Earth project](https://www.naturalearthdata.com/) as provided by the `R` package `maps`.");
      if( id ) x <- paste0(x, "<a id='Fig",ifelse(supplementary,"S",""),capFigNo,"'/>");
    }
    capFigNo <<- capFigNo + 1;
  }; 
  x;
}

# Prepare variable for being the DV in a beta regression by making sure there's no pure 0.0 and 1.0 values:
prepare_for_beta_regression <- function(x, delta=1e-7){ x[x <= 0.0] <- (0.0 + delta); x[x >= 1.0] <- (1.0 - delta); return (x); }

# Panel function for the pairwise plots:
panel.cor <- function(x, y){
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- cor.test(x, y, method="pearson");
  rho <- cor.test(x, y, method="spearman");
  txt <- sprintf("r=%.2f\n(p=%.4g)\nrho=%.2f\n(p=%.4g)", 
                 r$estimate, r$p.value, rho$estimate, rho$p.value)
  cex.cor <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex=1.2)
}
upper.panel<-function(x, y){
  points(x,y, pch = 21, bg="lightgray", col="gray30")
  abline(lm(y ~ x), col="blue", lwd=2);
}
panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col = "lightgray", ...)
}


# Scientific notation using Markdown conventions (inspired from https://www.r-bloggers.com/2015/03/scientific-notation-for-rlatex/):
scinot <- function(xs, digits=2, pvalue=TRUE)
{
  scinot1 <- function(x)
  {
    if(x == 0.0) return ("0");
    sign <- "";
    if(x < 0)
    {
      sign <- "-";
      x <- -x;
    }
    exponent <- floor(log10(x));
    if(exponent && pvalue && exponent < -3) 
    {
      xx <- round(x / 10^exponent, digits=digits);
      e <- paste0("×10^", round(exponent,0), "^");
    } else 
    {
      xx <- round(x, digits=digits+1);
      e <- "";
    }
    paste0(sign, xx, e);
  }
  vapply(xs, scinot1, character(1));
}

# Define a function to partition an item set into mokken scales - lowerbound from .05 to .60 
# (this tells R to apply aisp to any given data.frame for a set of lowerbounds if you call moscales.for.lowerbounds)
# ^^^^^^^ change .60 value to higher if you need to see how items behave at higher levels of c 
# DD: parallelize it and add handle "Error in aisp(x, lowerbound = lowerbound) : no lowerbound provided"
moscales.for.lowerbounds <- function(x, lowerbounds=seq(from=0.05,to=0.60,by=0.05), ncores=1)
{
  if( ncores == 1 )
  {
    # run the original version:
    ret.value <- NULL;
    for( lowerbound in lowerbounds )
    {
      tmp <- aisp( x,  lowerbound=lowerbound );
      if( is.null(ret.value) )
      {
        ret.value <- data.frame( "Item"=rownames(tmp), "Scales."=tmp[,1] );
      }
      else
      {
        ret.value <- cbind( ret.value, "Scales."=tmp[,1] );
      }
      names(ret.value)[ncol(ret.value)] <- paste("c=",sprintf("%.2f",lowerbound),sep="");
    }
    rownames(ret.value) <- NULL;
    ret.value;
  } else
  {
    # run the parallel version:
    ret.value <- as.data.frame(do.call(cbind, mclapply(lowerbounds, function(lowerbound)
    {
      tmp <- NULL;
      try(tmp <- aisp( x,  lowerbound=lowerbound ), silent=TRUE);
      if( is.null(tmp) ){ return (rep(NA,ncol(x))) } else { return (tmp[,1]) }
    }, mc.cores=ncores)));
    names(ret.value) <- sprintf("c=%.2f",lowerbounds);
    return (ret.value);
  }
}

# Write colored text in the Rmarkdown document (https://bookdown.org/yihui/rmarkdown-cookbook/font-color.html):
colorize_text <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

**Links to the main figures and tables referenced in the main paper:**

- *Figures:*
  + [Figure S1. Distribution of gender.](#FigS1)
  + [Figure S2. Distribution of age.](#FigS2)
  + [Figure S4. Distribution of years of musical traning.](#FigS4)
  + [Figures S5-S6. Distribution of years of education by gender and by location.](#FigS5)
  + [Figure S7. Predictive plot of the linear regression of education on gender, age and their interaction.](#FigS7)
  + [Figures S9-S11. PCA of working memory.](#FigS9)
  + [Figures S12-S14. EFA of working memory.](#FigS12)
  + [Figure S15. CFA of working memory.](#FigS15)
  + [Figure S16. Normalized working memory by gender and age.](#FigS16)
  + [Figure S17. Number of correct responses on the tone task (original dataset).](#FigS17)
  + [Figures S18-S21. Correlation matrix, its histogram, hierarchical clustering, and average correlations for the tone task (original dataset).](#FigS18)
  + [Figure S24. Guttman errors for the tone task (original dataset).](#FigS24)
  + [Figures S25-S27. PCA of the tone task (original dataset).](#FigS25)
  + [Figures S28-S30. EFA of the tone task (original dataset).](#FigS28)
  + [Figures S31-S35. Correlation matrix, its histogram, hierarchical clustering, and average correlations for the tone task (recoded dataset).](#FigS31)
  + [Figures S36-S38. PCA of the tone task (recoded dataset).](#FigS36)
  + [Figures S39-S41. EFA of the tone task (recoded dataset).](#FigS39)
  + [Figures S42-S43. Histograms and correlations heatmaps (with clustering) for pcr on the three datasets.](#FigS42)
  + [Figures S46-S47. Histograms and correlations heatmaps (with clustering) for SDT-deroved measures on the three datasets.](#FigS46)
  + [Figure S55. K-means clustering for the tone task (original dataset).](#FigS55)
  + [Figure S56. % different vs % same for the tone task clusters.](#FigS56)
  + [Figures S57-S58. Correlations heatmaps (with clustering) for all measures of interest.](#FigS57)
  + [Figure S60. Mediation gender → education → working memory.](#FigS60)
  + [Figure S61. Mediation age → education → working memory.](#FigS61)
  + [Figures S62-S64. Multiple regression of pcr (recoded dataset).](#FigS62)
  + [Figure S65. Mediation gender → education → tone (recoded dataset).](#FigS65)
  + [Figure S66. Mediation age → education → tone (recoded dataset).](#FigS66)
  + [Figure S67. Mediation location → education → tone (recoded dataset).](#FigS67)
  + [Figure S68. Mediation education, age, gender → working memory → tone (recoded dataset).](#FigS68)
  + [Figures S70-S77. Corresponding analyses for d' (recoded dataset).](#FigS70)

- *Tables:*
  + [Table S1. The working memory trials.](#TabS1)
  + [Tables S5-S6. Tone task stimuli.](#TabS5)
  + [Table S7. Frequencies of correct responses on the tone task (original dataset).](#TabS7)
  + [Tables S9-S10. Item homogeneities for the tone task (original dataset).](#TabS9)
  + [Table S11. aisp for the tone task (original dataset).](#TabS11)
  + [Table S12. Frequencies of correct responses on the tone task (recoded dataset).](#TabS12)
  + [Tables S13-S14. Item homogeneities for the tone task (recoded dataset).](#TabS13)
  + [Table S15. aisp for the tone task (recoded dataset).](#TabS15)
  + [Tables S16-S17. Item homogeneities for the tone task (reduced dataset).](#TabS16)
  + [Table S19. aisp for the tone task (reduced dataset).](#TabS19)
  + [Table S24. W and non-W item correlations.](#TabS24)
  + [Table S26. Comparisons between measures and datasets.](#TabS26)
  + [Table S27. Examples of response patterns detectable by SDT but not by % correct responses.](#TabS27)


# Introduction

This document (irrespective of its format, most probably `HTML` or `PDF`) resulted from compiling the corresponding `Rmarkdown` script and contains all the results and plots supporting the paper (its **Supplementary Materials**).
The primary data are available upon request from the corresponding author (Manxiang Wu <manxiangwu1022@163.com>), as instructed in the paper, but all the code needed to reproduce this document are available in the *GitHub* repository [https://github.com/ddediu/tone_ax_dong](https://github.com/ddediu/tone_ax_dong).

Please note that this `Rmarkdown` script caches some very expensive computations in the `./cached_results` directory, but it can be manually forced to recompute everything (setting the variable `FORCE_COMPUTE_ALL` to `TRUE`) and it also forces a full recomputation if any of the input files (in the directory `./input_files`) was changed, deleted or added (or if this is the first time this is run).
However, it is highly recommended to **not** run such a full recomputation during the knitting of the `Rmarkdown`, but instead in a "normal" `R` session using, for example, the "Run ▾" → "Run all" menu in `RStudio` (knitting the whole thing seems to generate crashes due to memory issues).
Also, this full recomputation should be done on a powerful machine (with at least 32Gb RAM and a 4 cores CPU) running `Linux` or `macOS` (to fully use multicore parallelism; moreover, this script was not tested on `Windows`) and it may take a while (it takes about 3 hours on an AMD Ryzen 7 3700X with 64Gb RAM), but the subsequent knitting or small changes can be run on a "normal" machine as the expensive results are cached for later use.


## Typographic conventions

This document uses the following font and color conventions:

- *regular text* is rendered as "regular text";
- *emphasis* is represented using *italic text*, **bold text** or ***bold+italic text***;
- *software* or programming concepts (e.g., applications, packages or function names) are represented using `fixed font text`;
- *section heads* use specific font sizes and are numbered;
- *hyperlinks* to sections of this document and to external resources on the web are represented as [link to the Introduction](#introduction) or [link to R project's website](https://www.r-project.org/), and can be clicked to navigate there;
- *notes* are represented as numbered superscripts[^example_note] which can be clicked to go to the note's text;
- *captions* use colored font, are numbered, and are placed below the corresponding figure and above the corresponding table;
- *raw output*, as produced by various `R` functions and expressions, is shown using `fixed font text` in clearly marked boxes;

[^example_note]: This is an example note. Click the symbol at the end of the note to go back to where the note is called in the text.


## Software and hardware info

The full information about the version of `R` [@R2023], the packages and the hardware and software platform used to obtain this document are given in the Section [*Session information*](#session-information) at the end of this document[^technical_note].

[^technical_note]: Please note that some of the models used here are computationally expensive, and even compiling this `Rmarkdown` script might require a relatively powerful machine. To help with this, and to ensure full replicability of our results, we have cached some of these expensive sections in the `cached_results` folder as `XZ`-compressed `RData` files. However, it might happen that versions of some of the packages different from those that we used here might not be fully compatible with the saved `RData` files, resulting in errors compiling this `Rmarkdown` script or errors displaying/plotting the results. In this case, we recommend using the exact same versions of `R` and of the packages that we used (listed in the [*Session information*](#session-information)), or, if not possible, the deletion of the offending `RData` files and the full recompilation of the `Rmarkdown` script (which is smart enough to re-generate only those missing cached results).


```{r load and prepare data, error=FALSE, include=FALSE}
# Check that the working directory contains this RMarkdown document!
if( !file.exists("./tone_ax_dong.Rmd") ) stop("Please make sure the current directory is the directory where this Rmarkdown document resides!")

# Load the data:
d_demo  <- read.csv("./input_files/demographics.csv", na.strings="");
d_tone  <- read.csv("./input_files/tone_task.csv");
d_stims <- read.csv("./input_files/tone_stimuli.csv");
d_wm    <- read.csv("./input_files/wm_task.csv");

# Pre-process the tone data:
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/d_tone.csv.xz") ) # computationally expensive
{
  d_tone$isword2 <- d_tone$syllable2 <- d_tone$isword1 <- d_tone$syllable1 <- NA;
  for(i in 1:nrow(d_tone)) # slow but clear
  {
    stim <- d_tone$stimulus[i];
    if(substring(stim,5,8) == "same")
    {
      # Same syllables:
      ord <- substring(stim,10,10);
      stimdiff <- paste0(substring(stim,1,3), "_diff_pair"); # stimulus to match
      if(ord == "1")
      {
        # Corresponds to 1st syllable of pair 1:
        s <- grep(paste0(stimdiff,"1"), d_stims$stimulus, fixed=TRUE); # locate the stimulus pair
        if( length(s) != 1 ) stop(paste0("Can't locate stimulus '",paste0(stimdiff,"1"),"', corresponding to '",substring(stim, 1, 14),"' (row ",i,") in the stimulus description table!"));
        d_tone$syllable1[i] <- d_tone$syllable2[i] <- d_stims$syllable1[s];
        d_tone$isword1[i]   <- d_tone$isword2[i]   <- d_stims$isword1[s];
      } else if(ord == "2")
      {
        # Corresponds to 2nd syllable of pair 1:
        s <- grep(paste0(stimdiff,"1"), d_stims$stimulus, fixed=TRUE); # locate the stimulus pair
        if( length(s) != 1 ) stop(paste0("Can't locate stimulus '",paste0(stimdiff,"1"),"', corresponding to '",substring(stim, 1, 14),"' (row ",i,") in the stimulus description table!"));
        d_tone$syllable1[i] <- d_tone$syllable2[i] <- d_stims$syllable2[s];
        d_tone$isword1[i]   <- d_tone$isword2[i]   <- d_stims$isword2[s];
      } else if(ord == "3")
      {
        # Corresponds to 1st syllable of pair 2:
        s <- grep(paste0(stimdiff,"2"), d_stims$stimulus, fixed=TRUE); # locate the stimulus pair
        if( length(s) != 1 ) stop(paste0("Can't locate stimulus '",paste0(stimdiff,"2"),"', corresponding to '",substring(stim, 1, 14),"' (row ",i,") in the stimulus description table!"));
        d_tone$syllable1[i] <- d_tone$syllable2[i] <- d_stims$syllable1[s];
        d_tone$isword1[i]   <- d_tone$isword2[i]   <- d_stims$isword1[s];
      } else if(ord == "4")
      {
        # Corresponds to 2nd syllable of pair 2:
        s <- grep(paste0(stimdiff,"2"), d_stims$stimulus, fixed=TRUE); # locate the stimulus pair
        if( length(s) != 1 ) stop(paste0("Can't locate stimulus '",paste0(stimdiff,"2"),"', corresponding to '",substring(stim, 1, 14),"' (row ",i,") in the stimulus description table!"));
        d_tone$syllable1[i] <- d_tone$syllable2[i] <- d_stims$syllable2[s];
        d_tone$isword1[i]   <- d_tone$isword2[i]   <- d_stims$isword2[s];
      } else 
      {
        stop(paste0("Unknown order '",ord,"' (row ",i,")!"));
      }
    } else
    {
      # Different syllables:
      s <- grep(substring(stim, 1, 14), d_stims$stimulus, fixed=TRUE); # locate the stimulus pair
      if( length(s) != 1 ) stop(paste0("Can't locate stimulus '",substring(stim, 1, 14),"' (row ",i,") in the stimulus description table!"));
      if( substring(stim, 16, 21) == substring(d_stims$stimulus[s], 16, 21) )
      {
        # Same order as in the stimulus description:
        d_tone$syllable1[i] <- d_stims$syllable1[s]; d_tone$isword1[i] <- d_stims$isword1[s];
        d_tone$syllable2[i] <- d_stims$syllable2[s]; d_tone$isword2[i] <- d_stims$isword2[s];
      } else
      {
        # The other order:
        d_tone$syllable1[i] <- d_stims$syllable2[s]; d_tone$isword1[i] <- d_stims$isword2[s];
        d_tone$syllable2[i] <- d_stims$syllable1[s]; d_tone$isword2[i] <- d_stims$isword1[s];
      }
    }
  }
    
  write.csv(d_tone, file=xzfile("./cached_results/d_tone.csv.xz", compression=9), row.names=FALSE);
} else
{
  d_tone <- read.csv(xzfile("./cached_results/d_tone.csv.xz"));
}

# Keep only those participants with data for all relevant measures:
d_ppts_orig <- d_demo[ d_demo$ID %in% d_tone$ID &  # data for the tone task
                    d_demo$ID %in% d_wm$ID, ];     # data for the working memory task

# Make sure he levels and contrasts are correctly set:
d_ppts_orig$ID <- factor(d_ppts_orig$ID);
d_ppts_orig$gender <- factor(d_ppts_orig$gender, levels=c("F","M"));
d_ppts_orig$family <- factor(d_ppts_orig$family);

d_ppts <- d_ppts_orig[ d_ppts_orig$hearing_problems == "N" & d_ppts_orig$brain_cognitive_impairment == "N", 
                       c("ID", "age", "gender", "music_years", "education_years", "location", "family", "generation")]; # keep only those participants without hearing of brain/cognitive impairments

# Build the dataframe where we will collect all the data (after pre-processing):
d_all <- d_ppts;

# Save the data for the paper:
write.csv(d_ppts_orig, file="./paper/data/d_ppts_orig.csv", row.names=FALSE);
```


# The data

## The population and language

We collected data from native speakers of a Southern dialect of *Kam*, described in detail in [@wu_grammar_2018], probably Glottolog [sout2741](https://glottolog.org/resource/languoid/id/sout2741), which is characterized by a very complex tone system [see @wu_grammar_2018,p.28-36] with 10 *phonemic* (i.e., contrastive) tones realized as 15 *phonetic* tones (and effected by various tone sandhi rules and language contact-induced ongoing changes).
As expected, WALS assigns a "Complex tone system" to this variety (see "Chapter 13A" for [Language Dong (Southern)](https://wals.info/languoid/lect/wals_code_don)); unfortunately, neither PHOIBLE not LAPSyD seem to contain any information about it.

In total, we collected usable data from `r nrow(d_ppts_orig)` unique participants, from which we further excluded `r sum(d_ppts_orig$hearing_problems == "Y")` participants who reported hearing problems (no participant reported brain or cognitive impairment), leaving a total of `r nrow(d_ppts)` participants in the sample.

Concerning self-declared **gender**, there are `r sprintf("%d (%.1f%%)", sum(d_ppts$gender == "F"), 100*sum(d_ppts$gender == "F")/nrow(d_ppts))` self-declared females and `r sprintf("%d (%.1f%%)", sum(d_ppts$gender == "M"), 100*sum(d_ppts$gender == "M")/nrow(d_ppts))` self-declared males:
```{r fig.width=4, fig.height=4, fig.cap=capFig("Distribution of *gender* in the sample.")}
ggplot(d_ppts, aes(x=gender, fill=gender, color=gender)) + geom_bar(alpha=0.25) + scale_fill_manual(values=c("F"="red", "M"="blue")) + scale_color_manual(values=c("F"="red", "M"="blue")); 
```

At the time of data collection, the **age** of the participants was distributed between `r min(d_ppts$age)` and `r max(d_ppts$age)` years, with a mean of `r round(mean(d_ppts$age),1)` (and median `r round(median(d_ppts$age),1)`) and standard deviation of `r round(sd(d_ppts$age),1)` (and interquartile range, IQR, `r round(IQR(d_ppts$age),1)`):
```{r fig.width=4, fig.height=4, fig.cap=capFig("Distribution of *age* overall (thick black curve) and by *gender* (colored transparent curves) in the sample.")}
ggplot(d_ppts, aes(x=age, fill=gender, color=gender)) + geom_density(alpha=0.25) + geom_density(data=d_ppts, aes(x=age), alpha=0.5, color="black", fill=NA, linewidth=1.0) + scale_fill_manual(values=c("F"="red", "M"="blue")) + scale_color_manual(values=c("F"="red", "M"="blue"));
```

It can be seen that the sample has `r round(sum(d_ppts$gender == "F")/sum(d_ppts$gender == "M"),1)` times more females than males (the *χ*^2^ test against the expected 50%:50% distribution is highly significant: *χ*^2^(`r (tmp <- chisq.test(table(d_ppts$gender), p=c(0.5, 0.5)))$parameter`) = `r round(tmp$statistic,1)`, *p*=`r scinot(tmp$p.value)`), but the ages are distributed in similar ways between the two genderes, with a bi-modal distribution suggesting two age groups: one composed of *adolescents and young adults* (centered around 20 years of age and ranging between the minimum of `r min(d_ppts$age)` and about 30 years old) and the other ("*adults*") centered around late 40s.

```{r results='hide'}
tmp <- table(d_ppts$location, useNA="no");
```
We used the **location** as a proxy for any relevant socio-linguistic dimensions of variation, and we ended up with participants from `r length(tmp)` locations:
```{r}
pander(sort(tmp, decreasing=TRUE));
```
The vast majority of participants comes from two neighboring locations A and B (about 8km apart), speaking very similar dialects of Kam (Manxiang Wu, *pc*); therefore we collapsed the remaining locations into an "other" category. 
Please note that for `r sprintf("%d (%.1f%%)", sum(is.na(d_ppts$location)), 100*sum(is.na(d_ppts$location))/nrow(d_ppts))` participants this information is missing.
```{r}
d_ppts$location[ !is.na(d_ppts$location) & !(d_ppts$location %in% c("B", "A")) ] <- "other"; d_ppts$location <- factor(d_ppts$location, levels=c("other", "A", "B"));
```

We also were able to retrieve some information concerning familial relationships for `r sprintf("%d (%.1f%%)", sum(!is.na(d_ppts$family)), 100*sum(!is.na(d_ppts$family))/nrow(d_ppts))` participants, grouped in `r length(unique(d_ppts$family))` nuclear families across a maximum of 3 generations: generation 0 comprises the youngest members, 1 is their parents' generation, and 2 is that of their grandparent's generation:
```{r fig.width=12, fig.height=4, fig.cap=capFig("Distribution of the participants with information about family by generation: 0=youngest (black), 1=their parents (gray) and 2=their grandparents (light gray). The families are identified with an arbitrary unique numerical ID.")}
barplot(table(d_ppts$generation, d_ppts$family), xlab="Family ID (random numeric identifier)", ylab="Number of family members of each generation");
```
It can be seen that there is only one participant in generation 2, so we collapsed generations 1 and 2, resulting into a binary split into a "young" and an "older" generation.
```{r}
d_ppts$generation <- factor(ifelse(d_ppts$generation == 0, "young", "older"), levels=c("young", "older"));
d_all$generation  <- factor(ifelse(d_all$generation == 0,  "young", "older"), levels=c("young", "older"));

pander(table(d_all$generation));
```


## The covariates

We considered three covariates here (as *age* has already been covered above, we focus on the remaining two).

### Years of musical traning

This variable (*music_years*) is self-declared and is distributed as follows:
```{r fig.width=4, fig.height=4, fig.cap=capFig("Distribution of *music_years* overall (thick black curve) and by *gender* (colored transparent curves) in the sample.")}
ggplot(d_all, aes(x=music_years, fill=gender, color=gender)) + 
  geom_histogram(alpha=0.25) + 
  scale_fill_manual(values=c("F"="red", "M"="blue")) + scale_color_manual(values=c("F"="red", "M"="blue")) + 
  NULL;
```

It is clear that in our sample it does not capture any interesting pattern of inter-individual variation, so we will ignore it in the following analyses.


### Years of formal education

This variable (*education_years*) is self-declared and is distributed as follows:
```{r fig.width=4, fig.height=4, fig.cap=capFig("Distribution of *education_years*by *gender* in the sample.")}
ggplot(d_all, aes(x=education_years, fill=gender, color=gender)) + 
  geom_histogram(alpha=0.25) + 
  scale_fill_manual(values=c("F"="red", "M"="blue")) + scale_color_manual(values=c("F"="red", "M"="blue")) + 
  NULL;
```
```{r fig.width=12, fig.height=4, fig.cap=capFig("Distribution of *education_years* by *location* in the sample. *NA* means that the location information was not available.")}
ggplot(d_all, aes(x=education_years, fill=location, color=location)) + 
  geom_histogram(alpha=0.25) + 
  facet_wrap( . ~ location, ncol=6) +
  NULL;
```

Overall, *gender* makes a significant difference difference (*t*(`r round((tmp <- t.test(education_years ~ gender, data=d_all))$parameter,1)`)=`r round(tmp$statistic,1)`, *p*=`r scinot(tmp$p.value)`; Mann-Whitney *W*=`r round((tmp1 <- wilcox.test(education_years ~ gender, data=d_all))$statistic,1)`, *p*=`r scinot(tmp1$p.value)`), with males having `r round(tmp$estimate["mean in group M"] - tmp$estimate["mean in group F"],2)` years of formal education more than females on average (i.e., `r round(tmp$estimate["mean in group M"],2)` for males vs  `r round(tmp$estimate["mean in group F"],2)` for females).

Focusing on the two locations that comprise the majority of the participants, location A has significantly higher educational levels that location B (*t*(`r round((tmp <- t.test(d_all$education_years[!is.na(d_all$location) & d_all$location == "A"], d_all$education_years[!is.na(d_all$location) & d_all$location == "B"]))$parameter,1)`)=`r round(tmp$statistic,1)`, *p*=`r scinot(tmp$p.value)`; Mann-Whitney *W*=`r round((tmp1 <- wilcox.test(d_all$education_years[!is.na(d_all$location) & d_all$location == "A"], d_all$education_years[!is.na(d_all$location) & d_all$location == "B"]))$statistic,1)`, *p*=`r scinot(tmp1$p.value)`), with the participants from location A having `r round(tmp$estimate["mean of x"] - tmp$estimate["mean of y"],2)` years of formal education more than the participants from B on average (i.e., `r round(tmp$estimate["mean of x"],2)` vs  `r round(tmp$estimate["mean of y"],2)`).

```{r include=FALSE}
m_educ <- lm(education_years ~ gender * age, data=d_all);
(s_m_educ <- summary(m_educ));
par(mfrow=c(2,2));plot(m_educ, ask=FALSE);par(mfrow=c(1,1)); # relatively ok
```
We performed the linear regression of *education_years* on *age*, *gender* and their interaction, and we found that this model behaves well (diagnostic plots not shown), and that it explains adjusted *R*^2^ = `r round(100*s_m_educ$adj.r.squared,1)`% of the variance, and that all terms are significant: *age* has a highly significant negative main effect (*β* = `r round(s_m_educ$coefficients["age","Estimate"],3)`, *p* = `r scinot(s_m_educ$coefficients["age","Pr(>|t|)"])`), *gender* has a very large and highly significant main effect with the males having less years of education than females (*β* = `r round(s_m_educ$coefficients["genderM","Estimate"],3)`, *p* = `r scinot(s_m_educ$coefficients["genderM","Pr(>|t|)"])`), but there is a highly significant interaction between the two (*β* = `r round(s_m_educ$coefficients["genderM:age","Estimate"],3)`, *p* = `r scinot(s_m_educ$coefficients["genderM:age","Pr(>|t|)"])`) that offsets the main negative effect of *gender* into an advantage for males versus the females:
```{r fig.width=4, fig.height=4, fig.cap=capFig("Predictive plot of the linear regression of *education_years* on *gender*, *age* and their interaction.")}
sjPlot::plot_model(m_educ, type="pred", terms=c("age [all]", "gender"));
```


## The measures

### The working memory task

The working memory task consists of 15 trials; in each trial a sequence of colors (each color appears only once in a trial) are shown to the participant and the participant has to reproduce the colors in the correct order, the score representing the number of colors reproduced in the correct position.
For example, trial 1 shows "red", "green" and "blue", and if a participant reproduces "yellow", "green", "red", her score is 1 (from "green").
The trials are the same across participants and vary from 3 to 7 colors, as show below:

| Trial | Length | Color 1 | Color 2 | Color 3 | Color 4 | Color 5 | Color 6 | Color 7 | 
|------:|-------:|---------|---------|---------|---------|---------|---------|---------|
| 1     | 3      | <span style="color:red">**red**</span> | <span style="color:green">**green**</span> | <span style="color:blue">**blue**</span> | | | | |
| 2     | 3      | <span style="color:black">**black**</span> | <span style="color:purple">**purple**</span> | <span style="color:yellow">**yellow**</span> | | | | |
| 3     | 3      | <span style="color:gray">**gray**</span> | <span style="color:green">**green**</span> | <span style="color:black">**black**</span> | | | | |
| 4     | 4      | <span style="color:red">**red**</span> | <span style="color:blue">**blue**</span> | <span style="color:purple">**purple**</span> | <span style="color:gray">**gray**</span> | | | |
| 5     | 4      | <span style="color:yellow">**yellow**</span> | <span style="color:black">**black**</span> | <span style="color:green">**green**</span> | <span style="color:blue">**blue**</span> | | | |
| 6     | 4      | <span style="color:green">**green**</span> | <span style="color:red">**red**</span> | <span style="color:black">**black**</span> | <span style="color:yellow">**yellow**</span> | | | |
| 7     | 5      | <span style="color:blue">**blue**</span> | <span style="color:black">**black**</span> | <span style="color:gray">**gray**</span> | <span style="color:yellow">**yellow**</span> | <span style="color:red">**red**</span> | | |
| 8     | 5      | <span style="color:gray">**gray**</span> | <span style="color:purple">**purple**</span> | <span style="color:yellow">**yellow**</span> | <span style="color:green">**green**</span> | <span style="color:blue">**blue**</span> | | |
| 9     | 5      | <span style="color:black">**black**</span> | <span style="color:red">**red**</span> | <span style="color:blue">**blue**</span> | <span style="color:gray">**gray**</span> | <span style="color:green">**green**</span> | | |
| 10    | 6      | <span style="color:green">**green**</span> | <span style="color:black">**black**</span> | <span style="color:purple">**purple**</span> | <span style="color:blue">**blue**</span> | <span style="color:gray">**gray**</span> | <span style="color:yellow">**yellow**</span> | |
| 11    | 6      | <span style="color:yellow">**yellow**</span> | <span style="color:purple">**purple**</span> | <span style="color:black">**black**</span> | <span style="color:red">**red**</span> | <span style="color:green">**green**</span> | <span style="color:gray">**gray**</span> | |
| 12    | 6      | <span style="color:gray">**gray**</span> | <span style="color:purple">**purple**</span> | <span style="color:blue">**blue**</span> | <span style="color:red">**red**</span> | <span style="color:green">**green**</span> | <span style="color:yellow">**yellow**</span> | |
| 13    | 7      | <span style="color:red">**red**</span> | <span style="color:green">**green**</span> | <span style="color:blue">**blue**</span> | <span style="color:black">**black**</span> | <span style="color:purple">**purple**</span> | <span style="color:yellow">**yellow**</span> | <span style="color:gray">**gray**</span> |
| 14    | 7      | <span style="color:blue">**blue**</span> | <span style="color:gray">**gray**</span> | <span style="color:black">**black**</span> | <span style="color:green">**green**</span> | <span style="color:red">**red**</span> | <span style="color:purple">**purple**</span> | <span style="color:yellow">**yellow**</span> |
| 15    | 7      | <span style="color:yellow">**yellow**</span> | <span style="color:red">**red**</span> | <span style="color:blue">**blue**</span> | <span style="color:green">**green**</span> | <span style="color:gray">**gray**</span> | <span style="color:black">**black**</span> | <span style="color:purple">**purple**</span> |
`r paste0("<div class='caption'>",capTab("The working memory trials showing their length and the color sequence. Each color may appear at most once in any give trial but can repeat across trials. The trials are fixed across all participants."),"</div>")`

```{r}
# Convert ot the long format:
d_wm_long <- tidyr::gather(d_wm[ d_wm$ID %in% d_ppts$ID, 
                                 c("ID", "trial_01", "trial_02", "trial_03", "trial_04", "trial_05", "trial_06", "trial_07", "trial_08", "trial_09", "trial_10", "trial_11", "trial_12", "trial_13", "trial_14", "trial_15")], 
                           trial, score, trial_01:trial_15);
d_wm_long <- merge(d_wm_long, d_ppts, by="ID", all.x=TRUE, all.y=FALSE); # add demographic info
```

The distribution of scores across participants by trial is:
```{r fig.width=2*5, fig.height=2*3, fig.cap=capFig("Distribution of the *working memory scores* across the trials by gender (showing the actual counts with the females stacked on top of the males).")}
ggplot(d_wm_long, aes(x=score, fill=gender)) + geom_bar(position="stack", alpha=0.75) + scale_fill_manual(values=c("F"="red", "M"="blue")) + 
  facet_wrap(. ~ trial, ncol=5);
```

*gender* makes a significant difference difference (*t*(`r round((tmp <- t.test(score ~ gender, data=d_wm_long))$parameter,1)`)=`r round(tmp$statistic,1)`, *p*=`r scinot(tmp$p.value)`; Mann-Whitney *W*=`r round((tmp1 <- wilcox.test(score ~ gender, data=d_wm_long))$statistic,1)`, *p*=`r scinot(tmp1$p.value)`), with males scoring slightly higher across trials than females by `r round(tmp$estimate["mean in group M"] - tmp$estimate["mean in group F"],2)` points on average (i.e., `r round(tmp$estimate["mean in group M"],2)` for males vs  `r round(tmp$estimate["mean in group F"],2)` for females).

```{r eval=FALSE, include=FALSE}
# Are the trials within one type different?
a3 <- aov(score ~ trial, data=d_wm_long[ d_wm_long$trial %in% c("trial_01", "trial_02", "trial_03"), ]); summary(a3); TukeyHSD(a3); # yes: trial_01 is easier
a4 <- aov(score ~ trial, data=d_wm_long[ d_wm_long$trial %in% c("trial_04", "trial_05", "trial_06"), ]); summary(a4); TukeyHSD(a4); # no
a5 <- aov(score ~ trial, data=d_wm_long[ d_wm_long$trial %in% c("trial_07", "trial_08", "trial_09"), ]); summary(a5); TukeyHSD(a5); # yes: trial_09 is easier
a6 <- aov(score ~ trial, data=d_wm_long[ d_wm_long$trial %in% c("trial_10", "trial_11", "trial_12"), ]); summary(a6); TukeyHSD(a6); # yes: trial_12 is easier than trial_10
a7 <- aov(score ~ trial, data=d_wm_long[ d_wm_long$trial %in% c("trial_13", "trial_14", "trial_15"), ]); summary(a7); TukeyHSD(a7); # yes: trial_14 is harder
```

The three trials with the same length visually seem to have similar behaviors, but there are nevertheless significant differences between them:

| Length | Trials     | ANOVA by trial | Signif. pairwise diffs. |
|--------|------------|----------------|-------------------------|
| 3      | 01, 02, 03 | `r sprintf("F(%d, %d)=%.2f, *p*=%s", (s <- summary(a <- aov(score ~ trial, data=d_wm_long[ d_wm_long$trial %in% c("trial_01", "trial_02", "trial_03"), ]))[[1]])$Df[1], s$Df[2], s[1,"F value"], scinot(s[1,"Pr(>F)"]))` | trial 01 is easier |
| 4      | 04, 05, 06 | `r sprintf("F(%d, %d)=%.2f, *p*=%s", (s <- summary(a <- aov(score ~ trial, data=d_wm_long[ d_wm_long$trial %in% c("trial_04", "trial_05", "trial_06"), ]))[[1]])$Df[1], s$Df[2], s[1,"F value"], scinot(s[1,"Pr(>F)"]))` | all trials are similar |
| 5      | 07, 08, 09 | `r sprintf("F(%d, %d)=%.2f, *p*=%s", (s <- summary(a <- aov(score ~ trial, data=d_wm_long[ d_wm_long$trial %in% c("trial_07", "trial_08", "trial_09"), ]))[[1]])$Df[1], s$Df[2], s[1,"F value"], scinot(s[1,"Pr(>F)"]))` | trial 09 is easier |
| 6      | 10, 11, 12 | `r sprintf("F(%d, %d)=%.2f, *p*=%s", (s <- summary(a <- aov(score ~ trial, data=d_wm_long[ d_wm_long$trial %in% c("trial_10", "trial_11", "trial_12"), ]))[[1]])$Df[1], s$Df[2], s[1,"F value"], scinot(s[1,"Pr(>F)"]))` | trial 12 is easier than trial 10 |
| 7      | 13, 14, 15 | `r sprintf("F(%d, %d)=%.2f, *p*=%s", (s <- summary(a <- aov(score ~ trial, data=d_wm_long[ d_wm_long$trial %in% c("trial_13", "trial_14", "trial_15"), ]))[[1]])$Df[1], s$Df[2], s[1,"F value"], scinot(s[1,"Pr(>F)"]))` | trial 14 is harder |
`r paste0("<div class='caption'>",capTab("Comparing the trials of the same length using one-way ANOVA with posthoc Tukey pairwise comparisons (details not shown here but summarized in the last column)."),"</div>")`

```{r include=FALSE}
# PCA and EFA across trials:
d <- d_wm[ d_wm$ID %in% d_ppts$ID, c("trial_01", "trial_02", "trial_03", "trial_04", "trial_05", "trial_06", "trial_07", "trial_08", "trial_09", "trial_10", "trial_11", "trial_12", "trial_13", "trial_14", "trial_15")]; rownames(d) <- d_wm$ID[ d_wm$ID %in% d_ppts$ID ];

# PCA:
wm_pca <- prcomp(d, scale.=TRUE, center=TRUE);
wm_pca;
summary(wm_pca);

# EFA:
# Tests of factorability (see https://towardsdatascience.com/exploratory-factor-analysis-in-r-e31b0015f224):
KMO(r=cor(d)); # Kaiser-Meyer-Olkin (KMO) sampling adequacy = 0.95 >> 0.60 -> ok
cortest.bartlett(d); # p=0 -> ok
det(cor(d)); # >0 -> ok

# Number of factors:
fa.parallel(d, fa="fa", n.iter=100, error.bars=TRUE); # -> 1 factor
nfactors(d); # -> 1 factor

wm_fa_1f <- factanal(d, 1, rotation="promax"); wm_fa_1f;
fa.diagram(wm_fa_1f$loadings);
```
We conducted both a **Principal Component Analysis (PCA)** and an **Exploratory Factor Analysis (EFA)** on all trials together, and we found that there seems to be a single factor.

For **PCA**, PC~1~ explains `r round(100*(s <- summary(wm_pca)$importance)["Proportion of Variance", "PC1"],1)`% of the variance, followed by PC~2~ which explains only `r round(100*(s <- summary(wm_pca)$importance)["Proportion of Variance", "PC2"],1)`%, suggesting that all trials load on a single latent variable:

```{r fig.width=5, fig.height=4, fig.cap=capFig("Screeplot of the PCA of all the working memory trials together.")}
fviz_eig(wm_pca);
```
```{r fig.width=5, fig.height=4, fig.cap=capFig("Loading of the working memory trials on the first 2 PCs.")}
fviz_pca_var(wm_pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             );
```
```{r fig.width=5, fig.height=4, fig.cap=capFig("The participants plotted on the first 2 PCs, colored by their their qualities of representation (cos2).")}
fviz_pca_ind(wm_pca,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             label = "none",
             repel = TRUE     # Avoid text overlapping
             );
```

For **EFA**, all the preliminary tests suggest that factor analysis is appropriate (`r sprintf("Kaiser-Meyer-Olkin = %.2f > 0.60; Bartlett's test is significant: *χ*^2^(%d)=%.1f, *p*=%.3g; and det(cor(data))=%.2g > 0", KMO(r=cor(d))$MSA, (tmp <- cortest.bartlett(d))$df, tmp$chisq, tmp$p.value, det(cor(d)))`) and all the recommended methods for finding the appropriate number of factors suggest that 1 factor is enough:

```{r fig.width=4, fig.height=4, fig.cap=capFig("Screeplot of the observed, simulated and randomized data with 1 standard deviation error bars (as generated by `fa.parallel())`."), results='hide'}
fa.parallel(d, fa="fa", n.iter=100, error.bars=TRUE); # -> 1 factor
```
```{r fig.width=4*2, fig.height=4*2, fig.cap=capFig("Number of factors as suggested by the VSS criterion (top left), the complexity of the solution (top right), BIC (bottom left) and Root Mean Residual (bottom right), as implemented by `nfactors()`."), results='hide'}
nfactors(d);
```

Even if this 1-factor model does not seem to formally be sufficient to explain all of the variance in the data (about 35.8% of the variance explained, but `r sprintf("*χ*^2^(%d)=%.1f, *p*=%.2g < 0.05", wm_fa_1f$dof, wm_fa_1f$STATISTIC, wm_fa_1f$PVAL)`), the loadings are very similar: 
```{r fig.width=6, fig.height=8, fig.cap=capFig("Loadings of the variables in the 1 factor model."), results='hide'}
fa.diagram(wm_fa_1f$loadings);
```

```{r include=FALSE}
# CFA 1 factor:
wm_cfa_1f <- ' 
    # the factor:
    wm =~ trial_01 + trial_02 + trial_03 + trial_04 + trial_05 + trial_06 + trial_07 + trial_08 + trial_09 + trial_10 + trial_11 + trial_12 + trial_13 + trial_14 + trial_15 
  ';
wm_cfa_1f_fit <- cfa(wm_cfa_1f, data=d, se="robust.sem");
summary(wm_cfa_1f_fit, fit.measures=TRUE, standardize=TRUE, rsquare=TRUE, estimates=TRUE, ci=TRUE);
fitMeasures(wm_cfa_1f_fit, c("chisq", "df", "pvalue", "cfi", "tli", "nnfi", "rfi"));
#lavaanPlot(model=wm_cfa_1f_fit, coefs=TRUE, sig=1.00, stand=TRUE, covs=TRUE, stars=c("regress", "latent", "covs"), edge_options=list(color="gray"));
modindices(wm_cfa_1f_fit, sort=TRUE);
```
We also implemented a **Confirmatory Factor Analysis (CFA)** with all trails loading on a single latent *wm* variable, and we found that while the model formally does not fit the data (`r sprintf("*χ*^2^(%0.f)=%.1f, *p*=%.2g ≤ 0.05", (s <- fitMeasures(wm_cfa_1f_fit, c("chisq", "df", "pvalue")))["chisq"], s["df"], s["pvalue"])`), its fit indices are ok (`r sprintf("CFI=%.2f, TLI=%.2f, RFI=%.2f, RMSEA=%.2f", (s <- fitMeasures(wm_cfa_1f_fit, c("cfi", "tli", "rfi", "rmsea")))["cfi"], s["tli"], s["rfi"], s["rmsea"])`) and the path coefficients suggest that all trials load in similar ways on the *wm* latent:
```{r fig.width=10, fig.height=3, fig.cap=capFig("Confirnatory factor analysis (CFA) of the working memory trislas with a single latent factor *wm*.")}
lavaanPlot(model=wm_cfa_1f_fit, coefs=TRUE, sig=1.00, stand=TRUE, covs=TRUE, stars=c("regress", "latent", "covs"), edge_options=list(color="gray"));
```

**Given all these**, it makes sense to compute a *total score* from all the trials (i.e., using the same weight of 1.0); we further normalized it between its minimum possible score of 0.0 and its maximum of 3 &times; (3+4+5+6+7) = 75 (this variable will be denoted as *wm_norm*).
```{r}
# Compute the total score for the working memory task:
d_wm$wm <- d_wm$trial_01 + d_wm$trial_02 + d_wm$trial_03 + d_wm$trial_04 + d_wm$trial_05 + d_wm$trial_06 + d_wm$trial_07 + d_wm$trial_08 + d_wm$trial_09 + d_wm$trial_10 + d_wm$trial_11 + d_wm$trial_12 + d_wm$trial_13 + d_wm$trial_14 + d_wm$trial_15;
d_wm$wm_norm <- (d_wm$wm - 0)/75; # normalize between 0 (minimum) and 75 (maximum)

# Add the WM score (raw and normalized) to the data:
d_all <- merge(d_all, d_wm[,c("ID", "wm", "wm_norm")], by="ID", all.x=TRUE, all.y=FALSE);
```

This variable is distributed as follows:
```{r fig.width=4*3, fig.height=4*1, fig.cap=capFig("**A**: distribution of *normalized working memory score* (*wm_norm*) overall (thick black curve) and by *gender* (colored transparent curves) in the sample. **B**: relationship between *wm_norm* and *age* by *gender* with linear regression lines (and 95%CIs). **C**: Relationship between *wm_norm* and *education_years* by *gender* with linear regression lines (and 95%CIs).")}
grid.arrange(ggplot(d_all, aes(x=wm_norm, fill=gender, color=gender)) + 
               geom_density(alpha=0.25) + geom_density(data=d_all, aes(x=wm_norm), alpha=0.5, color="black", fill=NA, linewidth=1.0) + 
               scale_fill_manual(values=c("F"="red", "M"="blue")) + scale_color_manual(values=c("F"="red", "M"="blue")) + xlim(c(0,1)) + ggtitle("A") + 
               NULL,
             ggplot(d_all, aes(x=age, y=wm_norm, color=gender)) + 
               geom_point(alpha=0.5) + geom_smooth(method="lm") + 
               scale_color_manual(values=c("F"="red", "M"="blue")) + ylim(c(0,1)) + ggtitle("B") + 
               NULL,
             ggplot(d_all, aes(x=education_years, y=wm_norm, color=gender)) + 
               geom_point(alpha=0.5) + geom_smooth(method="lm") + 
               scale_color_manual(values=c("F"="red", "M"="blue")) + ylim(c(0,1)) + ggtitle("C") + 
               NULL,
             ncol=3);
```

```{r include=FALSE}
# Start with the full model and manually simplify:
m0 <- lm(wm_norm ~ age * education_years * gender, data=d_all); summary(m0);
m1 <- update(m0, . ~ . - age:education_years:gender - age:education_years - age:gender - education_years:gender); summary(m1); anova(m1, m0);

m_wm_norm <- m1;
(s_m_wm_norm <- summary(m_wm_norm));
par(mfrow=c(2,2));plot(m_wm_norm, ask=FALSE);par(mfrow=c(1,1)); # looks ok
```
We performed the linear regression of *wm_norm* on *age*, the number of years of formal education (*education_years*), *gender* (the reference level being females) and all their interaction, and, following manual simplification, we found that this model behaves well (diagnostic plots not shown), that it explains adjusted *R*^2^ = `r round(100*s_m_wm_norm$adj.r.squared,1)`% of the variance, and that *age* has a highly significant negative effect (*β* = `r round(s_m_wm_norm$coefficients["age","Estimate"],3)`, *p* = `r scinot(s_m_wm_norm$coefficients["age","Pr(>|t|)"])`), *education_years* has a highly significant positive effect (*β* = `r round(s_m_wm_norm$coefficients["education_years","Estimate"],3)`, *p* = `r scinot(s_m_wm_norm$coefficients["education_years","Pr(>|t|)"])`), and *gender* shows a significant difference between males and females, with males having overall smaller score than females (*β* = `r round(s_m_wm_norm$coefficients["genderM","Estimate"],3)`, *p* = `r scinot(s_m_wm_norm$coefficients["genderM","Pr(>|t|)"])`).

```{r include=FALSE, echo=FALSE}
# For paper:
ggplot(d_all, aes(x=wm_norm, fill=gender, color=gender)) + 
  geom_density(alpha=0.25) + geom_density(data=d_all, aes(x=wm_norm), alpha=0.5, color="black", fill=NA, linewidth=1.0) + 
  scale_fill_manual(values=c("F"="red", "M"="blue")) + scale_color_manual(values=c("F"="red", "M"="blue")) + xlim(c(0,1)) + xlab("Normalized working memory total score") +
  NULL;
ggsave("./paper/figures/distr_wm_norm.tiff", device="tiff", units="in", width=5, height=5, dpi=300, compression="lzw");
ggsave("./paper/figures/distr_wm_norm.jpg",  device="jpeg", units="in", width=5, height=5, quality=85);
```


## The tone task

The tone task is a AX task in which the participant is presented, in a given trial, with a pair of syllables that may differ only in tone, and has to decide if the two syllables are the "same" or "different". 
For a given tone pair (let's say "^a^" and "^b^"), there are two syllables with different segmental content (let's say, "A" and "B"), resulting in the following four syllables+tone combinations: A^a^, A^b^, B^a^ and B^b^. 
With these, we have the following possible trials: 

- "same": A^a^A^a^, A^b^A^b^, B^a^B^a^ and B^b^B^b^ -- the tones and segmental content of the two syllables are the same (these are denoted as "same_1", "same_2", "same_3" and "same_4"),
- "different": A^a^A^b^, A^b^A^a^ (the order differs), B^a^B^b^, B^b^B^a^ (the order differs), resulting in 4 pair &times; order trials (denoted as "order_1" and "order_2", and "pair_1" and "pair_2", respectively).

Each trial was repeated twice and the repose to a given trial was scored as "correct" if the tones were different and the repose was "different", or if the tones were the same and the response was "same", and was scored as "incorrect" otherwise.
Each participant was presented with a random (unique) order of the trials.
Please note that some of the possible trials were not included in the task, as the participants showed a ceiling effects during the pilot study. 

We used the 9 (of the 10) phonological tones in the language that occur in unchecked syllables [@wu_grammar_2018], represented here by letters:
```{r}
# Some sanity checks:
if( sum(d_tone$tones == "same"      & d_tone$syllable1 != d_tone$syllable2) > 0 ) stop("There are 'same' trials where the two syllables differ!");
if( sum(d_tone$tones == "different" & d_tone$syllable1 == d_tone$syllable2) > 0 ) stop("There are 'different' trials where the two syllables are the same!");

# d_tone_notations <- read.table(text="
# letter tone.levels
# l      55
# p      35
# s      323
# v      53
# h      33
# t      13
# x      31
# c      11
# k      453",
# header=TRUE, sep="");

d_tone_notations <- d_stims;
d_tone_notations$tone.letter1 <- substring(d_tone_notations$stimulus,1,1);
d_tone_notations$tone.letter2 <- substring(d_tone_notations$stimulus,3,3);
d_tone_notations$segments1 <- gsub("\\d", "", d_tone_notations$syllable1); d_tone_notations$tone1 <- as.numeric(gsub("\\D", "", d_tone_notations$syllable1));
d_tone_notations$segments2 <- gsub("\\d", "", d_tone_notations$syllable2); d_tone_notations$tone2 <- as.numeric(gsub("\\D", "", d_tone_notations$syllable2));

d_tone_notations <- unique(rbind(d_tone_notations[, c("tone.letter1", "tone1")], 
                                 setNames(d_tone_notations[, c("tone.letter2", "tone2")], c("tone.letter1", "tone1"))));
names(d_tone_notations) <- c("tone.letter", "tone");
d_tone_notations <- d_tone_notations[ order(d_tone_notations$tone.letter), ];
#kable(d_tone_notations, row.names=FALSE, col.names=c("Letter", "Tone levels"), align=c("r","l"),
#      caption=capTab("Representing tones by letters. First column shows the letter while the second column gives the tone representation using the numeric notation with 5 levels. Please see @wu_grammar_2018 and @donohue_wu_2013 for details about the tones in this language, and, for example, @yip_tone_2002, for a general introduction to tone."));
tmp <- matrix(t(d_tone_notations)[1,], nrow=1); colnames(tmp) <- d_tone_notations[,2];
kable(tmp, row.names=FALSE, align=c("r"),
      caption=capTab("Representing tones (column names, using the numeric notation with 5 levels) by letters (first row). Please see @wu_grammar_2018 and @donohue_wu_2013 for details about the tones in this language, and, for example, @yip_tone_2002, for a general introduction to tone."));
```

Likewise, in the name of brevity, we denote the segmental content of the syllables using CAPITAL letters, as follows:
```{r}
d_segments <- sort(unique(gsub("\\d", "", unique(c(d_tone$syllable1, d_tone$syllable2))))); names(d_segments) <- LETTERS[1:length(d_segments)]; # map segmental contents to CAPITAL letters
tmp <- matrix(names(d_segments), nrow=1); colnames(tmp) <- d_segments;
#kable(data.frame("segments"=d_segments, "letter"=names(d_segments)), row.names=FALSE, align=c("r", "l"), caption=capTab("Uniquely mapping the segmental content of the syllables used onto CAPITAL letters (for notational brevity)."));
kable(tmp, row.names=FALSE, align=c("r"), caption=capTab("Uniquely mapping the segmental content of the syllables used onto CAPITAL letters (for notational brevity)."));
```

The actual stimuli used are listed below:

```{r}
d <- unique(d_tone[ d_tone$tones == "same", c("syllable1", "isword1") ]);
d$segments1 <- gsub("\\d", "", d$syllable1); d$tone1 <- as.numeric(gsub("\\D", "", d$syllable1));
#d <- merge(d, d_tone_notations, by.x="tone1", by.y="tone.levels", all.x=TRUE, all.y=FALSE);
d <- unique(d[ order(d$segments1, d$tone1),]);
d$item_short1 <- vapply(1:nrow(d), function(i) paste0(names(d_segments)[d_segments == gsub("\\d", "", d$syllable1[i])], d_tone_notations$tone.letter[ d_tone_notations$tone == gsub("\\D", "", d$syllable1[i]) ]), character(1));
d <- merge(d, d_tone_notations[,c("tone", "tone.letter")], by.x="tone1", by.y="tone", all.x=TRUE, all.y=FALSE);
kable(d %>% dplyr::mutate(show=paste0(ifelse(isword1=="yes","**",""),segments1,"^",tone1,"^",ifelse(isword1=="yes","**",""))) %>% dplyr::select(show, segments1, tone1, tone.letter, isword1, item_short1) %>% dplyr::arrange(item_short1), 
      row.names=FALSE, 
      col.names=c("Syllabe", "Segments", "Tone", "Tone letter", "Is real word?", "Short name"), 
      align=c("l", "l", "r", "r", "r", "r"),
      caption=capTab(paste0("The ",nrow(d)," stimuli used in the 'same' task, showing the segments and tone, orded alphabetically; some are actual words in the language (also maked using **bold**). Please note that the actual trials present pairs of such identical stimuli.")));
```

```{r}
d <- unique(d_tone[ d_tone$tones == "different", c("syllable1", "isword1", "syllable2", "isword2") ]);
d$segments1 <- gsub("\\d", "", d$syllable1); d$tone1 <- as.numeric(gsub("\\D", "", d$syllable1));
d$segments2 <- gsub("\\d", "", d$syllable2); d$tone2 <- as.numeric(gsub("\\D", "", d$syllable2));
#d <- merge(d, d_tone_notations, by.x="tone1", by.y="tone.levels", all.x=TRUE, all.y=FALSE);
d <- unique(d[ order(d$segments1, d$tone1, d$segments2, d$tone2),]);
d$item_short1 <- vapply(1:nrow(d), function(i) paste0(names(d_segments)[d_segments == gsub("\\d", "", d$syllable1[i])], d_tone_notations$tone.letter[ d_tone_notations$tone == gsub("\\D", "", d$syllable1[i]) ], 
                                                      d_tone_notations$tone.letter[ d_tone_notations$tone == gsub("\\D", "", d$syllable2[i]) ]), character(1));
d <- merge(d, d_tone_notations[,c("tone", "tone.letter")], by.x="tone1", by.y="tone", all.x=TRUE, all.y=FALSE);
d <- merge(d, d_tone_notations[,c("tone", "tone.letter")], by.x="tone2", by.y="tone", all.x=TRUE, all.y=FALSE, suffix=c("1", "2"));
kable(d %>% dplyr::mutate(show1=paste0(ifelse(isword1=="yes","**",""),segments1,"^",tone1,"^",ifelse(isword1=="yes","**","")), show2=paste0(ifelse(isword2=="yes","**",""),segments2,"^",tone2,"^",ifelse(isword2=="yes","**",""))) %>% 
        dplyr::select(show1, show2, segments1, tone1, tone.letter1, isword1, segments2, tone2, tone.letter2, isword2, item_short1) %>% dplyr::arrange(item_short1), 
      row.names=FALSE, 
      col.names=c("Syllabe 1", "Syllabe 2", "Segments 1", "Tone 1", "Tone letter 1", "Is real word 1?", "Segments 2", "Tone 2", "Tone letter 2", "Is real word 2?", "Short name"), 
      align=c("l", "l", "l", "l", "r", "r", "l", "l", "r", "r", "r"),
      caption=capTab(paste0("The ",nrow(d)," pairs of stimuli used in the 'different' task, showing the segments and tone, orded alphabetically; some are actual words in the language (also maked using **bold**). Please note that the reverse order of the stimuli is not shown.")));
d_different_items <- d;
```

```{r}
s <- vapply(which(d_stims$judged_difficult == "yes"), function(i) which(d_different_items$syllable1 == d_stims$syllable1[i] & d_different_items$syllable2 == d_stims$syllable2[i]), numeric(1)); # the "difficult" stimuli
```
Please note that stimuli `r paste0(d_different_items$segments1[s],"^",d_different_items$tone1[s],"^:",d_different_items$segments2[s],"^",d_different_items$tone2[s],"^ (",d_different_items$item_short1[s],")", collapse=", ")` are considered as "difficult" by the task designers in the sense that it is hard to hear the difference in tones even for these highly trained speakers of a tone language (Manxiang Wu, p.c.).


### Descriptives

We begin this analysis based on the ***"6 steps" approach***  of @dima_scale_2018 and the accompanying `R` code available at [https://github.com/alexadima/6-steps-protocol](https://github.com/alexadima/6-steps-protocol).

#### Percent correct responses

Please note that we will a short notation for the items composed of the segment CAPITAL letter, followed by the one (for 'same') or two (for 'different') letter tone notation, and the presentation number (e.g., Its2 is the 2nd presentation of the 'different' item sem^335^:sem^35^).

```{r results='hide'}
# NB: for counts logistic regression we need two columns: one counting the "successes" (aka, the "correct" responses) and one the "failures" (aka the "incorrect" responses).

if( FORCE_COMPUTE_ALL || 
    !all(file.exists("./cached_results/d_tone_wide.rds", "./cached_results/d_tone.rds")) ) # computationally expensive
{
  # Rename the items (and also record which presentation it was):
  d_tone$item_name <- ifelse(d_tone$tones == "same", d_tone$syllable1, paste0(d_tone$syllable1,"_",d_tone$syllable2));
  d_tone$item_short <- vapply(1:nrow(d_tone), function(i) ifelse(d_tone$tones[i] == "same", 
                                                                 paste0(names(d_segments)[d_segments == gsub("\\d", "", d_tone$syllable1[i])], d_tone_notations$tone.letter[ d_tone_notations$tone == gsub("\\D", "", d_tone$syllable1[i]) ]),
                                                                 paste0(names(d_segments)[d_segments == gsub("\\d", "", d_tone$syllable1[i])], d_tone_notations$tone.letter[ d_tone_notations$tone == gsub("\\D", "", d_tone$syllable1[i]) ], 
                                                                        ifelse(gsub("\\d", "", d_tone$syllable1[i]) != gsub("\\d", "", d_tone$syllable2[i]), 
                                                                               stop(paste0("The segmental content of syllable 1 and 2 are different for item '",d_tone$stimulus[i],",!")),
                                                                               d_tone_notations$tone.letter[ d_tone_notations$tone == gsub("\\D", "", d_tone$syllable2[i]) ]))), 
                              character(1)); # convention: same: segmental_LETTER+tone_number; different: segmental_LETTER+tone_number1+tone_number2
  d_tone$presentation <- NA;
  for(i in unique(d_tone$ID))
  {
    s <- which(d_tone$ID == i);
    d_tone$presentation[s] <- vapply(s, function(j) sum(d_tone$item_name[s[s < j]] == d_tone$item_name[j])+1, numeric(1)); # number of this presentation
    d_tone$item_name[s] <- paste0(d_tone$item_name[s],"_",d_tone$presentation[s]); # add presentation to the item name
    d_tone$item_short[s] <- paste0(d_tone$item_short[s],d_tone$presentation[s]); # add presentation to the item name
  }
  
  # The data needs to be in the wide format (i.e., one item per column):
  d_tone_wide <- reshape2::dcast(d_tone, ID ~ item_short, value.var="correct"); 
  d_tone_wide[ d_tone_wide == "yes" ] <- 1; d_tone_wide[ d_tone_wide == "no" ] <- 0; for(i in 1:ncol(d_tone_wide)) d_tone_wide[,i] <- as.numeric(d_tone_wide[,i]); str(d_tone_wide); # recode as numeric (yes == 1)
  
  saveRDS(d_tone,      file=xzfile("./cached_results/d_tone.rds",      compression=9));
  saveRDS(d_tone_wide, file=xzfile("./cached_results/d_tone_wide.rds", compression=9));
} else
{
  d_tone      <- readRDS(xzfile("./cached_results/d_tone.rds"));
  d_tone_wide <- readRDS(xzfile("./cached_results/d_tone_wide.rds"));
}

# Items order:
d_tone_wide_items_order <- data.frame("Items"=names(d_tone_wide)[-1],
                                      "YESCount"=apply(d_tone_wide[,-1], 2, sum),
                                      "YESPercentage"=apply(d_tone_wide[,-1], 2, function(x) sum(x)/length(x)));
d_tone_wide_items_order <- d_tone_wide_items_order[ order(d_tone_wide_items_order$YESPercentage), ];
```

```{r}
kable(d_tone_wide_items_order %>% dplyr::mutate(YESPercentage = sprintf("%.1f%%",YESPercentage*100)), 
      row.names=FALSE, col.names=c("Item (short name)", "# correct reponses", "% correct reponses"), align=c("l", "r", "r"), 
      caption=capTab("Frequencies of 'yes' responses (the items are ordered by % correct responses)."));
```

```{r, fig.width=8, fig.height=40, warning=FALSE, message=FALSE, echo=FALSE, fig.cap=capFig("Endorsement frequencies by item (items ordered by % correct responses).")}
# plots to visualize the data ####
# barplot of endorsement frequencies (number of respondents with affirmative answers)
# tiff( "./Figure1.tif", width=6, height=6, units="in", res=600);
par(oma=c(0,3,0,0))
barplot(d_tone_wide_items_order[,"YESCount"], 
        horiz = TRUE,
        main = "Endorsement frequencies",
        ylab="", 
        xlab="Number of respondents", 
        cex.lab=0.8,
        cex.axis=0.5,
        names.arg=d_tone_wide_items_order[, "Items"], 
        las=2, 
        cex.names=0.8);
# dev.off()
```


#### Correlations between items

```{r, fig.width=25, fig.height=25, warning=FALSE, message=FALSE, echo=FALSE, fig.cap=capFig("Correlation matrix between items.")}
# prepare correlation matrix
# for binary items, use tetrachoric correlation matrix
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/tetrachoric.rds") ) # computationally expensive
{
  bluesqs <- as.data.frame(tetrachoric(d_tone_wide[,-1])["rho"]);
  saveRDS(bluesqs, file=xzfile("./cached_results/tetrachoric.rds", compression=9));
} else
{
  bluesqs <- readRDS(xzfile("./cached_results/tetrachoric.rds"));
}
# heat plot of correlations matrix 
# uncomment the png & devoff lines if you want to save as png in the working directory
if( knitr::is_html_output() || knitr::is_latex_output() ) # running it directly in the console results in Error in plot.new() : figure margins too large
{
  # png('corplot.png')
  cor.plot(bluesqs, numbers=FALSE, main="Correlations between items", 
           labels=rownames(bluesqs), las=2,
           cex=1.0, cex.axis=1.0)
  # dev.off()
}

# if required, exclude from next analyses items that have no/little variance 
# (e.g. <5% in one category for binary variables, less than 5% in 2 adjacent response options for ordinal variables)
```

It can be seen that the tetrachoric correlations between the items are rather low, varying between `r round(min(as.numeric(bluesqs[upper.tri(bluesqs)])),2)` and  `r round(max(as.numeric(bluesqs[upper.tri(bluesqs)])),2)`, with a mean of  `r round(mean(as.numeric(bluesqs[upper.tri(bluesqs)])),2)`, a median of  `r round(median(as.numeric(bluesqs[upper.tri(bluesqs)])),2)`, sd of  `r round(sd(as.numeric(bluesqs[upper.tri(bluesqs)])),2)` and IQR of  `r round(IQR(as.numeric(bluesqs[upper.tri(bluesqs)])),2)`:
```{r, fig.width=6, fig.height=4, warning=FALSE, message=FALSE, echo=FALSE, fig.cap=capFig("Histogram of the tetracoric correlations between different itmes.")}
hist(as.numeric(bluesqs[upper.tri(bluesqs)]), main="Histogram of the tetracoric correlations", xlab="Tetracoric correlation", xlim=c(-1,1));
```

```{r, fig.width=5, fig.height=25, warning=FALSE, message=FALSE, echo=FALSE, fig.cap=capFig("Hierarchical clustering of the items using 1 - tetrachoric correlations.")}
items_clust <- hclust(as.dist(1-bluesqs));
par(cex=0.6, mar=c(5, 8, 4, 1));
plot(as.dendrogram(items_clust), horiz=TRUE);
```

```{r, fig.width=5, fig.height=40, warning=FALSE, message=FALSE, echo=FALSE, fig.cap=capFig("Mean tetrachoric correlation with the other items.")}
# look at the items that tend to have negative correlations:
bluesqs_mean <- apply(bluesqs, 1, mean);
par(oma=c(0,3,0,0))
barplot(bluesqs_mean, 
        horiz = TRUE,
        col = ifelse(bluesqs_mean < 0, "blue", "red"),
        main = "Mean correlation with the other items",
        ylab="", 
        xlab="Mean tetrachoric correlation", 
        cex.lab=0.8,
        cex.axis=0.5,
        names.arg=names(bluesqs_mean), 
        las=2, 
        cex.names=0.8);
```


#### "Weird" items

It can be seen that the following items seem "weird":
```{r}
d_all_items <- data.frame("Items"=names(bluesqs_mean), "mean_rho"=bluesqs_mean);
d_all_items <- merge(d_all_items, d_tone_wide_items_order, all.x=TRUE, all.y=FALSE);
d_special_items <- d_all_items[ d_all_items$mean_rho < 0, ];
kable(d_special_items %>% dplyr::mutate(YESPercentage = sprintf("%.1f%%",YESPercentage*100), mean_rho=round(mean_rho,2)), 
      row.names=FALSE, col.names=c("Item (short name)", "mean tetra. corr.", "# correct reponses", "% correct reponses"), align=c("l", "r", "r", "r"), 
      caption=capTab("'Weird' items that have an average negative correlation with the other items."));
```

Interestingly, they seem to form meaningful groups:

- the 4 "different" items involving segment **B ("`r d_segments["B"]`")** and tones **^h^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "h" ]`)** and **^x^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "x" ]`)** in both orders and both presentations; they also have very low % correct responses (around `r sprintf("%.0f%%",100*mean(d_special_items$YESPercentage[ d_special_items$Items %in% c("Bhx1", "Bhx2", "Bxh1", "Bxh2") ]))`) and also have correlations between them (tetrachoric rho's between `r tmp <- bluesqs[rownames(bluesqs) %in% c("Bhx1", "Bhx2", "Bxh1", "Bxh2"), colnames(bluesqs) %in% paste0("rho.",c("Bhx1", "Bhx2", "Bxh1", "Bxh2"))]; round(min(as.numeric(tmp[upper.tri(tmp)])),2)` and `r round(max(as.numeric(tmp[upper.tri(tmp)])),2)`, with mean = `r round(mean(as.numeric(tmp[upper.tri(tmp)])),2)` and sd = `r round(sd(as.numeric(tmp[upper.tri(tmp)])),2)`) → this suggests that tones ^h^ and ^x^ are very hard to distinguish when paired with segments B;

- the 4 "different" items involving segment **H ("`r d_segments["H"]`")** and tones **^s^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "s" ]`)** and **^t^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "t" ]`)** in both orders and both presentations; they also have very low % correct responses (around `r sprintf("%.0f%%",100*mean(d_special_items$YESPercentage[ d_special_items$Items %in% c("Hst1", "Hst2", "Hts1", "Hts2") ]))`) and also have correlations between them (tetrachoric rho's between `r tmp <- bluesqs[rownames(bluesqs) %in% c("Hst1", "Hst2", "Hts1", "Hts2"), colnames(bluesqs) %in% paste0("rho.",c("Hst1", "Hst2", "Hts1", "Hts2"))]; round(min(as.numeric(tmp[upper.tri(tmp)])),2)` and `r round(max(as.numeric(tmp[upper.tri(tmp)])),2)`, with mean = `r round(mean(as.numeric(tmp[upper.tri(tmp)])),2)` and sd = `r round(sd(as.numeric(tmp[upper.tri(tmp)])),2)`) → this suggests that tones ^s^ and ^t^ are very hard to distinguish when paired with segments H;

- the 4 "different" items involving segment **I ("`r d_segments["I"]`")** and tones **^s^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "s" ]`)** and **^t^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "t" ]`)** in both orders and both presentations; they also have very low % correct responses (around `r sprintf("%.0f%%",100*mean(d_special_items$YESPercentage[ d_special_items$Items %in% c("Ist1", "Ist2", "Its1", "Its2") ]))`) and also have correlations between them (tetrachoric rho's between `r tmp <- bluesqs[rownames(bluesqs) %in% c("Ist1", "Ist2", "Its1", "Its2"), colnames(bluesqs) %in% paste0("rho.",c("Ist1", "Ist2", "Its1", "Its2"))]; round(min(as.numeric(tmp[upper.tri(tmp)])),2)` and `r round(max(as.numeric(tmp[upper.tri(tmp)])),2)`, with mean = `r round(mean(as.numeric(tmp[upper.tri(tmp)])),2)` and sd = `r round(sd(as.numeric(tmp[upper.tri(tmp)])),2)`) → this suggests that tones ^s^ and ^t^ are very hard to distinguish when paired with segments I;

- the 4 "different" items involving segment **K ("`r d_segments["K"]`")** and tones **^k^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "k" ]`)** and **^l^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "l" ]`)** in both orders and both presentations (please note that technically Kkl1 has a very small positive average correlation); they also have very low % correct responses (around `r sprintf("%.0f%%",100*mean(d_special_items$YESPercentage[ d_special_items$Items %in% c("Kkl1", "Kkl2", "Klk1", "Klk2") ]))`) and also have correlations between them (tetrachoric rho's between `r tmp <- bluesqs[rownames(bluesqs) %in% c("Kkl1", "Kkl2", "Klk1", "Klk2"), colnames(bluesqs) %in% paste0("rho.",c("Kkl1", "Kkl2", "Klk1", "Klk2"))]; round(min(as.numeric(tmp[upper.tri(tmp)])),2)` and `r round(max(as.numeric(tmp[upper.tri(tmp)])),2)`, with mean = `r round(mean(as.numeric(tmp[upper.tri(tmp)])),2)` and sd = `r round(sd(as.numeric(tmp[upper.tri(tmp)])),2)`) → this suggests that tones ^k^ and ^l^ are very hard to distinguish when paired with segments K;

- the 4 "different" items involving segment **L ("`r d_segments["L"]`")** and tones **^l^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "l" ]`)** and **^p^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "p" ]`)** in both orders and both presentations; they also have very low % correct responses (around `r sprintf("%.0f%%",100*mean(d_special_items$YESPercentage[ d_special_items$Items %in% c("Llp1", "Llp2", "Lpl1", "Lpl2") ]))`) and also have correlations between them (tetrachoric rho's between `r tmp <- bluesqs[rownames(bluesqs) %in% c("Llp1", "Llp2", "Lpl1", "Lpl2"), colnames(bluesqs) %in% paste0("rho.",c("Llp1", "Llp2", "Lpl1", "Lpl2"))]; round(min(as.numeric(tmp[upper.tri(tmp)])),2)` and `r round(max(as.numeric(tmp[upper.tri(tmp)])),2)`, with mean = `r round(mean(as.numeric(tmp[upper.tri(tmp)])),2)` and sd = `r round(sd(as.numeric(tmp[upper.tri(tmp)])),2)`) → this suggests that tones ^l^ and ^p^ are very hard to distinguish when paired with segments L;

- Dhc1 (involving stimuli `r paste0(d_segments["D"],"^",d_tone_notations$tone[ d_tone_notations$tone.letter == "h" ],"^")` and `r paste0(d_segments["D"],"^",d_tone_notations$tone[ d_tone_notations$tone.letter == "c" ],"^")`) basically has an average correlation of 0.0, while Dhc2 and Dch1 have a very small positive correlation, but Dch2 has a large positive correlation, suggesting that this may be a different case;

- there are only `r nrow(tmp <- unique(d_tone[ d_tone$item_short %in% c("Bhx1", "Bhx2", "Bxh1", "Bxh2", "Hst1", "Hst2", "Hts1", "Hts2", "Ist1", "Ist2", "Its1", "Its2", "Kkl1", "Kkl2", "Klk1", "Klk2", "Llp1", "Llp2", "Lpl1", "Lpl2") & (d_tone$isword1 == "yes" | d_tone$isword2 == "yes"), 7:12]))` real words involved in these "special" items: `r paste0(unique(na.omit(c(ifelse(tmp$isword1 == "yes", paste0(tmp$syllable1," for ",tmp$item_short), NA), ifelse(tmp$isword2 == "yes", paste0(tmp$syllable2," for ",tmp$item_short), NA)))), collapse=", ")`.

Moreover:

- there is only one "same" item (Mc4, i.e, the 4th presentation of stimulus `r paste0(d_segments["M"],"^",d_tone_notations$tone[ d_tone_notations$tone.letter == "c" ],"^")`) that has a low positive average correlation;

- the **other "different" stimuli involving B ("`r d_segments["B"]`")** (i.e., `r paste0(tmp1 <- sort(unique(d_tone$item_short[ d_tone$tones == "different" & substring(d_tone$item_short,1,1) =="B" & !(d_tone$item_short %in% c("Bhx1", "Bhx2", "Bxh1", "Bxh2"))])), collapse=", ")`) have around `r sprintf("%.0f%%",100*mean(d_all_items$YESPercentage[ d_all_items$Items %in% tmp1 ]))` correct responses, and also have tetrachoric correlations between them between `r tmp <- bluesqs[rownames(bluesqs) %in% tmp1, colnames(bluesqs) %in% paste0("rho.",tmp1)]; round(min(as.numeric(tmp[upper.tri(tmp)])),2)` and `r round(max(as.numeric(tmp[upper.tri(tmp)])),2)`, with mean = `r round(mean(as.numeric(tmp[upper.tri(tmp)])),2)` and sd = `r round(sd(as.numeric(tmp[upper.tri(tmp)])),2)`;

- there are **no other "different" stimuli involving H ("`r d_segments["H"]`")**;

- the **other "different" stimuli involving I ("`r d_segments["I"]`")** (i.e., `r paste0(tmp1 <- sort(unique(d_tone$item_short[ d_tone$tones == "different" & substring(d_tone$item_short,1,1) =="I" & !(d_tone$item_short %in% c("Ist1", "Ist2", "Its1", "Its2"))])), collapse=", ")`) have around `r sprintf("%.0f%%",100*mean(d_all_items$YESPercentage[ d_all_items$Items %in% tmp1 ]))` correct responses, and also have tetrachoric correlations between them between `r tmp <- bluesqs[rownames(bluesqs) %in% tmp1, colnames(bluesqs) %in% paste0("rho.",tmp1)]; round(min(as.numeric(tmp[upper.tri(tmp)])),2)` and `r round(max(as.numeric(tmp[upper.tri(tmp)])),2)`, with mean = `r round(mean(as.numeric(tmp[upper.tri(tmp)])),2)` and sd = `r round(sd(as.numeric(tmp[upper.tri(tmp)])),2)`;

- there are **no other "different" stimuli involving K ("`r d_segments["K"]`")**;

- the **other "different" stimuli involving L ("`r d_segments["L"]`")** (i.e., `r paste0(tmp1 <- sort(unique(d_tone$item_short[ d_tone$tones == "different" & substring(d_tone$item_short,1,1) =="L" & !(d_tone$item_short %in% c("Llp1", "Llp2", "Lpl1", "Lpl2"))])), collapse=", ")`) have around `r sprintf("%.0f%%",100*mean(d_all_items$YESPercentage[ d_all_items$Items %in% tmp1 ]))` correct responses, and also have tetrachoric correlations between them between `r tmp <- bluesqs[rownames(bluesqs) %in% tmp1, colnames(bluesqs) %in% paste0("rho.",tmp1)]; round(min(as.numeric(tmp[upper.tri(tmp)])),2)` and `r round(max(as.numeric(tmp[upper.tri(tmp)])),2)`, with mean = `r round(mean(as.numeric(tmp[upper.tri(tmp)])),2)` and sd = `r round(sd(as.numeric(tmp[upper.tri(tmp)])),2)`;

- there are **no other "different" stimuli involving tones ^h^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "h" ]`) and ^x^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "x" ]`)**;

- there are **no other "different" stimuli involving tones ^s^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "t" ]`) and ^x^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "x" ]`)**;

- the **other "different" stimuli involving tones ^k^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "k" ]`) and ^l^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "l" ]`)** (i.e., `r paste0(tmp1 <- sort(unique(d_tone$item_short[ d_tone$tones == "different" & (substring(d_tone$item_short,2,3) %in% c("kl","lk")) & !(d_tone$item_short %in% c("Kkl1", "Kkl2", "Klk1", "Klk2"))])), collapse=", ")`) have around `r sprintf("%.0f%%",100*mean(d_all_items$YESPercentage[ d_all_items$Items %in% tmp1 ]))` correct responses, and also have tetrachoric correlations between them between `r tmp <- bluesqs[rownames(bluesqs) %in% tmp1, colnames(bluesqs) %in% paste0("rho.",tmp1)]; round(min(as.numeric(tmp[upper.tri(tmp)])),2)` and `r round(max(as.numeric(tmp[upper.tri(tmp)])),2)`, with mean = `r round(mean(as.numeric(tmp[upper.tri(tmp)])),2)` and sd = `r round(sd(as.numeric(tmp[upper.tri(tmp)])),2)`;

- there are **no other "different" stimuli involving tones ^l^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "l" ]`) and ^p^ (`r d_tone_notations$tone[ d_tone_notations$tone.letter == "p" ]`)**.

Taken together, these suggest that the four classes of "different" stimuli listed above (i.e., the permutations of B^hx^, H^st^, I^st^, K^kl^ and L^lp^) tend to be massively misinterpreted by the participants (resulting in "incorrect" responses).
However, we are missing here crucial items involving the H and K segments and other tone pairs, and the ^hx^, ^sx^ and ^lp^ pairs of tones involving other segments, to be able to speculate if this is related to the particular combination of segments and tone pairs, or to the segments and/or the tone pairs themselves.
It remains an interesting question of why these "different" items, representing `r sprintf("%d or %.1f%%", length(tmp <- c("Bhx", "Bxh", "Hst", "Hts", "Ist", "Its", "Kkl", "Klk", "Llp", "Lpl")), length(tmp)/nrow(d_different_items)*100)`, behave differently from the other "different" and from virtually all the "same" items. 
Apprently, these "incorrect" reponses persisted even when some of the participants were provided with explicit feedback by the experimenters (Manxiang Wu, p.c.) suggesting that these perceptions are "real" and not due to inattention or fatigue.
Moreover, please note that I^st^ and H^st^ are specifically marked as "difficult" by the task creators, suggesting it is the tone pair ^st^ that is indeed hard to perceive as being different.

```{r}
# Let's define them:
weird_items <- c("Bhx1", "Bhx2", "Bxh1", "Bxh2", 
                 "Hst1", "Hst2", "Hts1", "Hts2", 
                 "Ist1", "Ist2", "Its1", "Its2", 
                 "Kkl1", "Kkl2", "Klk1", "Klk2", 
                 "Llp1", "Llp2", "Lpl1", "Lpl2");
```


#### Does the repetition matter?

```{r}
# For a given item, compare the presentations:
no_presentations <- vapply(unique(gsub("\\d", "", names(d_tone_wide)[2:ncol(d_tone_wide)])), function(s){ ss <- (names(d_tone_wide)[2:ncol(d_tone_wide)])[grep(paste0(s,"\\d"), names(d_tone_wide)[2:ncol(d_tone_wide)])]; max(as.numeric(substring(ss, nchar(ss), nchar(ss)))) }, numeric(1)); # the max number of presentations for each item
max_no_presentations <- max(no_presentations);
```

While most "same" and all "different" items are presented twice, there are some "same" items that are presented more times: `r paste0(names(s <- no_presentations[ no_presentations > 2 ])," (",s,")", collapse=", ")`, with the maximum number of presentations being `r max_no_presentations`.
So, the question is: "do later presentations differ from the earlier ones?".

```{r}
# Store the % correct, % disagreement and tetrachoric correlation with the previous presentation, and the % disagreement and tetrachoric correlation between the first and last presentations:
d_presentation_summaries <- do.call(rbind, lapply(seq_along(no_presentations), function(i)
{
  d <- matrix(NA, nrow=1, ncol=6*max_no_presentations); 
  rownames(d) <- names(no_presentations)[i]; 
  colnames(d) <- c("perc_cor_1", "n_cor_1", "n_inc_1", 
                   vapply(2:max_no_presentations, function(i) c(paste0("perc_cor_",i), paste0("n_cor_",i), paste0("n_inc_",i), paste0("diff_cor",i,"_",i-1), paste0("fisher_p",i,"_",i-1), paste0("rho_",i,"_",i-1)), character(6)), 
                   "diff_cor_last_1", "fisher_p_last_1", "rho_last_1");
  for(j in 1:max_no_presentations)
  {
    if( paste0(names(no_presentations)[i],j) %in% names(d_tone_wide) && !is.null(d1 <- d_tone_wide[ paste0(names(no_presentations)[i],j) ]) )
    {
      if( j==1 )
      {
        d[1] <- sum(d1)/nrow(d1);
        d[2] <- sum(d1); # n success
        d[3] <- sum(!d1); # n fail
      } else
      {
        d2 <- table(d_tone_wide[ , c(paste0(names(no_presentations)[i],j), paste0(names(no_presentations)[i],j-1)) ])
        d[4+(j-2)*6+0] <- sum(d1)/nrow(d1);
        d[4+(j-2)*6+1] <- sum(d1); # n success
        d[4+(j-2)*6+2] <- sum(!d1); # n fail
        d[4+(j-2)*6+3] <- d2[1,2] - d2[2,1];
        d[4+(j-2)*6+4] <- fisher.test(d2)$p.value;
        d[4+(j-2)*6+5] <- tetrachoric(d_tone_wide[ , c(paste0(names(no_presentations)[i],j), paste0(names(no_presentations)[i],j-1)) ])$rho[1,2];
      }
    }
  }
  # last:
  d2 <- table(d_tone_wide[ , c(paste0(names(no_presentations)[i],1), paste0(names(no_presentations)[i],no_presentations[i])) ])
  d[1,"diff_cor_last_1"] <- d2[1,2] - d2[2,1];
  d[1,"fisher_p_last_1"] <- fisher.test(d2)$p.value;
  d[1,"rho_last_1"] <- tetrachoric(d_tone_wide[ , c(paste0(names(no_presentations)[i],1), paste0(names(no_presentations)[i],no_presentations[i])) ])$rho[1,2]; 
  
  return (d);
}));
```

```{r, fig.width=3*2*2, fig.height=3*(sum(nchar(rownames(d_presentation_summaries)) == 2))/2, warning=FALSE, message=FALSE, echo=FALSE, fig.cap=capFig("Successive presentations for the 'same' items: % correct and tetrachoric correlation between the current and the previous presentation.")}
if( knitr::is_html_output() || knitr::is_latex_output() ) # running it directly in the console results in Error in plot.new() : figure margins too large
{
  par(mfrow=c(sum(nchar(rownames(d_presentation_summaries)) == 2)/2, 4));
  s <- which(nchar(rownames(d_presentation_summaries)) == 2); # same items
  for( i in s )
  {
    plot(d_presentation_summaries[i, grep("perc_cor_", colnames(d_presentation_summaries), fixed=TRUE)], ylim=c(0,1), type="b", xlab="Presentation", ylab="% correct", main=paste0("% correct: ",rownames(d_presentation_summaries)[i]), col="blue");
    plot(d_presentation_summaries[i, grep("rho_", colnames(d_presentation_summaries)[-ncol(d_presentation_summaries)], fixed=TRUE)], ylim=c(-1,1), type="b", xlab="Presentation & presentation + 1", ylab="Tetrachoric correlation rho", main=paste0("rho(i,i-1)", rownames(d_presentation_summaries)[i]), col="blue");
  }
  par(mfrow=c(1,1));
}
```

```{r, fig.width=3*4, fig.height=3*(sum(nchar(rownames(d_presentation_summaries)) == 3)/4), warning=FALSE, message=FALSE, echo=FALSE, fig.cap=capFig("Successive presentations for the 'different' items: % correct and tetrachoric correlation between the current and the previous presentation.")}
if( knitr::is_html_output() || knitr::is_latex_output() ) # running it directly in the console results in Error in plot.new() : figure margins too large
{
  par(mfrow=c(sum(nchar(rownames(d_presentation_summaries)) == 3)/4, 4));
  s <- which(nchar(rownames(d_presentation_summaries)) == 3); names(s) <- rownames(d_presentation_summaries)[s]; # same items
  ss <- s[vapply(s, function(i)
  { 
    if(i==1) return(TRUE);
    n1 <- rownames(d_presentation_summaries)[i]; n2 <- rownames(d_presentation_summaries)[1:(i-1)];
    n2 <- paste0(substring(n2,1,1), substring(n2,3,3), substring(n2,2,2)); # flip 2nd and 3rd character
    return (!(n1 %in% n2));
  }, logical(1))];
  for( i in names(ss) )
  {
    i1 <- i;
    i2 <- paste0(substring(i1,1,1), substring(i1,3,3), substring(i1,2,2));
    plot(d_presentation_summaries[i1, grep("perc_cor_", colnames(d_presentation_summaries), fixed=TRUE)], ylim=c(0,1), type="b", xlab="Presentation", ylab="% correct", main=paste0("% correct: ", i1, " (blue) and ", i2, " (red)"), col="blue");
    points(d_presentation_summaries[i2, grep("perc_cor_", colnames(d_presentation_summaries), fixed=TRUE)], type="b", col="red");
    plot(d_presentation_summaries[i1, grep("rho_", colnames(d_presentation_summaries)[-ncol(d_presentation_summaries)], fixed=TRUE)], ylim=c(-1,1), type="b", xlab="Presentation & presentation + 1", ylab="Tetrachoric correlation rho", main=paste0("rho(i,i-1): ", i1, " (blue) and ", i2, " (red)"), col="blue");
    points(d_presentation_summaries[i2, grep("rho_", colnames(d_presentation_summaries)[-ncol(d_presentation_summaries)], fixed=TRUE)], type="b", col="red");
  }
  par(mfrow=c(1,1));
}
```

```{r results='hide'}
# Make the % correct per item into the long format:
d <- cbind(as.data.frame(d_presentation_summaries), "item"=rownames(d_presentation_summaries), "same"=nchar(rownames(d_presentation_summaries)) == 2);
d_presentation_summaries_long <- rbind(setNames(cbind(d[,c("item", "same", "perc_cor_1", "n_cor_1", "n_inc_1")], "presentation"=1), c("item", "same", "perc.cor", "n_cor", "n_inc", "presentation")),
                                       setNames(cbind(d[,c("item", "same", "perc_cor_2", "n_cor_2", "n_inc_2")], "presentation"=2), c("item", "same", "perc.cor", "n_cor", "n_inc", "presentation")),
                                       setNames(cbind(d[,c("item", "same", "perc_cor_3", "n_cor_3", "n_inc_3")], "presentation"=3), c("item", "same", "perc.cor", "n_cor", "n_inc", "presentation")),
                                       setNames(cbind(d[,c("item", "same", "perc_cor_4", "n_cor_4", "n_inc_4")], "presentation"=4), c("item", "same", "perc.cor", "n_cor", "n_inc", "presentation")),
                                       setNames(cbind(d[,c("item", "same", "perc_cor_5", "n_cor_5", "n_inc_5")], "presentation"=5), c("item", "same", "perc.cor", "n_cor", "n_inc", "presentation")),
                                       setNames(cbind(d[,c("item", "same", "perc_cor_6", "n_cor_6", "n_inc_6")], "presentation"=6), c("item", "same", "perc.cor", "n_cor", "n_inc", "presentation"))) %>% 
  filter(!is.na(perc.cor)) %>% dplyr::mutate("type"=factor(ifelse(same,"same","different"), levels=c("same", "different")));

# See if the presentation matters for the % correct responses (beta regression):
d_presentation_summaries_long$perc.cor.beta <- prepare_for_beta_regression(d_presentation_summaries_long$perc.cor);
m0 <- glmmTMB(perc.cor.beta ~ presentation * same, data=d_presentation_summaries_long, family=beta_family(), control=beta_ctrl); summary(m0); DHARMa::testDispersion(m0, plot=FALSE); # there's a bit of overdispersion
m1 <- update(m0, . ~ . - presentation:same); summary(m1); anova(m0, m1); DHARMa::testDispersion(m1, plot=FALSE); # interaction does not matter (and there's a bit of overdispersion)
m2 <- update(m1, . ~ . - presentation); summary(m2); anova(m0, m2); DHARMa::testDispersion(m2, plot=FALSE); # presentation does not matter (and there's a bit of overdispersion)
m3 <- update(m2, . ~ . - same); summary(m3); anova(m0, m3); DHARMa::testDispersion(m3, plot=FALSE); # same clearly matters (and there's a lot of overdispersion)
# -> so the repretions does not seem to matter (but the same has better overall performance than different)

# Let's see if there is any difference between the items:
d_presentation_summaries_long$item <- as.factor(d_presentation_summaries_long$item); contrasts(d_presentation_summaries_long$item) <- contr.sum(length(levels(d_presentation_summaries_long$item)));
m4 <- glmmTMB(perc.cor.beta ~ item, data=d_presentation_summaries_long, family=beta_family(), control=beta_ctrl); summary(m4); DHARMa::testDispersion(m4, plot=FALSE); # no overdispersion
# -> almost all items differ significantly from the grand mean

# Separately for the same and different items as well:
m0s <- glmmTMB(perc.cor.beta ~ presentation, data=d_presentation_summaries_long[d_presentation_summaries_long$same,], family=beta_family(), control=beta_ctrl); summary(m0s); DHARMa::testDispersion(m0s, plot=FALSE); # nope, and a bit of overdispersion
m0d <- glmmTMB(perc.cor.beta ~ presentation, data=d_presentation_summaries_long[!d_presentation_summaries_long$same,], family=beta_family(), control=beta_ctrl); summary(m0d); DHARMa::testDispersion(m0d, plot=FALSE); # nope, and no overdispersion
```

We fitted a beta regression model (using `glmmTMB`) of the *% correct responses* on the interaction between *presentation number* (between 1 and a maximum of `r max(d_presentation_summaries_long$presentation)`, varying by item) and the *item type* ("same" or "different").
Manual model simplification shows that the interaction and the *presentation number* do not matter (`r sprintf("*χ*^2^(%d)=%.1f, *p*=%s, ΔAIC=%.1f", (a <- anova(m0, m2))[2,"Chi Df"], a[2,"Chisq"], scinot(a[2,"Pr(>Chisq)"]), a$AIC[1]-a$AIC[2])`), but *item type* does (`r sprintf("*χ*^2^(%d)=%.1f, *p*=%s, ΔAIC=%.1f", (a <- anova(m0, m3))[2,"Chi Df"], a[2,"Chisq"], scinot(a[2,"Pr(>Chisq)"]), a$AIC[1]-a$AIC[2])`), with the "same" itesm having an overall higher % of correct responses than the "different" items (`r sprintf("Δ%%correct = %.1f%%, *p*=%s", 100*(mean(d_presentation_summaries_long$perc.cor.beta[d_presentation_summaries_long$same]) - mean(d_presentation_summaries_long$perc.cor.beta[!d_presentation_summaries_long$same])), scinot(summary(m2)$coefficients$cond["sameTRUE","Pr(>|z|)"]))`).
(There is some overdispersion in the model, `r sprintf("%.2f, *p*=%s", (s <- DHARMa::testDispersion(m2, plot=FALSE))$statistic, s$p.value)`, but probably not sufficient to qualitatively change these results.)
The same lack of a significant effect for *presentation number* is shown separately for the 'same' items only (*p*=`r scinot(summary(m0s)$coefficients$cond["presentation","Pr(>|z|)"])`) and for the 'different' items only (*p*=`r scinot(summary(m0d)$coefficients$cond["presentation","Pr(>|z|)"])`).

```{r results='hide'}
# Does the order of the tone matter for the different items? Test their difference against 0 (the sign should be random as the order is arbitrary):
d_diffs_order <- d_presentation_summaries_long[ !d_presentation_summaries_long$same, c("item", "presentation", "perc.cor") ]; d_diffs_order$item <- as.character(d_diffs_order$item);
items <- unique(vapply(1:nrow(d_diffs_order), function(i) paste0(substring(d_diffs_order$item[i],1,1),paste0(sort(c(substring(d_diffs_order$item[i],3,3),substring(d_diffs_order$item[i],2,2))), collapse="")), character(1)));
d_diffs_order_items <- vapply(items, function(s)
{
  i1 <- (d_diffs_order$item == s);
  i2 <- (d_diffs_order$item == paste0(substring(s,1,1),substring(s,3,3),substring(s,2,2))); 
  sum(d_diffs_order$perc.cor[i1]) - sum(d_diffs_order$perc.cor[i2]);
}, numeric(1));
t.test(d_diffs_order_items, mu=0);
```

Likewise, it seems the order of the tones for the 'different' items does not seem to matter (one-sample t-test against 0 of the differences between the two orders across the items is `r sprintf("*t*(%d)=%.1f, *p*=%s", (s <- t.test(d_diffs_order_items, mu=0))$parameter, s$statistic, scinot(s$p.value))`).

Thus, there seems to be no systematic effects of successive presentations of the same item, or of the order of the two tones for the 'different' items, on the percent of correct answers.


### Item properties: Mokken Scaling Analysis (MSA)

```{r, fig.width=4, fig.height=4, warning=FALSE, message=FALSE, echo=FALSE, fig.cap=capFig("MSA: histogram of the number of Guttman errors (gPlus) across all items.")}
# Outliers:
xPlus   <- rowSums(d_tone_wide[,-1]);
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/gErrors.rds") ) # computationally expensive
{
  gErrors <- mokken::check.errors(d_tone_wide[,-1], returnGplus=TRUE, returnOplus=TRUE);
  saveRDS(gErrors, file=xzfile("./cached_results/gErrors.rds", compression=9));
} else
{
  gErrors <- readRDS(xzfile("./cached_results/gErrors.rds"));
}
gPlus   <- gErrors$Gplus;
oPlus   <- gErrors$Oplus;
hist(gPlus, main="Histogram of gPlus", xlab="gPlus (# of Guttman errors)");
# cor(cbind(oPlus, gPlus, xPlus))

# identify the outliers:
Q3 <- summary(gPlus)[[5]];
IQR <- Q3 - summary(gPlus)[[2]];
outlier <- gPlus > Q3 + 1.5 * IQR;
# if needed to further analyse ouliers
# cbind(mydata, gPlus)[outlier,]
# then possible sensitivity analysis:
# coefH(d_tone_wide[!outlier, -1])
```

There were `r sum(outlier)` cases (out of `r length(gPlus)`, i.e. `r round(100*sum(outlier)/length(gPlus), 1)`%) with a number of Guttman errors bigger than (Q3 + 1.5*IQR) = `r Q3 + 1.5*IQR`. 

```{r, warning=FALSE, message=FALSE, echo=FALSE, results='hide'}
# Compute scalability coefficients
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/Hvalues.rds") ) # computationally expensive
{
  Hvalues_full <- mokken::coefH(d_tone_wide[,-1], se=TRUE, ci=0.95);
  Hvalues <- list("Hi"=Hvalues_full$Hi, "H"=Hvalues_full$H); # keep only the info we actually need, as the full object is way too big
  saveRDS(Hvalues, file=xzfile("./cached_results/Hvalues.rds", compression=9));
} else
{
  Hvalues <- readRDS(xzfile("./cached_results/Hvalues.rds"));
}

# examine aisp for increasing c levels (run the function you defined above and give it a name)
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/motable.rds") ) # computationally expensive
{
  motable <- moscales.for.lowerbounds(d_tone_wide[,-1], ncores=n_cores);
  motable <- cbind("Items"=rownames(motable), motable);
  saveRDS(motable, file=xzfile("./cached_results/motable.rds", compression=9));
} else
{
  motable <- readRDS(xzfile("./cached_results/motable.rds"));
}

# if you need to view it
# View(aispmydata)
```

```{r results = 'asis', echo=FALSE}
d <- as.data.frame(cbind("Items"=rownames(Hvalues$Hi), Hvalues$Hi)); names(d) <- c("item", "H", "se", "95CI");
d$H <- as.numeric(trimws(d$H));
knitr::kable(d, row.names=FALSE, align="r", col.names=c("Items", "Item H", "se", "95% ci"),
             caption=capTab("MSA: Homogeneity values (with standard errors and 95%CIs) for all items, sorted by item name."))
knitr::kable(d %>% dplyr::arrange(desc(H)), row.names=FALSE, align="r", col.names=c("Items", "Item H", "se", "95% ci"),
             caption=capTab("Same table as bove but sorted by homogeneity."))
```

The complete item set has a homogeneity value *H* (se, 95%CI) of `r Hvalues$H`: this is significantly lower than the recommended 0.30, suggesting that the scale is *not* homogeneous.
This is further supported by the fact that few items have a homogeneity around or above this value (only `r sum(d$H >= 0.30)` if we consider the point estimate, and `r sum(vapply(strsplit(d[,"95CI"],",",fixed=TRUE), function(s) as.numeric(trimws(strsplit(s[2],"]",fixed=TRUE)[[1]][1])), numeric(1)) > 0.3)` out of `r nrow(d)` if we consider a 95%CI with an upper limit above 0.3).
Interestingly, the homogeneity of related items (different presentations and different orders of the tones) are overall very similar, suggesting again that this an intrinsic property of the segments and tone(s) and not of their repeated presentation or of the order of tones.

```{r results = 'asis', echo=FALSE}
knitr::kable(motable, row.names=FALSE, align="r",
             caption=capTab("MSA: *aisp* for increasing H thresholds (c) for all items."));
```

These results suggest that there probably are more than 1 scales, and that some items behave very similarly, but before deciding if we continue we continue in this direction, let's see what PCA/FA might suggest.


### PCA and FA

```{r include=FALSE}
# PCA and EFA across trials:
d <- d_tone_wide[,-1]; rownames(d) <- d_tone_wide$ID;

# PCA:
t_pca <- prcomp(d, scale.=TRUE, center=TRUE);
t_pca;
summary(t_pca);
#fviz_eig(t_pca);
#fviz_pca_var(t_pca,
#             col.var = "contrib", # Color by contributions to the PC
#             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
#             repel = TRUE     # Avoid text overlapping
#             );
# PC1 20%, PC2 6% -> probably 1 main factor, but also possibly a 2nd

# EFA:
# Tests of factorability (see https://towardsdatascience.com/exploratory-factor-analysis-in-r-e31b0015f224):
KMO(r=cor(d)); # Kaiser-Meyer-Olkin (KMO) sampling adequacy = 0.89 >> 0.60 -> ok
cortest.bartlett(d); # p=0 -> ok
det(cor(d)); # very close to 0 (7.519714e-64) -> there might be issues...

# Number of factors:
#fa.parallel(d, fa="fa", n.iter=100, error.bars=TRUE); # -> suggests 23 (!!!) factors, but the plot says 1 or 2 at most
#nfactors(d); # -> 1 or 2 factors, but the story is more complex (BIC says 9 or 10!)

t_fa_1f <- factanal(d, 1, rotation="promax"); t_fa_1f; fa.diagram(t_fa_1f$loadings); # pretty bad: let's try 2:
t_fa_2f <- factanal(d, 2, rotation="promax"); t_fa_2f; fa.diagram(t_fa_2f$loadings); # a bit better: let's try 3:
t_fa_3f <- factanal(d, 3, rotation="promax"); t_fa_3f; fa.diagram(t_fa_3f$loadings); # not better than 2FAs (and the 3rd is weird) -> let' settle on 2FAs
```

For **PCA**, PC~1~ explains `r round(100*(s <- summary(t_pca)$importance)["Proportion of Variance", "PC1"],1)`% of the variance, followed by PC~2~ which explains `r round(100*(s <- summary(t_pca)$importance)["Proportion of Variance", "PC2"],1)`%, suggesting there is a main factor on which most items load, but the story is bit more complex, with at least a 2nd factor needed (and a lot of variation remaining unexplained by these 2 components):

```{r fig.width=5, fig.height=4, fig.cap=capFig("Screeplot of the PCA of all the tone items together.")}
fviz_eig(t_pca);
```
```{r fig.width=11, fig.height=10, fig.cap=capFig("Loading of the tone items on the first 2 PCs.")}
fviz_pca_var(t_pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             );
```
```{r fig.width=5, fig.height=4, fig.cap=capFig("The participants plotted on the first 2 PCs, colored by their their qualities of representation (cos2).")}
fviz_pca_ind(t_pca,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             label = "none",
             repel = TRUE     # Avoid text overlapping
             );
```

The actual loadings on the first two PCs are:
```{r}
kable(round(t_pca$rotation[,1:2],2));
```

It can be seen that:

- the repetitions of the same item tend to have very similar loadings,
- PC1 which opposes Its, Bxh, Ist, Hst, Llp, Klk (all presentations and variants except for Kkl1 and Kkl2 which are close to 0.0; these are all "weird" items) to all the others (excluding Dhc1, Dhc2, Dch1, Kkl1 and Mc4 which are close to 0.0),
- PC2 opposes the 'same' to the 'different' stimuli.


For **EFA**, all the preliminary tests suggest that factor analysis is appropriate, with the possible exception of a determinant very close to 0.0 (`r sprintf("Kaiser-Meyer-Olkin = %.2f > 0.60; Bartlett's test is significant: *χ*^2^(%d)=%.1f, *p*=%.3g; and det(cor(data))=%.2g > 0", KMO(r=cor(d))$MSA, (tmp <- cortest.bartlett(d))$df, tmp$chisq, tmp$p.value, det(cor(d)))`).
However, when it comes to the best number of factors, the various methods diverge, but the overall story seems to be that 1 or 2 factors might be enough (but there seems to be a lot of variation beyond this as well, just as in the case of the PCA):

```{r fig.width=4, fig.height=4, fig.cap=capFig("Screeplot of the observed, simulated and randomized data with 1 standard deviation error bars (as generated by `fa.parallel())`."), results='hide'}
fa.parallel(d, fa="fa", n.iter=100, error.bars=TRUE);
```
```{r fig.width=4*2, fig.height=4*2, fig.cap=capFig("Number of factors as suggested by the VSS criterion (top left), the complexity of the solution (top right), BIC (bottom left) and Root Mean Residual (bottom right), as implemented by `nfactors()`."), results='hide'}
nfactors(d);
```

It seems that a 2FA model is better than a 1FA while a 3rd factor does not improve the fit much, but the fit to the data is far from perfect: 
```{r fig.width=15, fig.height=60, fig.cap=capFig("Loadings of the variables in the 2-factors model."), results='hide'}
fa.diagram(t_fa_2f$loadings);
```
and the actual loadings on the two factors are (showing only those ≥ 0.1 in absolute value):
```{r}
t_fa_2f$loadings;
```

It can be seen that:

- the repetitions of the same item tend to have very similar loadings,
- FA1 is composed of all the 'same' items (all presentations, and all with + loadings; please note that the actual sign is arbitrary but the differences in signs matter) and a few 'different' items: Bhx, Hst, Ist, Kkl and Llp (all presentations and variants, all with - loadings) -- it is interesting to note that all these items are "weird",
- FA2 is composed of most of the 'different' items: Akt, Bsv, Chv, Dch, Dcx, Ecl, Ecx, Elx, Fsv, Gtx, Icp, Iks, Ipv, Ipx, Isk, Jkx, Lkl, Lpv, Mch, Mcl, Mhc, Mhl, and Nkp1 (all presentations and variants, all with + loadings).

Thus, PC1 basically opposes the "weird" items to all the other items, while PC2 opposes the 'same' to the 'different' items.
Likewise, it seems that FA1 really captures the 'same' items and the "weird" 'different' items (but with opposite signs), while FA2 captures the "normal" 'different' items.

These observations prompt the question: are the "weird" items really of the 'same' type?


### Are the "weird" items really of the 'same' type?

All these together suggest the hypothesis that the "weird" stimuli, while *designed* as 'different', are, in fact, *perceived* as (albeit rather difficult) 'same' items by the participants.
If true, this hypothesis implies that coding them as such (i.e., in fact flipping their "correct" and "incorrect" responses) should align them with the other 'same' items.

```{r}
# Flip the "weird" items' responses (i.e., consider them as 'same'):
weird_items_same <- unique(unlist(lapply(unique(substring(weird_items,1,3)), # these are the 'same' items corresponding to the "weird" ones:
                                         function(s) names(d_tone_wide)[c(grep(paste0(substring(s,1,2),"\\d"), names(d_tone_wide)), # ... using the first tone...
                                                                          grep(paste0(substring(s,1,1),substring(s,3,3),"\\d"), names(d_tone_wide)))])))
d_tone_wide_flip <- d_tone_wide;
d_tone_wide_flip[ ,weird_items] <- (1 - d_tone_wide_flip[ ,weird_items]);
```


#### Percent correct responses

```{r results='hide'}
# Items order:
d_tone_wide_items_order_flip <- data.frame("Items"=names(d_tone_wide_flip)[-1],
                                      "YESCount"=apply(d_tone_wide_flip[,-1], 2, sum),
                                      "YESPercentage"=apply(d_tone_wide_flip[,-1], 2, function(x) sum(x)/length(x)));
d_tone_wide_items_order_flip <- d_tone_wide_items_order_flip[ order(d_tone_wide_items_order_flip$YESPercentage), ];
```

```{r}
kable(d_tone_wide_items_order_flip %>% dplyr::mutate(YESPercentage = sprintf("%.1f%%",YESPercentage*100)), 
      row.names=FALSE, col.names=c("Item (short name)", "# correct reponses", "% correct reponses"), align=c("l", "r", "r"), 
      caption=capTab("'Flipped' weird items: Frequencies of 'yes' responses (the items are ordered by % correct responses)."));
```

```{r, fig.width=8, fig.height=40, warning=FALSE, message=FALSE, echo=FALSE, fig.cap=capFig("'Flipped' weird items: Endorsement frequencies by item (items ordered by % correct responses).")}
par(oma=c(0,3,0,0))
barplot(d_tone_wide_items_order_flip[,"YESCount"], 
        horiz = TRUE,
        main = "Endorsement frequencies",
        ylab="", 
        xlab="Number of respondents", 
        cex.lab=0.8,
        cex.axis=0.5,
        names.arg=d_tone_wide_items_order_flip[, "Items"], 
        las=2, 
        cex.names=0.8);
```


#### Correlations between items

```{r, fig.width=25, fig.height=25, warning=FALSE, message=FALSE, echo=FALSE, fig.cap=capFig("'Flipped' weird items: Correlation matrix between items.")}
# prepare correlation matrix
# for binary items, use tetrachoric correlation matrix
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/tetrachoric_flip.rds") ) # computationally expensive
{
  bluesqs_flip <- as.data.frame(tetrachoric(d_tone_wide_flip[,-1])["rho"]);
  saveRDS(bluesqs_flip, file=xzfile("./cached_results/tetrachoric_flip.rds", compression=9));
} else
{
  bluesqs_flip <- readRDS(xzfile("./cached_results/tetrachoric_flip.rds"));
}
# heat plot of correlations matrix 
# uncomment the png & devoff lines if you want to save as png in the working directory
if( knitr::is_html_output() || knitr::is_latex_output() ) # running it directly in the console results in Error in plot.new() : figure margins too large
{
  # png('corplot_flip.png')
  cor.plot(bluesqs_flip, numbers=FALSE, main="'Flipped' weird items: Correlations between items", 
           labels=rownames(bluesqs), las=2,
           cex=1.0, cex.axis=1.0)
  # dev.off()
}
```

It can be seen that the tetrachoric correlations between the items are rather low, varying between `r round(min(as.numeric(bluesqs_flip[upper.tri(bluesqs_flip)])),2)` and  `r round(max(as.numeric(bluesqs_flip[upper.tri(bluesqs_flip)])),2)`, with a mean of  `r round(mean(as.numeric(bluesqs_flip[upper.tri(bluesqs_flip)])),2)`, a median of  `r round(median(as.numeric(bluesqs_flip[upper.tri(bluesqs_flip)])),2)`, sd of  `r round(sd(as.numeric(bluesqs_flip[upper.tri(bluesqs_flip)])),2)` and IQR of  `r round(IQR(as.numeric(bluesqs_flip[upper.tri(bluesqs_flip)])),2)`:
```{r, fig.width=6, fig.height=4, warning=FALSE, message=FALSE, echo=FALSE, fig.cap=capFig("Histogram of the tetracoric correlations between different itmes.")}
hist(as.numeric(bluesqs_flip[upper.tri(bluesqs_flip)]), main="Histogram of the tetracoric correlations", xlab="Tetracoric correlation", xlim=c(-1,1));
```

```{r, fig.width=5, fig.height=25, warning=FALSE, message=FALSE, echo=FALSE, fig.cap=capFig("'Flipped' weird items: Hierarchical clustering of the items using 1 - tetrachoric correlations.")}
items_clust <- hclust(as.dist(1-bluesqs_flip));
par(cex=0.6, mar=c(5, 8, 4, 1));
plot(as.dendrogram(items_clust), horiz=TRUE);
```

```{r, fig.width=5, fig.height=40, warning=FALSE, message=FALSE, echo=FALSE, fig.cap=capFig("'Flipped' weird items: Mean correlation with the other items.")}
# look at the items that tend to have negative correlations:
bluesqs_flip_mean <- apply(bluesqs_flip, 1, mean);
par(oma=c(0,3,0,0))
barplot(bluesqs_flip_mean, 
        horiz = TRUE,
        col = ifelse(bluesqs_flip_mean < 0, "blue", "red"),
        main = "Mean correlation with the other items",
        ylab="", 
        xlab="Mean tetrachoric correlation", 
        cex.lab=0.8,
        cex.axis=0.5,
        names.arg=names(bluesqs_flip_mean), 
        las=2, 
        cex.names=0.8);
```


#### PCA and FA

```{r include=FALSE}
# PCA and EFA across trials:
d <- d_tone_wide_flip[,-1]; rownames(d) <- d_tone_wide_flip$ID;

# PCA:
t_pca_flip <- prcomp(d, scale.=TRUE, center=TRUE);
t_pca_flip;
summary(t_pca_flip);
#fviz_eig(t_pca_flip);
#fviz_pca_var(t_pca_flip,
#             col.var = "contrib", # Color by contributions to the PC
#             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
#             repel = TRUE     # Avoid text overlapping
#             );
# PC1 20%, PC2 5.5% -> probably 1 main factor, but also possibly a 2nd

# EFA:
# Tests of factorability (see https://towardsdatascience.com/exploratory-factor-analysis-in-r-e31b0015f224):
KMO(r=cor(d)); # Kaiser-Meyer-Olkin (KMO) sampling adequacy = 0.89 >> 0.60 -> ok
cortest.bartlett(d); # p=0 -> ok
det(cor(d)); # very close to 0 (7.519714e-64) -> there might be issues...

# Number of factors:
#fa.parallel(d, fa="fa", n.iter=100, error.bars=TRUE); # -> suggests 23 (!!!) factors, but the plot says 1 or 2 at most
#nfactors(d); # -> 1 or 2 factors, but the story is more complex (BIC says 9 or 10!)

t_fa_1f_flip <- factanal(d, 1, rotation="promax"); t_fa_1f_flip; fa.diagram(t_fa_1f_flip$loadings); # pretty bad: let's try 2:
t_fa_2f_flip <- factanal(d, 2, rotation="promax"); t_fa_2f_flip; fa.diagram(t_fa_2f_flip$loadings); # a bit better: let's try 3:
t_fa_3f_flip <- factanal(d, 3, rotation="promax"); t_fa_3f_flip; fa.diagram(t_fa_3f_flip$loadings); # not better than 2FAs (and the 3rd is weird) -> let' settle on 2FAs
```

For **PCA**, PC~1~ explains `r round(100*(s <- summary(t_pca_flip)$importance)["Proportion of Variance", "PC1"],1)`% of the variance, followed by PC~2~ which explains `r round(100*(s <- summary(t_pca_flip)$importance)["Proportion of Variance", "PC2"],1)`%, suggesting there is a main factor on which most items load, but the story is bit more complex, with at least a 2nd factor needed (and a lot of variation remaining unexplained by these 2 components):

```{r fig.width=5, fig.height=4, fig.cap=capFig("'Flipped' weird items: Screeplot of the PCA of all the tone items together.")}
fviz_eig(t_pca_flip);
```
```{r fig.width=11, fig.height=10, fig.cap=capFig("'Flipped' weird items: Loading of the tone items on the first 2 PCs.")}
fviz_pca_var(t_pca_flip,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             );
```
```{r fig.width=6, fig.height=4, fig.cap=capFig("'Flipped' weird items: The participants plotted on the first 2 PCs, colored by their their qualities of representation (cos2).")}
fviz_pca_ind(t_pca_flip,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             label = "none",
             repel = TRUE     # Avoid text overlapping
             );
```

The actual loadings on the first two PCs are:
```{r}
kable(round(t_pca_flip$rotation[,1:2],2));
```

It can be seen that:

- the repetitions of the same item tend to have very similar loadings,
- PC1 is harder to interpret but seems to oppose the vast majority of the items to basically Kkl and Dhc (which have loadings close to 0), while
- PC2 opposes the 'same' (including the 'flipped' items) to the 'different' items.


For **EFA**, all the preliminary tests suggest that factor analysis is appropriate, with the possible exception of a determinant very close to 0.0 (`r sprintf("Kaiser-Meyer-Olkin = %.2f > 0.60; Bartlett's test is significant: *χ*^2^(%d)=%.1f, *p*=%.3g; and det(cor(data))=%.2g > 0", KMO(r=cor(d))$MSA, (tmp <- cortest.bartlett(d))$df, tmp$chisq, tmp$p.value, det(cor(d)))`).
However, when it comes to the best number of factors, the various methods diverge, but the overall story seems to be that 1 or 2 factors might be enough (but there seems to be a lot of variation beyond this as well, just as in the case of the PCA):

```{r fig.width=4, fig.height=4, fig.cap=capFig("'Flipped' weird items: Screeplot of the observed, simulated and randomized data with 1 standard deviation error bars (as generated by `fa.parallel())`."), results='hide'}
fa.parallel(d, fa="fa", n.iter=100, error.bars=TRUE);
```
```{r fig.width=4*2, fig.height=4*2, fig.cap=capFig("'Flipped' weird items: Number of factors as suggested by the VSS criterion (top left), the complexity of the solution (top right), BIC (bottom left) and Root Mean Residual (bottom right), as implemented by `nfactors()`."), results='hide'}
nfactors(d);
```

It seems that a 2FA model is better than a 1FA while a 3rd factor does not improve the fit much, but the fit to the data is far from perfect: 
```{r fig.width=15, fig.height=60, fig.cap=capFig("'Flipped' weird items: Loadings of the variables in the 2-factors model."), results='hide'}
fa.diagram(t_fa_2f_flip$loadings);
```
and the actual loadings on the two factors are (showing only those ≥ 0.1 in absolute value):
```{r}
t_fa_2f_flip$loadings;
```

It can be seen that:

- the repetitions of the same item tend to have very similar loadings,
- FA1 is composed of all the 'same' items, including the "flipped" items Bhx, Hst, Ist, Kkl and Llp (all presentations, and all with loading of the same sign),
- FA2 is composed of all the 'different' items (all presentations and variants, all with the same sign).

Thus, it seems that the main difference now is between the ("extended") 'same' and the 'different' items, which seems to make much more sense.


#### MSA

Returning to the Mokken analysis:

```{r, warning=FALSE, message=FALSE, echo=FALSE, results='hide'}
# Compute scalability coefficients
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/Hvalues_flip.rds") ) # computationally expensive
{
  Hvalues_full_flip <- mokken::coefH(d_tone_wide_flip[,-1], se=TRUE, ci=0.95);
  Hvalues_flip <- list("Hi"=Hvalues_full_flip$Hi, "H"=Hvalues_full_flip$H); # keep only the info we actually need, as the full object is way too big
  saveRDS(Hvalues_flip, file=xzfile("./cached_results/Hvalues_flip.rds", compression=9));
} else
{
  Hvalues_flip <- readRDS(xzfile("./cached_results/Hvalues_flip.rds"));
}

# examine aisp for increasing c levels (run the function you defined above and give it a name)
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/motable_flip.rds") ) # computationally expensive
{
  motable_flip <- moscales.for.lowerbounds(d_tone_wide_flip[,-1], ncores=n_cores);
  motable_flip <- cbind("Items"=rownames(motable_flip), motable_flip)
  saveRDS(motable_flip, file=xzfile("./cached_results/motable_flip.rds", compression=9));
} else
{
  motable_flip <- readRDS(xzfile("./cached_results/motable_flip.rds"));
}
```

```{r results = 'asis', echo=FALSE}
d <- as.data.frame(cbind("Items"=rownames(Hvalues_flip$Hi), Hvalues_flip$Hi)); names(d) <- c("item", "H", "se", "95CI");
d$H <- as.numeric(trimws(d$H));
knitr::kable(d, row.names=FALSE, align="r", col.names=c("Items", "Item H", "se", "95% ci"),
             caption=capTab("'Flipped' weird items: MSA: Homogeneity values (with standard errors and 95%CIs) for all items, sorted by item name."))
knitr::kable(d %>% dplyr::arrange(desc(H)), row.names=FALSE, align="r", col.names=c("Items", "Item H", "se", "95% ci"),
             caption=capTab("Same table as bove but sorted by homogeneity."))
```

The complete item set has a homogeneity value *H* (se, 95%CI) of `r Hvalues_flip$H`: this is significantly lower than the recommended 0.30, suggesting that the scale is *not* homogeneous.
This is further supported by the fact that few items have a homogeneity around or above this value (only `r sum(d$H >= 0.30)` if we consider the point estimate, and `r sum(vapply(strsplit(d[,"95CI"],",",fixed=TRUE), function(s) as.numeric(trimws(strsplit(s[2],"]",fixed=TRUE)[[1]][1])), numeric(1)) > 0.3)` out of `r nrow(d)` if we consider a 95%CI with an upper limit above 0.3).
Interestingly, the homogeneity of related items (different presentations and different orders of the tones) are overall very similar, suggesting again that this an intrinsic property of the segments and tone(s) and not of their repeated presentation of order of tones.

```{r results = 'asis', echo=FALSE}
knitr::kable(motable_flip, row.names=FALSE, align="r",
             caption=capTab("'Flipped' weird items: MSA: *aisp* for increasing H thresholds (c) for all items."));
```

The results are slightly better, but still far from ideal for IRT...

#### Conclusions

So, it seems that the "weird" stimuli Bhx, Hst, Ist, Kkl1 and Llp (all presentations and variants) are perceived by the participants as (rather difficult) 'same'-type items and not as the intended 'different'-type items.
With this change, the items seem to fall into the two natural classes 'same' vs 'different' (even if there is a lot of unaccounted variation).


### Let's reduce the items set

The MSA suggests that there are too many items, and it seems clear that the successive presentations of the same item do not seem to make a difference and, for the (by design) 'different' items, the order of the tones does not seem to mater as well.
If this is so, we can reduce the items set by:

- keeping systematically the 2^nd^ presentation,
- and, for the 'different' items, only one order of tones (say, the 1^st^ in alphabetical order).

With these:

```{r}
d_tone_wide_flip_red <- d_tone_wide_flip[,c("ID", 
                                            "Ak1", "Akt1", "At1", 
                                            "Bh1", "Bhx1", "Bs1",  "Bsv1", "Bv1",  "Bx1", 
                                            "Ch1", "Chv1", "Cv1", 
                                            "Dc1", "Dch1", "Dcx1", "Dh1",  "Dx1", 
                                            "Ec1", "Ecl1", "Ecx1", "El1",  "Elx1", "Ex1", 
                                            "Fs1", "Fsv1", "Fv1", 
                                            "Gt1", "Gtx1", "Gx1", 
                                            "Hs1", "Hst1", "Ht1", 
                                            "Ic1", "Icp1", "Ik1",  "Iks1", "Ip1",  "Ipv1", "Ipx1", "Is1", "Ist1", "It1", "Iv1", "Ix1", 
                                            "Jk1", "Jkx1", "Jx1", 
                                            "Kk1", "Kkl1", "Kl1", 
                                            "Lk1", "Lkl1", "Ll1",  "Llp1", "Lp1",  "Lpv1", "Lv1", 
                                            "Mc1", "Mch1", "Mcl1", "Mh1",  "Mhl1", "Ml1", 
                                            "Nk1", "Nkp1", "Np1")];
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, results='hide'}
# Compute scalability coefficients
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/Hvalues_flip_red.rds") ) # computationally expensive
{
  Hvalues_full_flip_red <- mokken::coefH(d_tone_wide_flip_red[,-1], se=TRUE, ci=0.95);
  Hvalues_flip_red <- list("Hi"=Hvalues_full_flip_red$Hi, "H"=Hvalues_full_flip_red$H); # keep only the info we actually need, as the full object is way too big
  saveRDS(Hvalues_flip_red, file=xzfile("./cached_results/Hvalues_flip_red.rds", compression=9));
} else
{
  Hvalues_flip_red <- readRDS(xzfile("./cached_results/Hvalues_flip_red.rds"));
}

# examine aisp for increasing c levels (run the function you defined above and give it a name)
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/motable_flip_red.rds") ) # computationally expensive
{
  motable_flip_red <- moscales.for.lowerbounds(d_tone_wide_flip_red[,-1], ncores=n_cores);
  motable_flip_red <- cbind("Items"=rownames(motable_flip_red), motable_flip_red)
  saveRDS(motable_flip_red, file=xzfile("./cached_results/motable_flip_red.rds", compression=9));
} else
{
  motable_flip_red <- readRDS(xzfile("./cached_results/motable_flip_red.rds"));
}
```

```{r results = 'asis', echo=FALSE}
d <- as.data.frame(cbind("Items"=rownames(Hvalues_flip_red$Hi), Hvalues_flip_red$Hi)); names(d) <- c("item", "H", "se", "95CI");
d$H <- as.numeric(trimws(d$H));
knitr::kable(d, row.names=FALSE, align="r", col.names=c("Items", "Item H", "se", "95% ci"),
             caption=capTab("'Flipped' weird items: MSA: Homogeneity values (with standard errors and 95%CIs) for all items, sorted by item name."))
knitr::kable(d %>% dplyr::arrange(desc(H)), row.names=FALSE, align="r", col.names=c("Items", "Item H", "se", "95% ci"),
             caption=capTab("Same table as bove but sorted by homogeneity."))
```

The complete item set has a homogeneity value *H* (se, 95%CI) of `r Hvalues_flip_red$H`: this is significantly lower than the recommended 0.30, suggesting that the scale is *not* homogeneous.
This is further supported by the fact that few items have a homogeneity around or above this value (only `r sum(d$H >= 0.30)` if we consider the point estimate, and `r sum(vapply(strsplit(d[,"95CI"],",",fixed=TRUE), function(s) as.numeric(trimws(strsplit(s[2],"]",fixed=TRUE)[[1]][1])), numeric(1)) > 0.3)` out of `r nrow(d)` if we consider a 95%CI with an upper limit above 0.3).
Interestingly, the homogeneity of related items (different presentations and different orders of the tones) are overall very similar, suggesting again that this an intrinsic property of the segments and tone(s) and not of their repeated presentation of order of tones.

```{r results = 'asis', echo=FALSE}
knitr::kable(motable_flip_red, row.names=FALSE, align="r",
             caption=capTab("'Flipped' weird items: MSA: *aisp* for increasing H thresholds (c) for all items."));
```

Let's iteratively remove the unscalable items for c ≤ 0.30:
```{r, warning=FALSE, message=FALSE, echo=FALSE, results='hide'}
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/motable_flip_red2.rds") ) # computationally expensive
{
  removed_items <- c();
  while(TRUE)
  {
    motable_flip_red2 <- moscales.for.lowerbounds(d_tone_wide_flip_red[,-which(names(d_tone_wide_flip_red) %in% c("ID", removed_items))], ncores=n_cores);
    motable_flip_red2 <- motable_flip_red2[,1:6]; # c <= 0.30
    if( sum(motable_flip_red2 == 0) == 0 ) break; # no items with value 0 (unscalable items) found!
    for(i in 1:ncol(motable_flip_red2))
    {
      if( sum(s <- motable_flip_red2[,i] == 0) > 0 )
      {
        removed_items <- c(removed_items, rownames(motable_flip_red2)[which(s)[1]]);
        cat("  > removing ",rownames(motable_flip_red2)[which(s)[1]],"\n");
        break;
      }
    }
    if( length(setdiff(names(d_tone_wide_flip_red), c("ID", removed_items))) == 0 ) break; # all items have been removed
  }
  motable_flip_red2 <- cbind("Items"=rownames(motable_flip_red2), motable_flip_red2);
  #motable_flip_red2;
  saveRDS(list("motable_flip_red2"=motable_flip_red2, "removed_items"=removed_items), file=xzfile("./cached_results/motable_flip_red2.rds", compression=9));
} else
{
  tmp  <- readRDS(xzfile("./cached_results/motable_flip_red2.rds"));
  motable_flip_red2 <- tmp$motable_flip_red2; removed_items <- tmp$removed_items;
}

d_tone_wide_irt_subscale <- d_tone_wide_flip_red[,c("ID", rownames(motable_flip_red2))];
HvaluesSubscale1 <- mokken::coefH(d_tone_wide_irt_subscale[,-1], ci=0.95); # check H 
```

After removing `r length(removed_items)` items (`r paste0(removed_items,collapse=", ")`), we are left with the `r nrow(motable_flip_red2)` items (`r paste0(rownames(motable_flip_red2),collapse=", ")`), covering both 'same' and 'different' items, which seem to form a single scale (more or less, especially at c = 0.30):

```{r results = 'asis', echo=FALSE}
knitr::kable(motable_flip_red2, row.names=FALSE, align="r",
             caption=capTab("MSA: *aisp* for increasing H thresholds (c) after retaning only 1 item per class and after removing the unscalable itmes."));
```

and the subscale's H is now a much better `r HvaluesSubscale1$H`:

```{r}
kable(HvaluesSubscale1$Hi, caption="Homogeneity for the kept subscale.");
```

```{r, include=FALSE, warning=FALSE, echo=FALSE, message=FALSE}
# check conditional association (local independence)
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/cond_ind.rds") ) # computationally expensive
{
  CA.def.mysubscale1 <- check.ca(d_tone_wide_irt_subscale[,-1], TRUE);
  CA.def.mysubscale1$InScale
  CA.def.mysubscale1$Index
  CA.def.mysubscale1$Flagged
  saveRDS(CA.def.mysubscale1, file=xzfile("./cached_results/cond_ind.rds", compression=9));
} else
{
  CA.def.mysubscale1 <- readRDS(xzfile("./cached_results/cond_ind.rds"));
}

removed_items_ca <- names(d_tone_wide_irt_subscale[,-1])[!CA.def.mysubscale1$InScale[[ length(CA.def.mysubscale1$InScale) ]]];

d_tone_wide_irt_subscale <- d_tone_wide_irt_subscale[, -which(names(d_tone_wide_irt_subscale) %in% removed_items_ca) ];
```

`r length(removed_items_ca)` items do not meet the **local independence** criterion (`r paste0(removed_items_ca, collapse=", ")`) and are excluded from the analysis, leaving the `r ncol(d_tone_wide_irt_subscale)-1` items `r paste0(names(d_tone_wide_irt_subscale)[-1], collapse=", ")`.

```{r, warning=FALSE, echo=FALSE, message=FALSE}
# check monotonicity with default minsize:
monotonicity.def.mysubscale2 <- check.monotonicity(d_tone_wide_irt_subscale[,-1], minvi=.03);
#plot(monotonicity.def.mysubscale2)
```

**Monotonicity** tests for the remaining items are shown below for default minsize:

```{r results = 'asis', echo=FALSE}
knitr::kable(summary(monotonicity.def.mysubscale2), row.names=TRUE, caption=capTab("Monotonicity."));
```

```{r, warning=FALSE, echo=FALSE, message=FALSE}
# Investigate the assumption of non-intersecting item step response functions (ISRFs) 
# or using rest score (for binary items)
restscore.mysubscale2 <- check.restscore(d_tone_wide_irt_subscale[,-1]);
# several other options are available in mokken: pmatrix, mscpm, and IT
```

**Invariant item ordering (IIO)** tests are shown below for default minsize:

```{r results = 'asis', echo=FALSE}
knitr::kable(summary(restscore.mysubscale2), row.names=TRUE, caption=capTab("Invariant item ordering (IIO)."));
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, results='hide'}
HvaluesSubscale2 <- mokken::coefH(d_tone_wide_irt_subscale[,-1], ci=0.95); # check H 
```

and the subscale's H is now a much better `r HvaluesSubscale2$H`:

```{r}
kable(HvaluesSubscale2$Hi, caption="Homogeneity for the kept subscale.");
```

However, it can be seen that this subscale is composed entirely of 'same' items, which suggests that IRT/Mokken are not appropriate for analyzing such data, but they were, nevertheless, extremely useful in helping to detect the "weird" items.


### Rewind and restart: dealing with the "weird" items

Given the above analyses, it seems that the optimal way forward is:

1. the **"weird" items** (Bhx, Hst, Ist, Kkl1 and Llp, in all presentations and variants) require some sort of "special treatment", prompting us to perform *three independent analyses* using identical methods, as follows:
    - **keeping** these items as they are (denoted as "original"),
    - **removing** these items (denoted as "removed"), and
    - **recoding** them as 'same' items (denoted as "recoded").
  
2. summarizing the participants' performance on this task as:
    - the **% of correct responses across all items and presentations** (this is a very simple, natural and frequently used approach),
    - as suggested by the PCA and EFA analyses, we estimate the **% of correct responses *separately* for the 'same' and 'different' items**, and
    - using Signal Detection Theory [@macmillan_detection_1991], we estimate the **sensitivity** and the **bias**.


#### The three datasets

```{r}
# We'll work with the wide format here:

# original:
d_tone_wide_original <- d_tone_wide; 
item_types_original  <- list("same"=sort(names(d_tone_wide_original)[nchar(names(d_tone_wide_original)) == 3]), "different"=sort(names(d_tone_wide_original)[nchar(names(d_tone_wide_original)) == 4]));

# removed:
d_tone_wide_removed  <- d_tone_wide[, !(names(d_tone_wide) %in% weird_items) ]; 
item_types_removed   <- list("same"=sort(names(d_tone_wide_removed)[nchar(names(d_tone_wide_removed)) == 3]), "different"=sort(names(d_tone_wide_removed)[nchar(names(d_tone_wide_removed)) == 4]));

# recoded:
d_tone_wide_recoded  <- d_tone_wide_flip; 
item_types_recoded   <- list("same"=sort(union(item_types_original$same, weird_items)), "different"=sort(setdiff(item_types_original$different, weird_items)));
```

These three tone datasets all have the same number of participants (`r nrow(d_tone_wide_original)`, as we did not yet remove any outliers), but different number of items: `r ncol(d_tone_wide_original)-1` (original), `r ncol(d_tone_wide_removed)-1` (removed), and `r ncol(d_tone_wide_recoded)-1` (recoded) respectively.

As a reminder, there are `r length(weird_items)` "weird" items in total (`r paste0(weird_items, collapse=", ")`).


#### The three performance estimates

```{r}
d_all$ID <- as.character(d_all$ID); # make sure these are characters
```


##### % of correct responses across all items and presentations

```{r}
# Compute total % correct for each dataset:
pc_total <- function(d)
{
  data.frame("ID"=d$ID, 
             "pc_total"=vapply(1:nrow(d), function(i) mean(as.numeric(d[i,-1]), na.rm=TRUE), numeric(1)));
}
d_all <- merge(d_all, pc_total(d_tone_wide_original), by="ID", all.x=TRUE, all.y=FALSE); names(d_all)[ncol(d_all)] <- "pc_tot_orig";
d_all <- merge(d_all, pc_total(d_tone_wide_removed),  by="ID", all.x=TRUE, all.y=FALSE); names(d_all)[ncol(d_all)] <- "pc_tot_remv";
d_all <- merge(d_all, pc_total(d_tone_wide_recoded),  by="ID", all.x=TRUE, all.y=FALSE); names(d_all)[ncol(d_all)] <- "pc_tot_recd";
```
```{r fig.width=12, fig.height=4, fig.cap=capFig("Histogram of the % correct on each of the three datasets.")}
par(mfrow=c(1,3));
hist(100*d_all$pc_tot_orig, xlab="% correct (original)", main="Original", xlim=c(0,100));
hist(100*d_all$pc_tot_remv, xlab="% correct (removed)", main="Removed", xlim=c(0,100));
hist(100*d_all$pc_tot_recd, xlab="% correct (recoded)", main="Recoded", xlim=c(0,100));
par(mfrow=c(1,1));
```
```{r fig.width=8, fig.height=8, fig.cap=capFig("Heatmap with clustering for the Spearman correlations between % correct total across the three datasets.")}
# Correlations:
tmp <- cor(d_all[,c("pc_tot_orig", "pc_tot_remv", "pc_tot_recd")], method="spearman");
pander(round(tmp,2));
gplots::heatmap.2(tmp, cellnote=round(tmp,2), notecol="black", margins=c(12,12), 
                  labRow=c("original", "remove", "recode"), 
                  labCol=c("original", "remove", "recode"));
```

The correlations between the total % correct across the three datasets are very large (≥0.90) suggesting that we can simply use any one of them.


##### % of correct responses for 'same' and 'different' itmes

```{r}
# Compute % correct for 'same' and 'different' items for each dataset:
pc_same_different <- function(d, item_types)
{
  data.frame("ID"=d$ID, 
             "pc_same"=vapply(1:nrow(d), function(i) mean(as.numeric(d[i,item_types$same]), na.rm=TRUE), numeric(1)), 
             "pc_diff"=vapply(1:nrow(d), function(i) mean(as.numeric(d[i,item_types$different]), na.rm=TRUE), numeric(1)));
}
d_all <- merge(d_all, pc_same_different(d_tone_wide_original, item_types_original), by="ID", all.x=TRUE, all.y=FALSE); names(d_all)[ncol(d_all)+(-1:0)] <- c("pc_same_orig", "pc_diff_orig");
d_all <- merge(d_all, pc_same_different(d_tone_wide_removed,  item_types_removed),  by="ID", all.x=TRUE, all.y=FALSE); names(d_all)[ncol(d_all)+(-1:0)] <- c("pc_same_remv", "pc_diff_remv");
d_all <- merge(d_all, pc_same_different(d_tone_wide_recoded,  item_types_recoded),  by="ID", all.x=TRUE, all.y=FALSE); names(d_all)[ncol(d_all)+(-1:0)] <- c("pc_same_recd", "pc_diff_recd");
```
```{r fig.width=12, fig.height=8, fig.cap=capFig("Histogram of the % correct 'same' (top) and 'different' (bottom) on each of the three datasets.")}
par(mfcol=c(2,3));
hist(100*d_all$pc_same_orig, xlab="% correct (same)", main="Original", xlim=c(0,100));
hist(100*d_all$pc_diff_orig, xlab="% correct (different)", main="Original", xlim=c(0,100));
hist(100*d_all$pc_same_remv, xlab="% correct (same)", main="Removed", xlim=c(0,100));
hist(100*d_all$pc_diff_remv, xlab="% correct (different)", main="Removed", xlim=c(0,100));
hist(100*d_all$pc_same_recd, xlab="% correct (same)", main="Recoded", xlim=c(0,100));
hist(100*d_all$pc_diff_recd, xlab="% correct (different)", main="Recoded", xlim=c(0,100));
par(mfrow=c(1,1));
```
```{r fig.width=8, fig.height=8, fig.cap=capFig("Heatmap with clustering for the Spearman correlations between % correct for 'same' and 'different' itmes across the three datasets.")}
# Correlations:
tmp <- cor(d_all[,c("pc_same_orig", "pc_diff_orig", "pc_same_remv", "pc_diff_remv", "pc_same_recd", "pc_diff_recd")], method="spearman");
pander(round(tmp,2));
gplots::heatmap.2(tmp, cellnote=round(tmp,2), notecol="black", margins=c(12,12),
                  labRow=c("same (original)", "different (original)", "same (remove)", "different (remove)", "same (recode)", "different (recode)"), 
                  labCol=c("same (original)", "different (original)", "same (remove)", "different (remove)", "same (recode)", "different (recode)"));
```

It is a similar story here, with the % correct for the 'same' items being highly intercorrelated (≥0.94) as are those for the 'different' items (≥0.97), but the correlations between these two types are low (between 0.09 and 0.35) with the lowest for the 'original' dataset; therefore we will use the results for the 'remove' (or the 'recode') dataset.


##### Signal Detection Theory (sensitivity and bias)

We estimate the following measures (using `psycho::dprime()`, see @pallier_computing_2002, [https://bookdown.org/danbarch/psy_207_advanced_stats_I/signal-detection-theory.html](https://bookdown.org/danbarch/psy_207_advanced_stats_I/signal-detection-theory.html) and [https://www.birmingham.ac.uk/Documents/college-les/psych/vision-laboratory/sdtintro.pdf](https://www.birmingham.ac.uk/Documents/college-les/psych/vision-laboratory/sdtintro.pdf) for details):

- **d'** and **beta** (or  **c**): the "classic" measures of sensitivity (d') and bias (beta):
  + d' is theoretically positive and varies between 0.0 (no sensitivity) to +∞ (in practice, up to ~3), with higher values representing higher sensitivity (please note that it is possible to obtain small negative estimates when there is no capacity to detect the signal but this should be treated as essentially 0.0)
  + beta varies between 0.0 (bias towards 'different') to +∞ (bias towards 'same'), where 1.0 represents an unbiased observer, 
  + c is an alternative estimator of the bias representing the number of standard deviations from the midpoint between the 'signal, and the 'signal + noise' distributions, and is a symmetric continuum from negative values (preferring 'different' responses), to 0 ("unbiased") and to positive values (preferring 'same' responses);
- **A'** and **B''D**: are the non-parametric counterparts of d' and beta:
  + A' near 0.5 represents chance, while closer to 1.0 represents good detection,
  + B''D varies between -1 (bias towards 'different') and 1 (bias towards 'same'), with 0 being an unbiased observer.

```{r}
# Sensitivity and bias (both methods):
pc_sdt <- function(d, item_types)
{
  #responses <- matrix(NA, ncol=2, nrow=length(item_types$same) + length(item_types$different)); colnames(responses) <- c("expected", "given"); rownames(responses) <- c(item_types$same, item_types$different); # the expected vs given responses (0=same, 1=different)
  do.call(rbind, lapply(1:nrow(d), function(i)
  {
    ## recompose the expected answer and the actual response for the 'same' and 'different' items for this participant:
    #responses[,] <- NA; # initialize it
    #for(s in item_types$same)     { responses[s,"expected"] <- 0; responses[s,"given"] <- c(1,0)[d[i,s]+1]; } # for 'same' items, the expected is 0 and the "correct" given is 0 (clear but suboptimal implementation)
    #for(s in item_types$different){ responses[s,"expected"] <- 1; responses[s,"given"] <- c(0,1)[d[i,s]+1]; } # for 'different' items, the expected is 1 and the "correct" given is 1  (clear but suboptimal implementation)
    #stopifnot(all(as.numeric(responses[sort(rownames(responses)),1] == responses[sort(rownames(responses)),2]) == d[i,sort(rownames(responses))])); # sanity check
    ## compute now the Number of hits, Number of false alarms, Number of misses, Number of correct rejections, Number of targets, and Number of distractors (as per psycho::dprime):
    #n_hit  <- sum(responses[,"expected"] == 1 & responses[,"given"] == 1); # number of hits = 'different' items with response 'different'
    #n_fa   <- sum(responses[,"expected"] == 0 & responses[,"given"] == 1); # number of false alarms = 'same' items with response 'different'
    #n_miss <- sum(responses[,"expected"] == 1 & responses[,"given"] == 0); # number of misses = 'different' items with response 'same'
    #n_cr   <- sum(responses[,"expected"] == 0 & responses[,"given"] == 0); # number of correct rejections = 'same' items with response 'same'
    
    n_hit  <- sum(d[i, item_types$different] == 1); # number of hits = 'different' items with response 'different'
    n_fa   <- sum(d[i, item_types$same] == 0);      # number of false alarms = 'same' items with response 'different'
    n_miss <- sum(d[i, item_types$different] == 0); # number of misses = 'different' items with response 'same'
    n_cr   <- sum(d[i, item_types$same] == 1); # number of correct rejections = 'same' items with response 'same'
    res <- psycho::dprime(n_hit, n_fa, n_miss, n_cr, adjusted=TRUE);
    # assemble the results:
    data.frame("ID"=d$ID[i],
               "dprime"=max(0,res$dprime), "beta"=res$beta, "c"=res$c, # make sure the small negative d' estimates are forced to 0.0
               "aprime"=ifelse(is.na(res$aprime) || length(res$bppd)==0,NA,res$aprime), "bppd"=ifelse(is.na(res$aprime) || length(res$bppd)==0,NA,res$bppd));
  }));
}
d_all <- merge(d_all, pc_sdt(d_tone_wide_original, item_types_original), by="ID", all.x=TRUE, all.y=FALSE); names(d_all)[ncol(d_all)+(-4:0)] <- paste0(names(d_all)[ncol(d_all)+(-4:0)],"_orig");
d_all <- merge(d_all, pc_sdt(d_tone_wide_removed,  item_types_removed),  by="ID", all.x=TRUE, all.y=FALSE); names(d_all)[ncol(d_all)+(-4:0)] <- paste0(names(d_all)[ncol(d_all)+(-4:0)],"_remv");
d_all <- merge(d_all, pc_sdt(d_tone_wide_recoded,  item_types_recoded),  by="ID", all.x=TRUE, all.y=FALSE); names(d_all)[ncol(d_all)+(-4:0)] <- paste0(names(d_all)[ncol(d_all)+(-4:0)],"_recd");
```
```{r fig.width=12, fig.height=20, fig.cap=capFig("Histogram of the % correct 'same' (top) and 'different' (bottom) on each of the three datasets.")}
if( knitr::is_html_output() || knitr::is_latex_output() ) # running it directly in the console results in Error in plot.new() : figure margins too large
{
  par(mfcol=c(5,3));
  hist(d_all$dprime_orig, xlab="d'", main="d' (original)", xlim=c(-1,6.93));
  hist(d_all$beta_orig, xlab="beta", main="beta (original)", xlim=c(-5,40));
  hist(d_all$c_orig, xlab="c", main="c (original)", xlim=c(-3,3));
  hist(d_all$aprime_orig, xlab="A'", main="A' (original)", xlim=c(0, 1.0));
  hist(d_all$bppd_orig, xlab="B''D", main="B''D (original)", xlim=c(-1.0, 1.0));
  hist(d_all$dprime_remv, xlab="d'", main="d' (removed)", xlim=c(-1,6.93));
  hist(d_all$beta_remv, xlab="beta", main="beta (removed)", xlim=c(-5,40));
  hist(d_all$c_remv, xlab="c", main="c (removed)", xlim=c(-3,3));
  hist(d_all$aprime_remv, xlab="A'", main="A' (removed)", xlim=c(0, 1.0));
  hist(d_all$bppd_remv, xlab="B''D", main="B''D (removed)", xlim=c(-1.0, 1.0));
  hist(d_all$dprime_recd, xlab="d'", main="d' (recoded)", xlim=c(-1,6.93));
  hist(d_all$beta_recd, xlab="beta", main="beta (recoded)", xlim=c(-5,40));
  hist(d_all$c_recd, xlab="c", main="c (recoded)", xlim=c(-3,3));
  hist(d_all$aprime_recd, xlab="A'", main="A' (recoded)", xlim=c(0, 1.0));
  hist(d_all$bppd_recd, xlab="B''D", main="B''D (recoded)", xlim=c(-1.0, 1.0));
  par(mfrow=c(1,1));
}
```
```{r fig.width=12, fig.height=12, fig.cap=capFig("Heatmap with clustering for the Spearman correlations between SDT sensitivity and bias measures across the three datasets.")}
# Correlations:
tmp <- cor(d_all[,c("dprime_orig",  "beta_orig",  "c_orig",  "aprime_orig",  "bppd_orig",  "dprime_remv",  "beta_remv",  "c_remv",  "aprime_remv",  "bppd_remv",  "dprime_recd",  "beta_recd",  "c_recd",  "aprime_recd",  "bppd_recd")], 
           use="pairwise.complete.obs", method="spearman");
pander(round(tmp,2));
gplots::heatmap.2(tmp, cellnote=round(tmp,2), notecol="black", margins=c(12,12),
                  labRow=c("d' (original)",  "beta (original)",  "c (original)",  "A' (original)",  "B''D (original)", "d' (remove)",  "beta (remove)",  "c (remove)",  "A' (remove)",  "B''D (remove)", "d' (recode)",  "beta (recode)",  "c (recode)",  "A' (recode)",  "B''D (recode)"), 
                  labCol=c("d' (original)",  "beta (original)",  "c (original)",  "A' (original)",  "B''D (original)", "d' (remove)",  "beta (remove)",  "c (remove)",  "A' (remove)",  "B''D (remove)", "d' (recode)",  "beta (recode)",  "c (recode)",  "A' (recode)",  "B''D (recode)"));
```

First, the signal estimates d' and A' are very highly intercorrelated across datasets (≥0.93 for d' and ≥0.95 for A') and between them (between 0.91 and 0.96).
Second, within each dataset, beta and c have high correlations of ~0.9.
Between datasets, beta for 'remove' seems to have the best intercorrelations (0.92 with 'recode' and 0.89 with 'original') while the correlation for 'original' and 'recode' is 0.75; however, c shows much higher intercorrelations (between 0.82 and 0.95).
B''D for 'original' is moderately correlated with the other two (0.73 with 'recode' and 0.90 with 'remove'), while these two correlate at 0.90.
Within datasets, B''D correlates very strongly with c (around 0.92) and almost perfectly with beta (≥0.99).

Coupled with the more natural interpretation of some of these estimates estimates, we will focus primarily on d' and c, but also keep A' and B''D in mind.


##### Combining the estimates

From the above, it seems that either the 'remove' or the 'recode' datasets would be best if we had to use a single dataset, and there seems to be a slight data-driven advantage for the former (in the sense that it has the best correlations with the other two datasets), but at the cost of completely losing the information provided by the "weird" items -- given these, we will continue using the three datasets for now.
In terms of actual estimates, we will keep the % correct responses overall and for the 'same' and 'different' items separately, while for the Signal Detection Theory we will focus on d' and c.

However, first we will detect those participants with very high biases (one way or another):

```{r fig.width=12, fig.height=4, fig.cap=capFig("Histogram of the bias c on each of the three datasets (repeated from above).")}
par(mfrow=c(1,3));
hist(d_all$c_orig, main="The 'original' dataset", xlab="c", xlim=c(-3,3));
hist(d_all$c_remv, main="The 'remove' dataset", xlab="c", xlim=c(-3,3));
hist(d_all$c_recd, main="The 'recode' dataset", xlab="c", xlim=c(-3,3));
par(mfrow=c(1,1));

#t.test(d_all$c_orig, mu=0); # mean=0.52, p < 2.2e-16
#t.test(d_all$c_remv, mu=0); # mean=0.30, p < 2.2e-16
#t.test(d_all$c_recd, mu=0); # mean=0.21, p < 2.2e-16

#t.test(d_all$c_orig, d_all$c_remv); # p < 2.2e-16
#t.test(d_all$c_orig, d_all$c_recd); # p < 2.2e-16
#t.test(d_all$c_remv, d_all$c_recd); # p = 9.116e-05
```

It can be seen that in all three datasets there is an overall bias towards answering 'same' (c > 0, highly significant in all three cases), but much smaller (and statistically highly significantly so) for the 'remove' (`r round(mean(d_all$c_remv,na.rm=TRUE),2)`) and especially 'recode' (`r round(mean(d_all$c_recd,na.rm=TRUE),2)`) than for the 'original' (`r round(mean(d_all$c_orig,na.rm=TRUE),2)`) dataset. 

```{r fig.width=4, fig.height=4, fig.cap=capFig("Histogram of the bias c on each of the three datasets."), fig.show="hold", out.width="33%"}
ggplot(d_all, aes(x=c_orig, y=c_remv)) + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + xlab("original") + ylab("remove") + xlim(-3,3) + ylim(-3,3);
ggplot(d_all, aes(x=c_orig, y=c_recd)) + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + xlab("original") + ylab("recode") + xlim(-3,3) + ylim(-3,3);
ggplot(d_all, aes(x=c_remv, y=c_recd)) + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + xlab("remove")   + ylab("recode") + xlim(-3,3) + ylim(-3,3);
```

It is clear that there are some participants with strong biases in all three datasets; let's identify them.
First, there is one participant which *systematically answered "same" irrespective of the item*: 
```{r}
kable(d_all[which.max(d_all$c_orig), c("age",	"gender", "music_years",	"education_years",	"location")], row.names=FALSE, caption=capTab("One participant systematcally answered 'same'"));
d_all <- d_all[ -which.max(d_all$c_orig), ]; # remove participant 6826462 as she systematically answered 'same'
```

```{r include=FALSE}
# 2SDs:
sum(d_all$c_orig <= -2 | d_all$c_orig >= 2); # 0
sum(d_all$c_remv <= -2 | d_all$c_remv >= 2); # 0
sum(d_all$c_recd <= -2 | d_all$c_recd >= 2); # 0

# 1SD:
sum(d_all$c_orig <= -1 | d_all$c_orig >= 1); # 36
sum(d_all$c_remv <= -1 | d_all$c_remv >= 1); # 17
sum(d_all$c_recd <= -1 | d_all$c_recd >= 1); # 14
```
Second, there are no other participants with strong biases (≤-2 or ≥2) in any dataset and only a few with c between -1 and 1, so let' remove this one only.

Interestingly, while for the "original" dataset there are quite a few participants with biases |c|≥1 (`r sum(d_all$c_orig <= -1)` with c≤-1 and `r sum(d_all$c_orig > 1)` with c≥1), there are fewer for the "remove" (`r sum(d_all$c_remv <= -1)` with c≤-1 and `r sum(d_all$c_remv > 1)` with c≥1) and for the "recode" (`r sum(d_all$c_recd <= -1)` with c≤-1 and `r sum(d_all$c_recd > 1)` with c≥1) datasets, again supporting the idea that the "weird" items are treated as 'same' and not 'different'.

```{r include=FALSE}
#t.test(d_all$dprime_recd, d_all$dprime_remv); # p = 0.009863
#t.test(d_all$dprime_recd, d_all$dprime_orig); # p = 6.375e-05
#t.test(d_all$dprime_remv, d_all$dprime_orig); # p = 1.077e-10
```
Focusing on the "de-biased" signal sensitivity d', it interesting to note that it is higher, on average, on the "recoded" (`r round(mean(d_all$dprime_recd),2)`) dataset than on the "removed" (`r round(mean(d_all$dprime_remv),2)`) dataset and than on the "original" (`r round(mean(d_all$dprime_orig),2)`) dataset (all differences are highly significant as judged with two-sample t-tests), again supporting the view that the "weird" items are 'same' and not 'different'.


Let's look at the relationships between estimates *within* datasets:

```{r fig.width=6, fig.height=6, fig.cap=capFig("Heatmap with clustering for the Spearman correlations between various measures in each dataset."), fig.show="hold", out.width="33%"}
# Correlations:
r_orig <- cor(d_all[,c("pc_tot_orig",  "pc_same_orig",  "pc_diff_orig",  "dprime_orig",  "aprime_orig")], use="pairwise.complete.obs", method="spearman");
gplots::heatmap.2(r_orig, cellnote=round(r_orig,2), notecol="black", margins=c(12,12), main="Original",
                  labRow=c("% total",  "% same",  "% different",  "d'",  "A'"), 
                  labCol=c("% total",  "% same",  "% different",  "d'",  "A'"));

r_remv <- cor(d_all[,c("pc_tot_remv",  "pc_same_remv",  "pc_diff_remv",  "dprime_remv",  "aprime_remv")], use="pairwise.complete.obs", method="spearman");
gplots::heatmap.2(r_remv, cellnote=round(r_remv,2), notecol="black", margins=c(12,12), main="Removed",
                  labRow=c("% total",  "% same",  "% different",  "d'",  "A'"), 
                  labCol=c("% total",  "% same",  "% different",  "d'",  "A'"));

r_recd <- cor(d_all[,c("pc_tot_recd",  "pc_same_recd",  "pc_diff_recd",  "dprime_remv",  "aprime_remv")], use="pairwise.complete.obs", method="spearman");
gplots::heatmap.2(r_recd, cellnote=round(r_recd,2), notecol="black", margins=c(12,12), main="Recoded",
                  labRow=c("% total",  "% same",  "% different",  "d'",  "A'"), 
                  labCol=c("% total",  "% same",  "% different",  "d'",  "A'"));
```

```{r fig.width=4*3, fig.height=4*3, fig.cap=capFig("The three main measures of success on each dataset.")}
grid.arrange(ggplot(d_all, aes(x=dprime_orig, y=aprime_orig))     + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + xlab("d'") + ylab("A'")      + xlim(0,6) + ylim(0,1) + ggtitle("Original"),
             ggplot(d_all, aes(x=dprime_orig, y=100*pc_tot_orig)) + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + xlab("d'") + ylab("% total") + xlim(0,6) + ylim(0,100) + ggtitle(""),
             ggplot(d_all, aes(x=aprime_orig, y=100*pc_tot_orig)) + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + xlab("A'") + ylab("% total") + xlim(0,1) + ylim(0,100) + ggtitle(""),
             
             ggplot(d_all, aes(x=dprime_remv, y=aprime_remv))     + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + xlab("d'") + ylab("A'")      + xlim(0,6) + ylim(0,1) + ggtitle("Removed"),
             ggplot(d_all, aes(x=dprime_remv, y=100*pc_tot_remv)) + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + xlab("d'") + ylab("% total") + xlim(0,6) + ylim(0,100) + ggtitle(""),
             ggplot(d_all, aes(x=aprime_remv, y=100*pc_tot_remv)) + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + xlab("A'") + ylab("% total") + xlim(0,1) + ylim(0,100) + ggtitle(""),
             
             ggplot(d_all, aes(x=dprime_recd, y=aprime_recd))     + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + xlab("d'") + ylab("A'")      + xlim(0,6) + ylim(0,1) + ggtitle("Recoded"),
             ggplot(d_all, aes(x=dprime_recd, y=100*pc_tot_recd)) + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + xlab("d'") + ylab("% total") + xlim(0,6) + ylim(0,100) + ggtitle(""),
             ggplot(d_all, aes(x=aprime_recd, y=100*pc_tot_recd)) + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + xlab("A'") + ylab("% total") + xlim(0,1) + ylim(0,100) + ggtitle(""),
             
             ncol=3);
```

d' and A' are highly correlated (≥ 0.92), as expected, and each is highly correlated with the % total correct responses: d' (between 0.87 and 0.96) and A' is virtually perfectly correlated (≥0.97).


##### Consider the "weird" items separately

Keeping, removing or recoding the "weird" items all share the downside that they ignore the participants that seemingly answered correctly for these items.
Therefore, we will consider these items as 'different' items, together with their associated 'same' items, *separately*.

```{r}
# just the "weird" items, considered as 'different' (and their associated 'same' items):
d_tone_wide_weird <- d_tone_wide[, c("ID", # participant ID
                                     weird_items, # the weird items as 'different' (original)
                                     weird_items_same # and the corresponding 'same' items
                                    )]; 
item_types_weird  <- list("same"=sort(setdiff(names(d_tone_wide_weird), c("ID",weird_items))), "different"=sort(weird_items));

# % correct responses overall:
d_all <- merge(d_all, pc_total(d_tone_wide_weird), by="ID", all.x=TRUE, all.y=FALSE); names(d_all)[ncol(d_all)] <- "pc_tot_weird";
# % correct responses separately for 'same' and 'different':
d_all <- merge(d_all, pc_same_different(d_tone_wide_weird, item_types_weird), by="ID", all.x=TRUE, all.y=FALSE); names(d_all)[ncol(d_all)+(-1:0)] <- c("pc_same_weird", "pc_diff_weird");
# SDT:
d_all <- merge(d_all, pc_sdt(d_tone_wide_weird, item_types_weird), by="ID", all.x=TRUE, all.y=FALSE); names(d_all)[ncol(d_all)+(-4:0)] <- paste0(names(d_all)[ncol(d_all)+(-4:0)],"_weird");
```

```{r fig.width=9, fig.height=9, fig.cap=capFig("Histograms of the measures on the 'weird' dataset.")}
par(mfrow=c(3,3));
hist(d_all$pc_tot_weird*100, main="% total", xlab="% correct responses", xlim=c(0,100));
hist(d_all$pc_same_weird*100, main="% 'same'", xlab="% correct responses", xlim=c(0,100));
hist(d_all$pc_diff_weird*100, main="% 'different'", xlab="% correct responses", xlim=c(0,100));
hist(d_all$dprime_weird, main="d'", xlab="d'", xlim=c(-1,6.93));
hist(d_all$beta_weird, xlab="beta", main="beta", xlim=c(-5,40));
hist(d_all$c_weird, xlab="c", main="c", xlim=c(-3,3));
hist(d_all$aprime_weird, xlab="A'", main="A'", xlim=c(0, 1.0));
hist(d_all$bppd_weird, xlab="B''D", main="B''D", xlim=c(-1.0, 1.0));
par(mfrow=c(1,1));
```

A expected (see Table below), there is a low overall % correct responses due to the very low % correct responses on the 'different' items, which is reflected in rather low sensitivities, with strong biases towards 'same' responses compared with the other three datasets:
```{r}
kable(data.frame("dataset"=c("original",                                  "remove",                                    "recode",                                    "weird"), 
                 "pc_tot" =c(round(mean(d_all$pc_tot_orig*100),1),        round(mean(d_all$pc_tot_remv*100),1),        round(mean(d_all$pc_tot_recd*100),1),        round(mean(d_all$pc_tot_weird*100),1)), 
                 "pc_same"=c(round(mean(d_all$pc_same_orig*100),1),       round(mean(d_all$pc_same_remv*100),1),       round(mean(d_all$pc_same_recd*100),1),       round(mean(d_all$pc_same_weird*100),1)),
                 "pc_diff"=c(round(mean(d_all$pc_diff_orig*100),1),       round(mean(d_all$pc_diff_remv*100),1),       round(mean(d_all$pc_diff_recd*100),1),       round(mean(d_all$pc_diff_weird*100),1)),
                 "dprime" =c(round(mean(d_all$dprime_orig),2),            round(mean(d_all$dprime_remv),2),            round(mean(d_all$dprime_recd),2),            round(mean(d_all$dprime_weird),2)),
                 "beta"   =c(round(mean(d_all$beta_orig),2),              round(mean(d_all$beta_remv),2),              round(mean(d_all$beta_recd),2),              round(mean(d_all$beta_weird),2)),
                 "c"      =c(round(mean(d_all$c_orig),2),                 round(mean(d_all$c_remv),2),                 round(mean(d_all$c_recd),2),                 round(mean(d_all$c_weird),2)),
                 "aprime" =c(round(mean(d_all$aprime_orig,na.rm=TRUE),2), round(mean(d_all$aprime_remv,na.rm=TRUE),2), round(mean(d_all$aprime_recd,na.rm=TRUE),2), round(mean(d_all$aprime_weird,na.rm=TRUE),2)),
                 "bppd"   =c(round(mean(d_all$bppd_orig,na.rm=TRUE),2),   round(mean(d_all$bppd_remv,na.rm=TRUE),2),   round(mean(d_all$bppd_recd,na.rm=TRUE),2),   round(mean(d_all$bppd_weird,na.rm=TRUE),2))), 
      col.names=c("dataset", "% total", "% same", "% different", "d'", "beta", "c", "A'", "B''D"), row.names=FALSE, 
      caption=capTab("Comparing the means of the measures across all four datasets."));
```

Importantly, the maximum % correct is `r round(max(d_all$pc_tot_orig*100),1)`% overall, `r round(max(d_all$pc_same_orig*100),1)`% for 'same' and  `r round(max(d_all$pc_diff_orig*100),1)`% for 'different', suggesting that same participants seem to have answered correctly.
Let's see who these participants are and how their responses on the 'non-weird' items are like:

```{r fig.width=4*3, fig.height=4*3, fig.cap=capFig("The measures of success for the 'weird' and 'non-weird' ('remove') items.")}
grid.arrange(
  ggplot(d_all, aes(y=pc_tot_weird*100, x=pc_tot_remv*100))   + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + ylab("Weird") + xlab("Remove") + xlim(0,100) + ylim(0,100) + ggtitle("% total"),
  ggplot(d_all, aes(y=pc_same_weird*100, x=pc_same_remv*100)) + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + ylab("Weird") + xlab("Remove") + xlim(0,100) + ylim(0,100) + ggtitle("% 'same'"),
  ggplot(d_all, aes(y=pc_diff_weird*100, x=pc_diff_remv*100)) + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + ylab("Weird") + xlab("Remove") + xlim(0,100) + ylim(0,100) + ggtitle("% 'different'"),
  
  ggplot(d_all, aes(y=dprime_weird, x=dprime_remv)) + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + xlab("Weird") + ylab("Remove") + xlim(0,6)   + ylim(0,6)   + ggtitle("d'"),
  ggplot(d_all, aes(y=beta_weird, x=beta_remv))     + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + xlab("Weird") + ylab("Remove") + xlim(-5,30) + ylim(-5,30) + ggtitle("beta"),
  ggplot(d_all, aes(y=c_weird, x=c_remv))           + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + xlab("Weird") + ylab("Remove") + xlim(-3,3)  + ylim(-3,3)  + ggtitle("c"),
  
  ggplot(d_all, aes(y=aprime_weird, x=aprime_remv)) + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + xlab("Weird") + ylab("Remove") + xlim(0, 1.0)    + ylim(0, 1.0)    + ggtitle("A'"),
  ggplot(d_all, aes(y=bppd_weird, x=bppd_remv))     + geom_point(color="blue", alpha=0.25) + geom_smooth(method="loess", color="red", alpha=0.25, linewidth=0.5) + xlab("Weird") + ylab("Remove") + xlim(-1.0, 1.0) + ylim(-1.0, 1.0) + ggtitle("B''D"),
  
  ncol=3);
```

And let's look at their correlations:

```{r fig.width=12, fig.height=12, fig.cap=capFig("Heatmap with clustering for the correlations between all measures for the 'weird' and 'non-weird' ('remove' dataset) items.")}
# Correlations:
tmp <- cor(d_all[,c("pc_tot_weird", "pc_same_weird", "pc_diff_weird", "dprime_weird", "beta_weird", "c_weird", "aprime_weird", "bppd_weird",  
                    "pc_tot_remv",  "pc_same_remv",  "pc_diff_remv",  "dprime_remv",  "beta_remv",  "c_remv",  "aprime_remv",  "bppd_remv")], 
           use="pairwise.complete.obs", method="spearman");
#pander(round(tmp,2));
gplots::heatmap.2(tmp, cellnote=round(tmp,2), notecol="black", margins=c(12,12),
                  labRow=c("% tot (remove)",  "% same (remove)",  "% diff (remove)",  "d' (remove)",  "beta (remove)", "c (remove)",  "A' (remove)",  "B''D (remove)",  "% tot (weird)",  "% same (weird)",  "% diff (weird)",  "d' (weird)",  "beta (weird)", "c (weird)",  "A' (weird)",  "B''D (weird)"), 
                  labCol=c("% tot (remove)",  "% same (remove)",  "% diff (remove)",  "d' (remove)",  "beta (remove)", "c (remove)",  "A' (remove)",  "B''D (remove)",  "% tot (weird)",  "% same (weird)",  "% diff (weird)",  "d' (weird)",  "beta (weird)", "c (weird)",  "A' (weird)",  "B''D (weird)"));
```

The important relationships are those between the two sets ("weird" vs "non-weird" items) for:

**% total correct responses**: their correlation is 0.50 overall, but this is likely heavily biased by the 'same' items (which, as expected, behaves the same for the "weird" and "non-weird" items),

```{r include=FALSE, eval=FALSE}
library(cluster);
nclust <- cluster::clusGap(d_all[,c("pc_diff_remv", "pc_diff_weird")], FUNcluster=kmeans, K.max=10); # number of clusters
nclust;
fviz_gap_stat(nclust); # 3 clusters seem best...
library(NbClust);
nclust2 <- NbClust::NbClust(d, method="kmeans", distance="euclidean"); # 8 methods suggest 2 clusters, and 7 suggest 3...
```

**% correct for the 'different' items**: while overall there is no correlation (-0.03, *p*>0.05), there seem to be two or three groups of participants (`cluster::clusGap()` suggests 3 clusters, while `NbClust::NbClust()` finds that 8 methods suggest 2 clusters and 7 suggest 3):
```{r fig.width=10, fig.height=5, fig.cap=capFig("K-means clustering of the participants using the % correct for 'different' on the 'weird' and 'non-weird' items, with k=2 (left) and k=3 (right), trying to keep the colors and symbols of the corresponding clusters similar. Please note the order of the 'numbers' (i.e., '1', '2' and '3') in the legend is arbitrary.")}
d <- d_all[,c("pc_diff_remv", "pc_diff_weird")]; rownames(d) <- d_all$ID;
km2 <- kmeans(d, centers=2); km3 <- kmeans(d, centers=3);
# As the cluster numbers are arbitrary and may differ between runs and configurations, try to identify the clusters by their (approximate) centers:
cluster_names <- data.frame("k"=c(2, 2,
                                  3, 3, 3),
                            "cluster"=c(1:2, # what the cluster number should be
                                        1:3),
                            "number"=c(which.min(km2$centers[,"pc_diff_remv"]), which.max(km2$centers[,"pc_diff_remv"]), # for k=2, cluster 1 is the leftmost
                                       which.min(km3$centers[,"pc_diff_remv"]), which.max(km3$centers[,"pc_diff_remv"]), which.min(km3$centers[,"pc_diff_weird"])), # for k=3, cluster 1 is the leftmost, 2 is rightmost and 3 the lowest
                            "color"=c("red", "blue", # the plotting colors
                                      "red", "dodgerblue2", "royalblue4"),
                            "shape"=c(21, 23, # the plotting shapes
                                     21, 24, 25)
);
labels_k2 <- as.character(cluster_names$cluster[ cluster_names$k == 2]); names(labels_k2) <- as.character(cluster_names$number[ cluster_names$k == 2]);
colors_k2 <- as.character(cluster_names$color[ cluster_names$k == 2]);   names(colors_k2) <- as.character(cluster_names$number[ cluster_names$k == 2]);
shape_k2  <- cluster_names$shape[ cluster_names$k == 2];                 names(shape_k2)  <- as.character(cluster_names$number[ cluster_names$k == 2]);
labels_k3 <- as.character(cluster_names$cluster[ cluster_names$k == 3]); names(labels_k3) <- as.character(cluster_names$number[ cluster_names$k == 3]);
colors_k3 <- as.character(cluster_names$color[ cluster_names$k == 3]);   names(colors_k3) <- as.character(cluster_names$number[ cluster_names$k == 3]);
shape_k3  <- cluster_names$shape[ cluster_names$k == 3];                 names(shape_k3)  <- as.character(cluster_names$number[ cluster_names$k == 3]);

grid.arrange(
  fviz_cluster(km2, data=d, labelsize=9, geom="point", main="2 clusters") + 
    geom_smooth(aes(color=cluster), method="lm") + #geom_smooth(aes(color=cluster), method="loess") + 
    scale_colour_manual(values = colors_k2, label = labels_k2) + 
    scale_fill_manual(values = colors_k2, label = labels_k2) + 
    scale_shape_manual(values = shape_k2, label = labels_k2),
  fviz_cluster(km3, data=d, labelsize=9, geom="point", main="3 clusters") + 
    geom_smooth(aes(color=cluster), method="lm") + #geom_smooth(aes(color=cluster), method="loess") + 
    scale_colour_manual(values = colors_k3, label = labels_k3) + 
    scale_fill_manual(values = colors_k3, label = labels_k3) + 
    scale_shape_manual(values = shape_k3, label = labels_k3),
  ncol=2);
```
It can be seen that the 3-clusters solution is more or less splitting one of the clusters of the 2-clusters solution, with the rough correspondences between the k=2 and k=3 clusters 1(2) ≈ 1(3) [`r colorize_text("red", "red")` in the figure] and 2(2) ≈ 2(3)+3(3) [shades of `r colorize_text("blue", "blue")` in the figure], where the first digit is the cluster 'number' (which is arbitrary) and  in parentheses the number of clusters, k, so 1(2) means cluster '1' of the k=2 solution.
Within each cluster, we have the following correlations between the "weird" and "non-weird" % correct:
```{r}
kable(data.frame("no.clusters"=c("full dataset", "k=2", "k=3"),
                  "cluster.1"=c(sprintf("*r*=%.2f, *p*=%s; *ρ*=%.2f, *p*=%s", 
                                        (x<-cor.test(~ pc_diff_weird + pc_diff_remv, data=d, method="pearson"))$estimate, scinot(x$p.value), 
                                        (x<-cor.test(~ pc_diff_weird + pc_diff_remv, data=d, method="spearman"))$estimate, scinot(x$p.value)),
                                sprintf("*r*=%.2f, *p*=%s; *ρ*=%.2f, *p*=%s", 
                                        (x<-cor.test(~ pc_diff_weird + pc_diff_remv, data=d[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["1"])],], method="pearson"))$estimate, scinot(x$p.value), 
                                        (x<-cor.test(~ pc_diff_weird + pc_diff_remv, data=d[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["1"])],], method="spearman"))$estimate, scinot(x$p.value)),
                                sprintf("*r*=%.2f, *p*=%s; *ρ*=%.2f, *p*=%s", 
                                        (x<-cor.test(~ pc_diff_weird + pc_diff_remv, data=d[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])],], method="pearson"))$estimate, scinot(x$p.value), 
                                        (x<-cor.test(~ pc_diff_weird + pc_diff_remv, data=d[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])],], method="spearman"))$estimate, scinot(x$p.value))),
                  "cluster.2"=c("-",
                                sprintf("*r*=%.2f, *p*=%s; *ρ*=%.2f, *p*=%s", 
                                        (x<-cor.test(~ pc_diff_weird + pc_diff_remv, data=d[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["2"])],], method="pearson"))$estimate, scinot(x$p.value), 
                                        (x<-cor.test(~ pc_diff_weird + pc_diff_remv, data=d[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["2"])],], method="spearman"))$estimate, scinot(x$p.value)),
                                sprintf("*r*=%.2f, *p*=%s; *ρ*=%.2f, *p*=%s", 
                                        (x<-cor.test(~ pc_diff_weird + pc_diff_remv, data=d[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["2"])],], method="pearson"))$estimate, scinot(x$p.value), 
                                        (x<-cor.test(~ pc_diff_weird + pc_diff_remv, data=d[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["2"])],], method="spearman"))$estimate, scinot(x$p.value))),
                  "cluster.3"=c("-",
                                "-",
                                sprintf("*r*=%.2f, *p*=%s; *ρ*=%.2f, *p*=%s", 
                                        (x<-cor.test(~ pc_diff_weird + pc_diff_remv, data=d[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["3"])],], method="pearson"))$estimate, scinot(x$p.value), 
                                        (x<-cor.test(~ pc_diff_weird + pc_diff_remv, data=d[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["3"])],], method="spearman"))$estimate, scinot(x$p.value)))),
                  row.names=FALSE,
                  col.names=c("# clusters", "cluster 1", "cluster 2", "cluster 3"), 
      caption=capTab("Correlations (Pearson and Spearman) between % correct 'different' for the 'weird' and 'non-weird' items on the full dataset (1 cluster) and for each cluster separately (the same cluster notations as in the figure above)."));
```
```{r results='hide'}
m_all <- lm(pc_diff_weird ~ pc_diff_remv, data=d); summary(m_all);                                                         # whole dataset:         beta=-0.16***
m_2_1 <- lm(pc_diff_weird ~ pc_diff_remv, data=d[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["1"])],]); summary(m_2_1); # 2 clusters, cluster 1: beta= 0.63***
m_2_2 <- lm(pc_diff_weird ~ pc_diff_remv, data=d[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["2"])],]); summary(m_2_2); # 2 clusters, cluster 2: beta= 0.60***
m_3_1 <- lm(pc_diff_weird ~ pc_diff_remv, data=d[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])],]); summary(m_3_1); # 3 clusters, cluster 1: beta= 0.73***
m_3_2 <- lm(pc_diff_weird ~ pc_diff_remv, data=d[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["2"])],]); summary(m_3_2); # 3 clusters, cluster 2: beta= 0.15
m_3_3 <- lm(pc_diff_weird ~ pc_diff_remv, data=d[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["3"])],]); summary(m_3_3); # 3 clusters, cluster 3: beta= 0.15***
```
It can be seen that, while on the full dataset there basically is no correlation (Pearson's seems affected by a few outliers), in clusters 1(2), 1(3), 2(2) and 3(3) there is a strong positive and highly significant correlation, while on cluster 2(3) there is no correlation. 

For **k=2**, **cluster 1(2)** comprises only `r sum(km2$cluster == as.numeric(labels_k2["1"]))` or `r round(100*sum(km2$cluster == as.numeric(labels_k2["1"]))/length(km2$cluster),1)`% participants which seem to respond at chance level to the "non-weird" 'different' items (between `r round(min(d$pc_diff_remv[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["1"])]])*100,1)`% and `r round(max(d$pc_diff_remv[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["1"])]])*100,1)`%, mean = `r round(mean(d$pc_diff_remv[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["1"])]])*100,1)`% and median = `r round(median(d$pc_diff_remv[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["1"])]])*100,1)`%) and have low % for the "weird" items (between `r round(min(d$pc_diff_weird[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["1"])]])*100,1)`% and `r round(max(d$pc_diff_weird[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["1"])]])*100,1)`%, mean = `r round(mean(d$pc_diff_weird[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["1"])]])*100,1)`% and median = `r round(median(d$pc_diff_weird[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["1"])]])*100,1)`%).
Here, there is a significant positive relationship between the two % correct measures, the % correct of the "non-weird" items predicting very well that for the "weird" items (liner regression `r sprintf("*β*=%.2f±%.2f, *p*=%s", (tmp <- summary(m_2_1)$coefficients)["pc_diff_remv", "Estimate"], tmp["pc_diff_remv", "Std. Error"], scinot(tmp["pc_diff_remv", "Pr(>|t|)"]))`).

The same pattern holds for the corresponding **k=3**, **cluster 1(3)**: this comprises `r sum(km3$cluster == as.numeric(labels_k3["1"]))` or `r round(100*sum(km3$cluster == as.numeric(labels_k3["1"]))/length(km3$cluster),1)`% participants which seem to respond at chance level to the "non-weird" 'different' items (between `r round(min(d$pc_diff_remv[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])]])*100,1)`% and `r round(max(d$pc_diff_remv[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])]])*100,1)`%, mean = `r round(mean(d$pc_diff_remv[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])]])*100,1)`% and median = `r round(median(d$pc_diff_remv[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])]])*100,1)`%) and have low % for the "weird" items (between `r round(min(d$pc_diff_weird[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])]])*100,1)`% and `r round(max(d$pc_diff_weird[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])]])*100,1)`%, mean = `r round(mean(d$pc_diff_weird[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])]])*100,1)`% and median = `r round(median(d$pc_diff_weird[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])]])*100,1)`%).
Here, there is a significant positive relationship between the two % correct measures, the % correct of the "non-weird" items predicting very well that for the "weird" items (liner regression `r sprintf("*β*=%.2f±%.2f, *p*=%s", (tmp <- summary(m_3_1)$coefficients)["pc_diff_remv", "Estimate"], tmp["pc_diff_remv", "Std. Error"], scinot(tmp["pc_diff_remv", "Pr(>|t|)"]))`).

For **k=2**, **cluster 2(2)** comprises the vast majority of the participants (`r sum(km2$cluster == as.numeric(labels_k2["2"]))` or `r round(100*sum(km2$cluster == as.numeric(labels_k2["2"]))/length(km2$cluster),1)`%) which have high % correct responses for the "non-weird" 'different' items (≥ `r round(min(d$pc_diff_remv[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["2"])]])*100,1)`%, mean = `r round(mean(d$pc_diff_remv[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["2"])]])*100,1)`% and median = `r round(median(d$pc_diff_remv[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["2"])]])*100,1)`%) but mostly low % for the "weird" items (≤ `r round(max(d$pc_diff_weird[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["2"])]])*100,1)`%, mean = `r round(mean(d$pc_diff_weird[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["2"])]])*100,1)`% and median = `r round(median(d$pc_diff_weird[rownames(d) %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["2"])]])*100,1)`%).
Here, there is a significant positive relationship between the two % correct measures, the % correct of the "non-weird" items predicting that for the "weird" items (liner regression `r sprintf("*β*=%.2f±%.2f, *p*=%s", (tmp <- summary(m_2_2)$coefficients)["pc_diff_remv", "Estimate"], tmp["pc_diff_remv", "Std. Error"], scinot(tmp["pc_diff_remv", "Pr(>|t|)"]))`).

This group roughly splits in two for **k=3**: **cluster 2(3)** that comprises `r sum(km3$cluster == as.numeric(labels_k3["1"]))` or `r round(100*sum(km3$cluster == as.numeric(labels_k3["1"]))/length(km2$cluster),1)`% the participants with relatively high % for the "non-weird" items (≥ `r round(min(d$pc_diff_remv[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])]])*100,1)`%, mean = `r round(mean(d$pc_diff_remv[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])]])*100,1)`% and median = `r round(median(d$pc_diff_remv[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])]])*100,1)`%) and very low for the "weird" items (≤ `r round(max(d$pc_diff_weird[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])]])*100,1)`%, mean = `r round(mean(d$pc_diff_weird[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])]])*100,1)`% and median = `r round(median(d$pc_diff_weird[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])]])*100,1)`%), basically those that clearly treated the "weird" items as being of the 'same' type.
There is no relationship between the two % correct measures (`r sprintf("*β*=%.2f±%.2f, *p*=%s", (tmp <- summary(m_3_2)$coefficients)["pc_diff_remv", "Estimate"], tmp["pc_diff_remv", "Std. Error"], scinot(tmp["pc_diff_remv", "Pr(>|t|)"]))`).
The other part forms **cluster 3(3)** that comprises `r sum(km3$cluster == as.numeric(labels_k3["3"]))` or `r round(100*sum(km3$cluster == as.numeric(labels_k3["3"]))/length(km2$cluster),1)`% the participants with high % for the "non-weird" items (≥ `r round(min(d$pc_diff_remv[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["3"])]])*100,1)`%, mean = `r round(mean(d$pc_diff_remv[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["3"])]])*100,1)`% and median = `r round(median(d$pc_diff_remv[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["3"])]])*100,1)`%) and low for the "weird" items (≤ `r round(max(d$pc_diff_weird[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["3"])]])*100,1)`%, mean = `r round(mean(d$pc_diff_weird[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["3"])]])*100,1)`% and median = `r round(median(d$pc_diff_weird[rownames(d) %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["3"])]])*100,1)`%).
However, here there is no relationship between the two % correct measures, with a signifciant positive prediction of % correct of the "weird" items from the  "non-weird" ones (`r sprintf("*β*=%.2f±%.2f, *p*=%s", (tmp <- summary(m_3_3)$coefficients)["pc_diff_remv", "Estimate"], tmp["pc_diff_remv", "Std. Error"], scinot(tmp["pc_diff_remv", "Pr(>|t|)"]))`).

Importantly, the % correct for the "weird" items (which are nominally 'different') have very strong and highly significant *negative* correlations with the % correct for the 'same' "non-weird" items, and their linear regressions have negative and slopes *β* of comparable size (-0.90 ≤ *β*s ≤ -0.50) on the whole dataset and in each cluster separately:

```{r}
# same (remove) vs different (weird):
kable(data.frame("no.clusters"=c("full dataset", "k=2", "k=3"),
                  "cluster.1"=c(sprintf("*r*=%.2f, *p*=%s; *ρ*=%.2f, *p*=%s", 
                                        (x<-cor.test(~ pc_diff_weird + pc_same_remv, data=d_all, method="pearson"))$estimate, scinot(x$p.value), 
                                        (x<-cor.test(~ pc_diff_weird + pc_same_remv, data=d_all, method="spearman"))$estimate, scinot(x$p.value)),
                                sprintf("*r*=%.2f, *p*=%s; *ρ*=%.2f, *p*=%s", 
                                        (x<-cor.test(~ pc_diff_weird + pc_same_remv, data=d_all[d_all$ID %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["1"])],], method="pearson"))$estimate, scinot(x$p.value), 
                                        (x<-cor.test(~ pc_diff_weird + pc_same_remv, data=d_all[d_all$ID %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["1"])],], method="spearman"))$estimate, scinot(x$p.value)),
                                sprintf("*r*=%.2f, *p*=%s; *ρ*=%.2f, *p*=%s", 
                                        (x<-cor.test(~ pc_diff_weird + pc_same_remv, data=d_all[d_all$ID %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])],], method="pearson"))$estimate, scinot(x$p.value), 
                                        (x<-cor.test(~ pc_diff_weird + pc_same_remv, data=d_all[d_all$ID %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])],], method="spearman"))$estimate, scinot(x$p.value))),
                  "cluster.2"=c("-",
                                sprintf("*r*=%.2f, *p*=%s; *ρ*=%.2f, *p*=%s", 
                                        (x<-cor.test(~ pc_diff_weird + pc_same_remv, data=d_all[d_all$ID %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["2"])],], method="pearson"))$estimate, scinot(x$p.value), 
                                        (x<-cor.test(~ pc_diff_weird + pc_same_remv, data=d_all[d_all$ID %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["2"])],], method="spearman"))$estimate, scinot(x$p.value)),
                                sprintf("*r*=%.2f, *p*=%s; *ρ*=%.2f, *p*=%s", 
                                        (x<-cor.test(~ pc_diff_weird + pc_same_remv, data=d_all[d_all$ID %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["2"])],], method="pearson"))$estimate, scinot(x$p.value), 
                                        (x<-cor.test(~ pc_diff_weird + pc_same_remv, data=d_all[d_all$ID %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["2"])],], method="spearman"))$estimate, scinot(x$p.value))),
                  "cluster.3"=c("-",
                                "-",
                                sprintf("*r*=%.2f, *p*=%s; *ρ*=%.2f, *p*=%s", 
                                        (x<-cor.test(~ pc_diff_weird + pc_same_remv, data=d_all[d_all$ID %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["3"])],], method="pearson"))$estimate, scinot(x$p.value), 
                                        (x<-cor.test(~ pc_diff_weird + pc_same_remv, data=d_all[d_all$ID %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["3"])],], method="spearman"))$estimate, scinot(x$p.value)))),
                  row.names=FALSE,
                  col.names=c("# clusters", "cluster 1", "cluster 2", "cluster 3"), 
      caption=capTab("Correlations (Pearson and Spearman) between % correct for the 'weird' and for the *'same'* for the 'non-weird' items on the full dataset (1 cluster) and for each cluster separately (the same cluster notations as in the figure above)."));
```
```{r fig.width=3*4, fig.height=1*5, fig.cap=capFig("The % correct responses for the 'weird' (y axis) vs the % correct responses foe the *'same'* 'non-weird' (aka 'remove') items (x axis) overall (left), for k=2 (middle) and k=3 (right), keeping the same cluster colors and symbols as above.")}
d1 <- cbind(d_all[,c("ID", "pc_same_remv", "pc_diff_weird")], 
            "km2.cluster"=as.factor(vapply(km2$cluster[d_all$ID], function(i) labels_k2[names(labels_k2) == i], character(1))), 
            "km3.cluster"=as.factor(vapply(km3$cluster[d_all$ID], function(i) labels_k3[names(labels_k3) == i], character(1)))); 
hull_km2 <- d1 %>% group_by(km2.cluster) %>% slice(chull(pc_same_remv, pc_diff_weird));
hull_km3 <- d1 %>% group_by(km3.cluster) %>% slice(chull(pc_same_remv, pc_diff_weird));
grid.arrange(
  # overall:
  ggplot(d1, aes(x=100*pc_same_remv, y=100*pc_diff_weird)) + 
    geom_point(alpha=0.25, aes(color="black")) + geom_smooth(method="lm", alpha=0.25, color="gray40") + 
    xlab("% 'same' remove") + ylab("% 'different' weird") + xlim(0,100) + ylim(0,100) + scale_color_identity(name="", labels="participant", guide="legend") + 
    theme(legend.position="bottom") + ggtitle("Whole dataset"),
  # k=2:
  ggplot(d1, aes(x=100*pc_same_remv, y=100*pc_diff_weird, color=km2.cluster, shape=km2.cluster, fill=km2.cluster)) + 
    geom_point(alpha=0.25) + geom_smooth(method="lm", alpha=0.25) + geom_polygon(data=hull_km2, alpha=0.10, linewidth=0.2) + 
    scale_colour_manual(name="cluster", values = colors_k2, label=labels_k2) +  
    scale_fill_manual(name="cluster", values = colors_k2, label=labels_k2) + 
    scale_shape_manual(name="cluster", values = shape_k2, label=labels_k2) +
    xlab("% 'same' remove") + ylab("% 'different' weird") + xlim(0,100) + ylim(0,100) + 
    theme(legend.position="bottom") + ggtitle("k=2"),
  # k=3:
  ggplot(d1, aes(x=100*pc_same_remv, y=100*pc_diff_weird, color=km3.cluster, shape=km3.cluster, fill=km3.cluster)) + 
    geom_point(alpha=0.25) + geom_smooth(method="lm", alpha=0.25) + geom_polygon(data=hull_km3, alpha=0.10, linewidth=0.2) + 
    scale_colour_manual(name="cluster", values = colors_k3, label=labels_k3) + 
    scale_fill_manual(name="cluster", values = colors_k3, label=labels_k3) + 
    scale_shape_manual(name="cluster", values = shape_k3, label=labels_k3) +
    xlab("% 'same' remove") + ylab("% 'different' weird") + xlim(0,100) + ylim(0,100) + 
    theme(legend.position="bottom") + ggtitle("k=3"),
  ncol=3);
```

```{r results='hide'}
ms_all <- lm(pc_diff_weird ~ pc_same_remv, data=d_all); summary(ms_all);                                                      # whole dataset:         beta=-0.73***
ms_2_1 <- lm(pc_diff_weird ~ pc_same_remv, data=d_all[d_all$ID %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["1"])],]); summary(ms_2_1); # 2 clusters, cluster 1: beta=-0.77***
ms_2_2 <- lm(pc_diff_weird ~ pc_same_remv, data=d_all[d_all$ID %in% names(km2$cluster)[km2$cluster == as.numeric(labels_k2["2"])],]); summary(ms_2_2); # 2 clusters, cluster 2: beta=-0.89***
ms_3_1 <- lm(pc_diff_weird ~ pc_same_remv, data=d_all[d_all$ID %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["1"])],]); summary(ms_3_1); # 3 clusters, cluster 1: beta=-0.77***
ms_3_2 <- lm(pc_diff_weird ~ pc_same_remv, data=d_all[d_all$ID %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["2"])],]); summary(ms_3_2); # 3 clusters, cluster 2: beta=-0.52***
ms_3_3 <- lm(pc_diff_weird ~ pc_same_remv, data=d_all[d_all$ID %in% names(km3$cluster)[km3$cluster == as.numeric(labels_k3["3"])],]); summary(ms_3_3); # 3 clusters, cluster 3: beta=-0.51***
```
This is precisely the pattern to be expected if the "weird" items behave like the 'same' items: the participants with higher performance on the same items have lower performance on the "weird" items because their responses are in fact "flipped".


#### Conclusions

It turns out that the tone task is far from simple, in particular, that the 5 items Bhx, Hst, Ist, Kkl1 and Llp (so-called "weird" items) clearly behave like the 'same' items and not as the other 'different' items.
While this is interesting question from a phonological/phonetic point of view, we must leave this for future research.

Here, the relevant questions is "what to do with these items?". 
There are three basic options: (1) leave them as they are (the "original" dataset), (2) simply drop them (the "removed" dataset), or (3) consider them as 'same' items and "flip" their responses (the "recode" dataset).
From the analyses performed above and theoretical considerations, we will focus on the "recode" dataset but also keep the original dataset for reference.

With these we have the following primary and secondary measures:

- **primary measures**: these are estimates of the performance at discriminating the tones: % correct responses overall and d' on the "original" and the "recode" datasets, and
- **secondary measures**: 
  + the *% correct responses overall* and *d'* on the "remove" dataset, and
  + *% correct responses* for *'same'* and *'different'* items separately, and the measure of bias *c* on all three datasets.
  
Finally, please note that while *d'* can arguably be modeled well using linear regression, for the *% correct responses* we have several potential choices:

i.    *linear regression on the % of correct responses*: this is arguably wrong as it assumes values outside the [0,100] range and we will not pursue it here except for complex path models where beta regression (see below) does not currently work and where the fitted models using both approaches are very similar.
ii.   *beta regression on the proportion of correct responses* (i.e., on the %/100.0): this is a technique implemented in `R` by, for example, `glmmTMB(..., family=beta_family())`, standardly used to model proportion data, but is has two drawbacks: it cannot deal with proportions of exactly 0.0 and 1.0, requiring these to be converted to something almost 0.0 and 1.0 respectively, and, for most participants, it produces relatively hard-to-interpret estimates.
iii. *logistic regression on the actual responses* (or, equivalently, on the counts of "0" and "1" responses): this is probably the best to model our data as logistic regression is very well supported in `R` and most practitioners know how to interpret its results.

Therefore, we will systematically perform **logistic regression on the counts of "0" ("incorrect") and "1" ("correct") responses**, but we might still plot and show the % of correct responses when appropriate.


# Results

```{r}
# Make gender numeric (F=0, M=1):
d_all$gender_n <- ifelse(d_all$gender == "F", 0, 1);
# Also binarize the location into A (0) and B (1), ignoring the other 3 participants from other locations:
d_all$location_bin <- ifelse(d_all$location == "A", 0, ifelse(d_all$location == "B", 1, NA)); 
# ... and as factor:
d_all$location_ab <- factor(ifelse(d_all$location %in% c("A", "B"), d_all$location, NA), levels=c("A", "B"));

# Save the data:
saveRDS(d_all, file="./cached_results/d_all.rds", compress="xz");
```

## Correlations between measures

**Pearson's:**

```{r fig.width=10, fig.height=10, fig.cap=capFig("Heatmap with clustering for the Pearson's correlations between the conitnuous measures of interest.")}
# Correlations:
tmp <- cor(d_all[,c("age", "education_years", "gender_n", "location_bin", 
                    "wm_norm", 
                    "pc_tot_orig", "pc_same_orig", "pc_diff_orig", "dprime_orig", "c_orig",
                    "pc_tot_recd", "pc_same_recd", "pc_diff_recd", "dprime_recd", "c_recd")], 
           method="pearson", use="pairwise.complete.obs");
#pander(round(tmp,2));
gplots::heatmap.2(tmp, cellnote=round(tmp,2), notecol="black", margins=c(12,12), 
                  labRow=c("age", "years education", "gender (F:M)", "location (A:B)", "working memory", 
                           "% total (original)", "% same (original)", "% different (original)", "d' (original)", "c (original)",
                           "% total (recode)",   "% same (recode)",   "% different (recode)",   "d' (recode)",   "c (recode)"), 
                  labCol=c("age", "years education", "gender (F:M)", "location (A:B)", "working memory", 
                           "% total (original)", "% same (original)", "% different (original)", "d' (original)", "c (original)",
                           "% total (recode)",   "% same (recode)",   "% different (recode)",   "d' (recode)",   "c (recode)"));
```

**Spearman's:**

```{r fig.width=10, fig.height=10, fig.cap=capFig("Heatmap with clustering for the Spearman's correlations between the conitnuous measures of interest.")}
# Correlations:
tmp <- cor(d_all[,c("age", "education_years", "gender_n", "location_bin", 
                    "wm_norm", 
                    "pc_tot_orig", "pc_same_orig", "pc_diff_orig", "dprime_orig", "c_orig",
                    "pc_tot_recd", "pc_same_recd", "pc_diff_recd", "dprime_recd", "c_recd")], 
           method="spearman", use="pairwise.complete.obs");
#pander(round(tmp,2));
gplots::heatmap.2(tmp, cellnote=round(tmp,2), notecol="black", margins=c(12,12), 
                  labRow=c("age", "years education", "gender (F:M)", "location (A:B)", "working memory", 
                           "% total (original)", "% same (original)", "% different (original)", "d' (original)", "c (original)",
                           "% total (recode)",   "% same (recode)",   "% different (recode)",   "d' (recode)",   "c (recode)"), 
                  labCol=c("age", "years education", "gender (F:M)", "location (A:B)", "working memory", 
                           "% total (original)", "% same (original)", "% different (original)", "d' (original)", "c (original)",
                           "% total (recode)",   "% same (recode)",   "% different (recode)",   "d' (recode)",   "c (recode)"));
```

It can be seen that:

- the results using either correlation estimate are very similar,
- *age* is negatively correlated (significantly or not) with all the other variables, especially strongly with *years of education* and the *working memory task*, and much less with the various measures of *tone task* performance,
- *gender* is correlated with *years of education* (higher for males),
- the *location* correlates with *years of education* (slightly higher in A) and with the performance on the tone task (especially the 'different' items, better for A),
- *% correct answers* and *d'* on both datasets behave very similarly: negatively affected by *ag*e, by *location* (better in A) and for more *education*, *gender* (slightly better for males), and positively effected by *working memory*, 
- *% correct answers for 'different'* items on both datasets are very similar and like % total and d' above,
- *% correct answers for the 'same'* items on both datasets are very similar and like % total and d' above,
- finally, the bias estimators *c* behave strikingly differently on both datasets: while on the *'original'* it is negatively affected by age and positively by education and working memory, on the *'recoded'* it does not seem influenced by any such covariate.

These bi-variate correlations, however, might hide more complex relationships between the measures of interest and covariates, that can be tested using multiple regression, mediation and (piecewise) path and structural models.


## The working memory task 

```{r results='hide'}
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/wm_task_results.rds") ) # computationally expensive
{
  ## Family and location:
  # Does the family structure matter?
  m_wm__fam <- lmer(wm_norm ~ 1 + (1 | family), data=d_all); summary(m_wm__fam);
  (icc_wm__fam <- performance::icc(m_wm__fam)); # 0.024
  # -> nope
  
  # ... but does the generation matter?
  m_wm__gen <- lm(wm_norm ~ generation, data=d_all); summary(m_wm__gen); # beta(old)=-0.082, p=0.12
  # -> nope
  # -> so, family and generation do not seem to matter (at our statistical power)...
  
  # Does location matter (it has missing data so important to know if we include it or not)?
  m_wm__location <- lm(wm_norm ~ location_ab, data=d_all); summary(m_wm__location); # beta=-0.008, p=0.69
  # -> nope
  
  # Keep only the relevant non-missing data:
  d <- na.omit(d_all[,c("wm_norm", "age", "gender", "gender_n", "education_years")]);
  
  ## Multiple regression:
  # Manual simplification starting with the full model:
  m_wm_full <- lm(wm_norm ~ age * gender * education_years, data=d); summary(m_wm_full); # we don't include the location
  m_wm <- update(m_wm_full, . ~ . - 
                   age:gender:education_years -  # 3-way interaction
                   age:education_years - age:gender - gender:education_years # 2-way interactions
  ); summary(m_wm); (anova_m_wm = anova(m_wm, m_wm_full));
  # -> age (-0.0058), gender (M: -0.032) and education_years (0.021) matter
  
  
  ## Mediation models:
  # gender is a proxy for education:
  med_wm__mediator <-  glm(education_years ~ gender_n, family=poisson(), data=d); summary(med_wm__mediator); # poisson regression as education_years is a count
  med_wm__outcome  <- lm(wm_norm ~ gender_n + education_years, data=d); summary(med_wm__outcome);
  med_wm_gender <- mediation::mediate(med_wm__mediator, med_wm__outcome, mediator="education_years", treat="gender_n", outcome="wm_norm", sims=10000, robustSE=TRUE); summary(med_wm_gender); plot(med_wm_gender); # mostly mediated effect, very little direct effect
  
  psem_wm_gender <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ gender_n, family=poisson(), data=d),
    lm(wm_norm ~ gender_n + education_years, data=d),
    data=d);
  (summary_psem_wm_gender <- summary(psem_wm_gender, .progressBar = FALSE));
  plot(psem_wm_gender);
  psem_wm_gender_full <- piecewiseSEM::psem( # just indirect effect (aka full mediation)
    glm(education_years ~ gender_n, family=poisson(), data=d),
    lm(wm_norm ~ education_years, data=d),
    data=d);
  (summary_psem_wm_gender_full <- summary(psem_wm_gender_full, .progressBar = FALSE));
  piecewiseSEM::dSep(psem_wm_gender_full, .progressBar = FALSE); # p = 0.008666379 **
  plot(psem_wm_gender_full);
  (psem_anova_wm_gender <- anova(psem_wm_gender, psem_wm_gender_full)); # p = 0.0084 **
  # -> there is a weak but important direct effect of gender (males are slightly worse) on top of the largely mediated effect (males have more education and education is better)
  
  # age has an independent effect from education:
  med_wm__mediator <-  glm(education_years ~ age, family=poisson(), data=d); summary(med_wm__mediator); # poisson regression as education_years is a count
  med_wm__outcome  <- lm(wm_norm ~ age + education_years, data=d); summary(med_wm__outcome);
  med_wm_age <- mediation::mediate(med_wm__mediator, med_wm__outcome, mediator="education_years", treat="age", outcome="wm_norm", sims=10000, robustSE=TRUE); summary(med_wm_age); plot(med_wm_age); # mostly direct effect, very little mediated effect
  
  psem_wm_age <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ age, family=poisson(), data=d),
    lm(wm_norm ~ age + education_years, data=d),
    data=d);
  (summary_psem_wm_age <- summary(psem_wm_age, .progressBar = FALSE));
  plot(psem_wm_age);
  psem_wm_age_full <- piecewiseSEM::psem( # just indirect effect (aka full mediation)
    glm(education_years ~ age, family=poisson(), data=d),
    lm(wm_norm ~ education_years, data=d),
    data=d);
  (summary_psem_wm_age_full <- summary(psem_wm_age_full, .progressBar = FALSE));
  piecewiseSEM::dSep(psem_wm_age_full, .progressBar = FALSE); # p = 2.653246e-17 ***
  plot(psem_wm_age_full);
  (psem_anova_wm_age <- anova(psem_wm_age, psem_wm_age_full)); # p = 0 ***
  # -> there is a clear negative direct effect of age besides the mediated effect (older participants have less education)
  
  # save them to file:
  wm_task_results <- list("icc_fam"=icc_wm__fam, "m_gen"=summary(m_wm__gen), "m_location"=summary(m_wm__location), # checks
                          "multreg"=summary(m_wm), # multiple regression model
                          "med_gender__education"=list("mediation"=summary(med_wm_gender),
                                                    "piecewise"=list("model"=psem_wm_gender, "dsep.p"=piecewiseSEM::dSep(psem_wm_gender_full, .progressBar = FALSE)$P.Value, "anova"=psem_anova_wm_gender)),
                          "med_age__education"=list("mediation"=summary(med_wm_age),
                                                    "piecewise"=list("model"=psem_wm_age, "dsep.p"=piecewiseSEM::dSep(psem_wm_age_full, .progressBar = FALSE)$P.Value, "anova"=psem_anova_wm_age)));
  saveRDS(wm_task_results, file=xzfile("./cached_results/wm_task_results.rds", compression=9));
} else
{
  wm_task_results <- readRDS(xzfile("./cached_results/wm_task_results.rds"));
}
```

We use here the *normalized working memory performance* estimate (*wm_norm*).

### Location and family

There are no significant differences between the two main locations (A and B) in terms of the working memory task performance (linear regression `r sprintf("*β~B-A~*=%.2g, *p*=%s", wm_task_results$m_location$coefficients[2,"Estimate"], scinot(wm_task_results$m_location$coefficients[2,"Pr(>|t|)"]))`), and, for those `r sum(!is.na(d_all$generation))` participants with information about family relationships, the generation they belong to also has no effect (linear regression `r sprintf("*β~old-young~*=%.2g, *p*=%s", wm_task_results$m_gen$coefficients[2,"Estimate"], scinot(wm_task_results$m_gen$coefficients[2,"Pr(>|t|)"]))`), and, moreover, there is no clustering within families (the linear model with family as a random effect has an ICC of `r round(wm_task_results$icc_fam$ICC_adjusted*100,1)`%), suggesting that we need not model these factors here.

### Multiple regression

A linear multiple linear regression of the *working memory task performance* on *age*, *gender* and *years of education* and all their interactions simplifies (using manual simplification based on F-test's *p* value) to a model with main effects only:
```{r}
wm_task_results$multreg;
```
```{r fig.width=4, fig.height=4, fig.cap=capFig("Untransformed slopes with standard errors.")}
sjPlot::plot_model(wm_task_results$multreg, show.values=TRUE, value.offset=.2, vline.color="gray20", transform=NULL);
```
suggesting that *age* has a negative effect, *years of education* a positive effect, and that males have worse performance than the females.
However, it is possible that the causal model is more complex, with the effect of *gender* and *age* largely mediated by the *years of education*.

### Mediation: gender → education → working memory

Indeed, fitting a mediation model where *gender* influences *working memory* through *years of education* (*N.B.* while the outcome model is a linear regression of working memory performance on the mediator and treatment, the mediator model is the Poisson regression of years of education on the treatment because the years of education is a count variable) finds a highly significant positive indirect effect (ACME), but also a significant direct effect (ADE); as fitted using `mediation::mediate()`:
```{r}
wm_task_results$med_gender__education$mediation;
```
and as fitted using `piecewiseSEM::psem()`:
```{r}
summary(wm_task_results$med_gender__education$piecewise$model);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model of *gender*, *years of education* and *working memory performance* showing the standardized coefficients (fitted using `piecewiseSEM::psem()`).")}
plot(wm_task_results$med_gender__education$piecewise$model, node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=8, width=1), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```
Moreover, testing this partial mediation model against the full mediation model (that does not include a direct effect but only the indirect effect) using d-separation and model comparison finds that while the effect of *gender* is mostly mediated through *years of education* (males have more and years of education are positively related to *working memory*), the direct effect of *gender* on *working memory* also matters (`r sprintf("d-sep *p*=%s, model comparison *χ*^2^(%d)=%.1f, *p*=%s", scinot(wm_task_results$med_gender__education$piecewise$dsep.p), as.numeric(wm_task_results$med_gender__education$piecewise$anova[[1]][2,"Df.diff"]), as.numeric(wm_task_results$med_gender__education$piecewise$anova[[1]][2,"Chisq.diff"]), scinot(as.numeric(wm_task_results$med_gender__education$piecewise$anova[[1]][2,"P.value"])))`).

### Mediation: age → education → working memory

Likewise, fitting a mediation model where *age* influences *working memory* through *years of education* finds a highly significant negative direct effect (ADE), but also a significant negative mediated effect (ACME); as fitted using `mediation::mediate()`:
```{r}
wm_task_results$med_age__education$mediation;
```
and as fitted using `piecewiseSEM::psem()`:
```{r}
summary(wm_task_results$med_age__education$piecewise$model);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model of *age*, *years of education* and *working memory performance* showing the standardized coefficients (fitted using `piecewiseSEM::psem()`).")}
plot(wm_task_results$med_age__education$piecewise$model, node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=8, width=1), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```
Moreover, the effect of *age* is split between the direct negative effect and the mediated (negative effect on *years of education*) effect (`r sprintf("d-sep *p*=%s, model comparison *χ*^2^(%d)=%.1f, *p*=%s", scinot(wm_task_results$med_age__education$piecewise$dsep.p), as.numeric(wm_task_results$med_age__education$piecewise$anova[[1]][2,"Df.diff"]), as.numeric(wm_task_results$med_age__education$piecewise$anova[[1]][2,"Chisq.diff"]), wm_task_results$med_age__education$piecewise$anova[[1]][2,"P.value"])`).

### Conclusions

The *working memory performance* is not influenced by the location (as expected) and does not cluster within families, but is *strongly positively* correlated with *years of education* (causality is unclear but could be bidirectional), is *strongly negatively* influenced by *age* both directly and indirectly by negatively influencing the years of education, and by *gender*, mainly indirectly as the males have more years of education, but also directly, as the males seem to have slightly worse performance than the females.


## The tone task 

```{r}
# Prepare the data:
# - with all the info for all individual items:
d_all_long <- merge(d_ppts, d_wm[,c("ID", "wm", "wm_norm")], by="ID", all.x=TRUE, all.y=FALSE);
d_all_long <- merge(d_all_long, d_tone[,c("ID", "presentation", "item_short", "tones", "correct")], all.x=TRUE, all.y=TRUE);
d_all_long$item_pattern <- substring(d_all_long$item_short, 1, nchar(d_all_long$item_short)-1);
d_all_long$is_weird <- factor(ifelse(d_all_long$item_short %in% weird_items, "yes", "no"), levels=c("no", "yes"));
d_all_long$item_type <- factor(ifelse(d_all_long$is_weird == "yes", "weird", d_all_long$tones), levels=c("same", "different", "weird"));
d_all_long$location_ab <- factor(ifelse(d_all_long$location %in% c("A", "B"), d_all_long$location, NA), levels=c("A", "B"));
d_all_long$correct_n <- as.numeric(d_all_long$correct == "yes"); # "yes" ->1, "no" -> 0
d_all_long$gender_n <- as.numeric(d_all_long$gender == "M"); # M -> 1, F -> 0
d_all_long$location_bin <- as.numeric(d_all_long$location_ab == "A"); # A -> 1, B -> 0
# - and summarized per participant:
d_all_grouped <- d_all_long %>% dplyr::group_by(ID) %>% 
  dplyr::summarise(age=unique(age), 
                   gender=unique(gender), gender_n=unique(gender_n),
                   music_years=unique(music_years), 
                   education_years=unique(education_years), 
                   location=unique(location), location_ab=unique(location_ab), location_bin=unique(location_bin),
                   family=unique(family), generation=unique(generation), 
                   wm=unique(wm), wm_norm=unique(wm_norm),
                   n_cor_orig=sum(correct=="yes"), n_inc_orig=sum(correct=="no"), per_cor_orig=sum(correct=="yes")/length(correct),
                   n_cor_remv=sum(correct=="yes" & is_weird=="no"), n_inc_remv=sum(correct=="no" & is_weird=="no"), per_cor_remv=sum(correct=="yes" & is_weird=="no")/sum(is_weird=="no"),
                   n_cor_recd=sum((correct=="yes" & is_weird=="no") | (correct=="no" & is_weird=="yes")), 
                   n_inc_recd=sum((correct=="no" & is_weird=="no") | (correct=="yes" & is_weird=="yes")), 
                   per_cor_recd=sum((correct=="yes" & is_weird=="no") | (correct=="no" & is_weird=="yes"))/length(correct));
# tmp <- merge(d_all_grouped, d_all); cor.test(tmp$per_cor_orig, tmp$pc_tot_orig); cor.test(tmp$per_cor_remv, tmp$pc_tot_remv); cor.test(tmp$per_cor_recd, tmp$pc_tot_recd); # all correlations are 1.0d -> good!
d_all_grouped <- merge(d_all_grouped, d_all[,c("ID", "dprime_orig", "c_orig", "dprime_recd", "c_recd")]); # add the SDT measures


if( FALSE )
{
  # Let's check various ways of modelling the correct/incorrect data:
  
  # Logistic models:
  m_flat_bin <- glm(correct_n ~ gender * location_ab, data=d_all_long, family=binomial("logit")); summary(m_flat_bin); sjPlot::plot_model(m_flat_bin, type="pred", terms=c("gender", "location_ab")); # binary "flat"
  m_counts   <- glm(cbind(n_cor_orig, n_inc_orig) ~ gender * location_ab, data=d_all_grouped, family=binomial("logit")); summary(m_counts); sjPlot::plot_model(m_counts, type="pred", terms=c("gender", "location_ab")); # counts "success" and "failure"
  # -> these two are virtually identical (as expected)
  m_mix_bin  <- glmer(correct_n ~ gender * location_ab + (1 | ID) + (1 | item_pattern), data=d_all_long, family=binomial("logit")); summary(m_mix_bin); sjPlot::plot_model(m_mix_bin, type="pred", terms=c("gender", "location_ab")); # mixed-effects binary
  icc(m_mix_bin); # 43%
  # -> much slower but better at dealing with the participant (and item) grouping
  
  # And beta regression:
  m_beta     <- glmmTMB(per_cor_orig ~ gender * location_ab, data=d_all_grouped, family=beta_family()); summary(m_beta); sjPlot::plot_model(m_beta, type="pred", terms=c("gender", "location_ab")); # beta "flat"
  AIC(m_mix_bin); # 71718.51
  AIC(m_beta); # -784.8951
  # -> the beta regression is quite similar to the mixed-effects logistic but much faster and its AIC is much smaller
  
  # How to model the "weird" items:
  m_mix_weird <- glmer(correct_n ~ gender * location_ab * item_type + (1 | ID) + (1 | item_pattern), data=d_all_long, family=binomial("logit")); summary(m_mix_weird); sjPlot::plot_model(m_mix_weird, type="pred", terms=c("gender", "location_ab", "item_type"));
  # -> confirms that the weird items behave exactly in the opposite way as expected.
  
  ## Take-home messages: 
  ## - the counts logistic is the same as the "flat" logistic (i.e., it does not model the participant level at all)
  ## - beta regression is similar to the mixed-effect logistics (but much faster)
  ## - beta regression is still "flat" (i.e., it cannot model item-level factors)
}
```

Given that the tone task results can be potentially analyses in several ways, we conducted a preliminary comparison to decide what would be the best approach here:

a. *"flat" logistic regression* (i.e., the participants and the items are "pooled" together): this has the advantage that is a very simple and well-understood regression model and works on the actual responses by a given individual to a given item, allowing thus, in principle, the modelling of item characteristics (e.g., 'same' vs 'different', 'weird' vs 'non-weird' or if it contains an actual word), but suffers from the same drawback as all models that ignore the hierarchical structure of the data namely that their estimation of significance may be extremely biased,
b. *mixed-effects logistic regression* including the participant and item as crossed random effects (aka `... + (1 | participant) + (1 | item)`) which keeps the advantages of (a) but properly deal with its main shortcoming, and
c. *beta regression on the % correct responses*: this collapses the responses across items for a given participant addressing the issues of participant (but not of item) clustering.

We compared these on our data and we found that:

- all these methods produced comparable regression coefficient estimates, but
- method (a) is inappropriate as it results in overly optimistic *p*-values,
- the (adjusted) ICC of the participants and items is around 40%, confirming that we need to somehow address this clustering (and confirming the inadequacy of (a)),
- methods (b) and (c) agree not only in the point estimates but also in their standard errors and *p*-values
- method (b) is slower and may have convergence issues for complex fixed effects structures
- by using a single estimate per participant, (c) is similar both with the approach in @wong_aspm_2020 and with the use of *d'*
- likewise, it is easier to introduce in mediation models using other variables of interest that have a single value per participant (e.g., gender, working memory, years of education...).

Therefore, **we will perform the *beta regression* of the % correct responses**, using a mixed-effects logistic regression as sanity check in some cases.


### % correct total (recode)


```{r results='hide'}
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/tone_pcr_results.rds") ) # computationally expensive
{
  ## Prepare for beta regression:
  d_all$pcr <- prepare_for_beta_regression(d_all$pc_tot_recd);
  
  ## Family and location:
  # Does the family structure matter?
  m_pcr__fam <- glmmTMB(pcr ~ 1 + (1 | family), family=beta_family(), control=beta_ctrl, data=d_all); summary(m_pcr__fam); DHARMa::testDispersion(m_pcr__fam, plot=FALSE);
  (icc_pcr__fam <- performance::icc(m_pcr__fam)); # 1.7/100 !!! (but with warning "Results are not reliable")
  # so let's try a linear model:
  m_pcr__fam <- lmer(pcr ~ 1 + (1 | family), data=d_all); summary(m_pcr__fam); DHARMa::testDispersion(m_pcr__fam, plot=FALSE);
  (icc_pcr__fam <- performance::icc(m_pcr__fam)); # 9.9%
  # and also a beta regression but with family as fixed effect:
  m_pcr__fam <- glmmTMB(pcr ~ family, family=beta_family(), control=beta_ctrl, data=d_all[ !is.na(d_all$family), ]); summary(m_pcr__fam); DHARMa::testDispersion(m_pcr__fam, plot=FALSE);
  (anova_pcr__fam <- anova(m_pcr__fam, update(m_pcr__fam, . ~ . -family, data=d_all[ !is.na(d_all$family), ]))); # p=0.016 *
  # -> tone performance seems to be somewhat clustered within (some) families, but we don't have enough data and the amount of explained variance is small, so we'll ignore it
  
  # ... but does the generation matter?
  m_pcr__gen <- glmmTMB(pcr ~ generation, family=beta_family(), control=beta_ctrl, data=d_all); summary(m_pcr__gen); # beta=-0.019, p=0.93
  # -> nope
  # -> so, we will not model family and generation...
  
  # Does location matter (it has missing data so important to know if we include it or not)?
  m_pcr__location <- glmmTMB(pcr ~ location_ab, family=beta_family(), control=beta_ctrl, data=d_all); summary(m_pcr__location); # beta=-0.45, p=1.45e-09 ***
  # -> YES (A is better)
  
  # Keep only the relevant non-missing data:
  d <- na.omit(d_all[,c("pcr", "age", "gender", "gender_n", "education_years", "location_ab", "location_bin", "wm_norm")]);
  
  ## Multiple regression:
  # Manual simplification starting with the full model:
  m_pcr_full <- glmmTMB(pcr ~ age * gender * education_years * location_ab * wm_norm, family=beta_family(), control=beta_ctrl, data=d); summary(m_pcr_full);
  m_pcr <- update(m_pcr_full, . ~ . - 
                    age:gender:education_years:location_ab:wm_norm - # 5-way interaction
                    age:gender:location_ab:wm_norm - age:education_years:location_ab:wm_norm - gender:education_years:location_ab:wm_norm - age:gender:education_years:wm_norm - age:gender:education_years:location_ab - # 4-way interactions
                    gender:location_ab:wm_norm - education_years:location_ab:wm_norm - age:location_ab:wm_norm - age:gender:location_ab - age:gender:wm_norm - 
                    gender:education_years:wm_norm - age:education_years:wm_norm - age:gender:education_years - age:education_years:location_ab - gender:education_years:location_ab - # 3-way interactions
                    age:wm_norm - gender:location_ab - age:location_ab - gender:wm_norm - education_years:wm_norm - location_ab:wm_norm - gender:education_years - education_years:location_ab # 2-way interactions
  ); summary(m_pcr); (anova_m_pcr = anova(m_pcr, m_pcr_full));
  sjPlot::plot_model(m_pcr, transform=NULL); 
  sjPlot::plot_model(m_pcr, type="pred", terms=c("age", "gender"));
  sjPlot::plot_model(m_pcr, type="pred", terms=c("education_years", "age"));

  
  ## Mediation models:
  
  ## gender is a proxy for education:
  med_pcr__mediator <-  glm(education_years ~ gender_n, family=poisson(), data=d); summary(med_pcr__mediator); # poisson regression as education_years is a count
  med_pcr__outcome  <- lm(pcr ~ gender_n + education_years, data=d); summary(med_pcr__outcome); # mediation::mediate cannot do beta regression so let's use linear regression instead...
  summary(glmmTMB(pcr ~ gender_n + education_years, family=beta_family(), control=beta_ctrl, data=d)); # ... and the estimates and p-values are very similar between the two
  med_pcr_gender <- mediation::mediate(med_pcr__mediator, med_pcr__outcome, mediator="education_years", treat="gender_n", outcome="pcr", sims=10000, robustSE=TRUE); summary(med_pcr_gender); plot(med_pcr_gender); # no direct effect
  
  # use beta regression:
  psem_pcr_gender_beta <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ gender_n, family=poisson(), data=d),
    glmmTMB(pcr ~ gender_n + education_years, family=beta_family(), control=beta_ctrl, data=d),
    data=d);
  summary_psem_pcr_gender_beta <- NULL; #(summary_psem_pcr_gender_beta <- summary(psem_pcr_gender_beta, .progressBar = FALSE));
  #plot(psem_pcr_gender_beta, show="unstd"); # standardized not available
  # the full mediation model does not work...
  
  # use linear regression (very similar estimates and p-values):
  psem_pcr_gender <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ gender_n, family=poisson(), data=d),
    lm(pcr ~ gender_n + education_years, data=d),
    data=d);
  (summary_psem_pcr_gender <- summary(psem_pcr_gender, .progressBar = FALSE));
  plot(psem_pcr_gender, show="unstd"); # standardized not available for education_years
  psem_pcr_gender_full <- piecewiseSEM::psem( # just indirect effect (aka full mediation)
    glm(education_years ~ gender_n, family=poisson(), data=d),
    lm(pcr ~ education_years, data=d),
    data=d);
  (summary_psem_pcr_gender_full <- summary(psem_pcr_gender_full, .progressBar = FALSE));
  piecewiseSEM::dSep(psem_pcr_gender_full, .progressBar = FALSE); # p = 0.90
  plot(psem_pcr_gender_full, show="unstd"); # standardized not available for education_years
  (psem_anova_pcr_gender <- anova(psem_pcr_gender, psem_pcr_gender_full)); # p = 0.90
  # -> no direct effect of gender, but all is mediated through education
  
  
  ## age has an independent effect from education:
  med_pcr__mediator <-  glm(education_years ~ age, family=poisson(), data=d); summary(med_pcr__mediator); # poisson regression as education_years is a count
  med_pcr__outcome  <- lm(pcr ~ age + education_years, data=d); summary(med_pcr__outcome); # mediation::mediate cannot do beta regression so let's use linear regression instead...
  summary(glmmTMB(pcr ~ age + education_years, family=beta_family(), control=beta_ctrl, data=d)); # ... and the estimates and p-values are relatively (but not very) similar between the two
  med_pcr_age <- mediation::mediate(med_pcr__mediator, med_pcr__outcome, mediator="education_years", treat="age", outcome="pcr", sims=10000, robustSE=TRUE); summary(med_pcr_age); plot(med_pcr_age); # positive direct effect and negative indirect effect
  
  # use beta regression:
  psem_pcr_age_beta <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ age, family=poisson(), data=d),
    glmmTMB(pcr ~ age + education_years, family=beta_family(), control=beta_ctrl, data=d),
    data=d);
  summary_psem_pcr_age_beta <- NULL; #(summary_psem_pcr_age_beta <- summary(psem_pcr_age_beta, .progressBar = FALSE));
  plot(psem_pcr_age_beta, show="unstd"); # standardized not available
  # the full mediation model does not work...
  
  # use linear regression (relatively similar estimates and p-values):
  psem_pcr_age <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ age, family=poisson(), data=d),
    lm(pcr ~ age + education_years, data=d),
    data=d);
  (summary_psem_pcr_age <- summary(psem_pcr_age, .progressBar = FALSE));
  plot(psem_pcr_age, show="unstd"); # standardized not available for education_years
  psem_pcr_age_full <- piecewiseSEM::psem( # just indirect effect (aka full mediation)
    glm(education_years ~ age, family=poisson(), data=d),
    lm(pcr ~ education_years, data=d),
    data=d);
  (summary_psem_pcr_age_full <- summary(psem_pcr_age_full, .progressBar = FALSE));
  piecewiseSEM::dSep(psem_pcr_age_full, .progressBar = FALSE); # p = 0.0048 **
  plot(psem_pcr_age_full, show="unstd"); # standardized not available for education_years
  (psem_anova_pcr_age <- anova(psem_pcr_age, psem_pcr_age_full)); # p = 0.0047 **
  # -> there is a positive direct effect of age besides the mediated effect (older participants have less education)
  
  
  ## location has an independent effect from education:
  med_pcr__mediator <-  glm(education_years ~ location_bin, family=poisson(), data=d); summary(med_pcr__mediator); # poisson regression as education_years is a count
  med_pcr__outcome  <- lm(pcr ~ location_bin + education_years, data=d); summary(med_pcr__outcome); # mediation::mediate cannot do beta regression so let's use linear regression instead...
  summary(glmmTMB(pcr ~ location_bin + education_years, family=beta_family(), control=beta_ctrl, data=d)); # ... and the estimates and p-values are relatively (but not very) similar between the two
  med_pcr_location <- mediation::mediate(med_pcr__mediator, med_pcr__outcome, mediator="education_years", treat="location_bin", outcome="pcr", sims=10000, robustSE=TRUE); summary(med_pcr_location); plot(med_pcr_location); # negative direct and indirect effects
  
  # use beta regression:
  psem_pcr_location_beta <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ location_bin, family=poisson(), data=d),
    glmmTMB(pcr ~ location_bin + education_years, family=beta_family(), control=beta_ctrl, data=d),
    data=d);
  summary_psem_pcr_location_beta <- NULL; #(summary_psem_pcr_location_beta <- summary(psem_pcr_location_beta, .progressBar = FALSE));
  plot(psem_pcr_location_beta, show="unstd"); # standardized not available
  # the full mediation model does not work...
  
  # use linear regression (relatively similar estimates and p-values):
  psem_pcr_location <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ location_bin, family=poisson(), data=d),
    lm(pcr ~ location_bin + education_years, data=d),
    data=d);
  (summary_psem_pcr_location <- summary(psem_pcr_location, .progressBar = FALSE));
  plot(psem_pcr_location, show="unstd"); # standardized not available for education_years
  psem_pcr_location_full <- piecewiseSEM::psem( # just indirect effect (aka full mediation)
    glm(education_years ~ location_bin, family=poisson(), data=d),
    lm(pcr ~ education_years, data=d),
    data=d);
  (summary_psem_pcr_location_full <- summary(psem_pcr_location_full, .progressBar = FALSE));
  piecewiseSEM::dSep(psem_pcr_location_full, .progressBar = FALSE); # p = 9.29e-05 ***
  plot(psem_pcr_location_full, show="unstd"); # standardized not available for education_years
  (psem_anova_pcr_location <- anova(psem_pcr_location, psem_pcr_location_full)); # p = 1e-04 ***
  # -> the direct effect is needed
  
  ## working memory has a direct effect besides education, age and gender would predict:
  med_pcr__mediator <-  lm(wm_norm ~ education_years + gender_n + age, data=d); summary(med_pcr__mediator);
  med_pcr__outcome  <- lm(pcr ~ wm_norm + education_years + gender_n + age, data=d); summary(med_pcr__outcome); # mediation::mediate cannot do beta regression so let's use linear regression instead...
  summary(glmmTMB(pcr ~ wm_norm + education_years + gender_n + age, family=beta_family(), control=beta_ctrl, data=d)); # ... and the estimates and p-values are very similar between the two
  med_pcr_wm <- mediation::mediate(med_pcr__mediator, med_pcr__outcome, mediator="wm_norm", treat="education_years", outcome="pcr", sims=10000, robustSE=TRUE); summary(med_pcr_wm); plot(med_pcr_wm);
  
  # use beta regression:
  psem_pcr_wm_beta <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    lm(wm_norm ~ education_years + gender_n + age, data=d),
    glmmTMB(pcr ~ wm_norm + education_years + gender_n + age, family=beta_family(), control=beta_ctrl, data=d),
    data=d);
  summary_psem_pcr_wm_beta <- NULL; #(summary_psem_pcr_wm_beta <- summary(psem_pcr_wm_beta, .progressBar = FALSE));
  plot(psem_pcr_wm_beta, show="unstd"); # standardized not available
  
  # use linear regression (relatively similar estimates and p-values):
  psem_pcr_wm <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    lm(wm_norm ~ education_years + gender_n + age, data=d),
    lm(pcr ~ wm_norm + education_years + gender_n + age, data=d),
    data=d);
  (summary_psem_pcr_wm <- summary(psem_pcr_wm, .progressBar = FALSE));
  plot(psem_pcr_wm, show="unstd"); # standardized not available

  
  ## More complex path model:
  psem_pcr_path_beta <- piecewiseSEM::psem(
    glm(education_years ~ age + gender_n + location_bin, family=poisson(), data=d), # age + gender -> education
    lm(age ~ location_bin + gender_n, data=d), # location + gender -> age
    glm(gender_n ~ location_bin, family=binomial(), data=d), # location -> gender
    lm(wm_norm ~ education_years + gender_n + age, data=d), # age + gender + education -> wm
    glmmTMB(pcr ~ wm_norm + age + gender_n + education_years + location_bin, family=beta_family(), control=beta_ctrl, data=d), # all -> pcr
    data=d);
  summary_psem_pcr_path_beta <- NULL; #(summary_psem_pcr_path_beta <- summary(psem_pcr_path_beta, .progressBar = FALSE));
  plot(psem_pcr_path_beta, show="unstd"); # standardized not available
  
  # use linear regression (relatively similar estimates and p-values):
  psem_pcr_path <- piecewiseSEM::psem(
    glm(education_years ~ age + gender_n + location_bin, family=poisson(), data=d), # age + gender -> education
    lm(age ~ location_bin + gender_n, data=d), # location + gender -> age
    glm(gender_n ~ location_bin, family=binomial(), data=d), # location -> gender
    lm(wm_norm ~ education_years + gender_n + age, data=d), # age + gender + education -> wm
    lm(pcr ~ wm_norm + age + gender_n + education_years + location_bin, data=d), # all -> pcr
    data=d);
  (summary_psem_pcr_path <- summary(psem_pcr_path, .progressBar = FALSE));
  plot(psem_pcr_path, show="unstd"); # standardized not available
  
  
  # save them to file:
  tone_pcr_results <- list("icc_fam"=icc_pcr__fam ,"anova_fam"=anova_pcr__fam, "m_gen"=summary(m_pcr__gen), "m_location"=summary(m_pcr__location), # checks
                           "multreg"=m_pcr, # multiple regression model
                           "med_gender__education"=list("mediation"=summary(med_pcr_gender),
                                                     "piecewise"=list("betareg"=list("model"=psem_pcr_gender_beta, "summary"=summary_psem_pcr_gender_beta),
                                                                      "linearreg"=list("model"=psem_pcr_gender, "dsep.p"=piecewiseSEM::dSep(psem_pcr_gender_full, .progressBar = FALSE)$P.Value, "anova"=psem_anova_pcr_gender))),
                           "med_age__education"=list("mediation"=summary(med_pcr_age),
                                                     "piecewise"=list("betareg"=list("model"=psem_pcr_age_beta, "summary"=summary_psem_pcr_age_beta),
                                                                      "linearreg"=list("model"=psem_pcr_age, "dsep.p"=piecewiseSEM::dSep(psem_pcr_age_full, .progressBar = FALSE)$P.Value, "anova"=psem_anova_pcr_age))),
                           "med_location__education"=list("mediation"=summary(med_pcr_location),
                                                     "piecewise"=list("betareg"=list("model"=psem_pcr_location_beta, "summary"=summary_psem_pcr_location_beta),
                                                                      "linearreg"=list("model"=psem_pcr_location, "dsep.p"=piecewiseSEM::dSep(psem_pcr_location_full, .progressBar = FALSE)$P.Value, "anova"=psem_anova_pcr_location))),
                           "med_wm__all"=list("mediation"=summary(med_pcr_wm),
                                              "piecewise"=list("betareg"=list("model"=psem_pcr_wm_beta, "summary"=summary_psem_pcr_wm_beta),
                                                               "linearreg"=list("model"=psem_pcr_wm))),
                           "path_model"=list("betareg"=list("model"=psem_pcr_path_beta, "summary"=summary_psem_pcr_path_beta),
                                             "linearreg"=list("model"=psem_pcr_path, "summary"=summary_psem_pcr_path)),
                           "data"=d);
  saveRDS(tone_pcr_results, file=xzfile("./cached_results/tone_pcr_results.rds", compression=9));
} else
{
  tone_pcr_results <- readRDS(xzfile("./cached_results/tone_pcr_results.rds"));
}
```

We focus here on the *% total correct responses on the 'recoded' dataset* estimate (*pcr* = *p*ercent *c*orrect *r*ecoded).
Given that these are percents bounded, by definition, between 0% and 100%, we used beta regression (as implemented by `glmmTMB::glmmTMB()`); however, the mediation modelling function `mediation::mediate` has troubles with beta regression, as has `piecewiseSEM::psem()` when it comes to fitting the full mediation model, so we employed in these cases the equivalent linear regressions (the relevant coefficient estimates and p-values are similar enough between the two to ensure that the qualitative conclusions hold).
Moreover, `piecewiseSEM` has troubles estimating the standardized path coefficients, so we report here the *un*standardized ones.

#### Location and family

There is a highly significant difference between the two main locations (A has higher overall performance than B; beta regression `r sprintf("*β~B-A~*=%.2g, *p*=%s", tone_pcr_results$m_location$coefficients$cond[2,"Estimate"], scinot(tone_pcr_results$m_location$coefficients$cond[2,"Pr(>|z|)"]))`).
For those `r sum(!is.na(d_all$generation))` participants with information about family relationships, the generation they belong to has no effect (beta regression `r sprintf("*β~old-young~*=%.2g, *p*=%s", tone_pcr_results$m_gen$coefficients$cond[2,"Estimate"], scinot(tone_pcr_results$m_gen$coefficients$cond[2,"Pr(>|z|)"]))`).
However, there is a slight clustering within families (the linear model with family as a random effect has an ICC of `r round(100*tone_pcr_results$icc_fam$ICC_adjusted,1)`%, and including family as a fixed effect in a "flat" beta regression vs excluding it results in a *p*=`r scinot(tone_pcr_results$anova_fam[2,"Pr(>Chisq)"])`), but given the loss of sample size and the relatively small amount of variation explained we will ignore it here.

#### Multiple regression

A beta multiple linear regression on *working memory*, *age*, *gender*, *years of education* and *location* and all their interactions simplifies (using manual simplification based on F-test's *p* value) to a model with main effects and two 2-way interactions only:
```{r}
summary(tone_pcr_results$multreg);
```
```{r fig.width=4, fig.height=4, fig.cap=capFig("Untransformed slopes with standard errors.")}
sjPlot::plot_model(tone_pcr_results$multreg, show.values=TRUE, value.offset=.2, vline.color="gray20", transform=NULL);
```
```{r fig.width=4, fig.height=4, fig.cap=capFig("Predicted values of pcr showing the interaction of*gender* and *age*.")}
sjPlot::plot_model(tone_pcr_results$multreg, type="pred", terms=c("age", "gender")) + ylim(0,1);
```
```{r fig.width=4, fig.height=4, fig.cap=capFig("Predicted values of pcr showing the interaction of *education_years* and *age*.")}
sjPlot::plot_model(tone_pcr_results$multreg, type="pred", terms=c("education_years", "age")) + ylim(0,1);
```
suggesting that *age* and *years of education* have no main effects, that *working memory* does have a positive effect, and that *males* have better performance than the females but there is an interaction between education and age, and between age and gender, and that participants from *A* have better performance than those from B.
As above, we test more complex of causality using mediation and path analysis.

#### Mediation: gender → education → tone

The mediation model where *gender* influences *tone* through *years of education* finds a highly significant positive indirect effect (ACME) and no direct effect (ADE); as fitted using `mediation::mediate()` (with linear regression):
```{r}
tone_pcr_results$med_gender__education$mediation;
```
and as fitted using `piecewiseSEM::psem()` (using linear regression):
```{r}
summary(tone_pcr_results$med_gender__education$piecewise$linearreg$model, , .progressBar = FALSE);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model of *gender*, *years of education* and *pcr* showing the *un*standardized coefficients (fitted using `piecewiseSEM::psem()`); dotted arrows are not significant.")}
plot(tone_pcr_results$med_gender__education$piecewise$linearreg$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=8, width=1), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```
Moreover, testing this partial mediation model against the full mediation model using d-separation and model comparison (N.B., using linear regression) finds that there is no direct effect of *gender*, but that it is entirely mediated through *years of education* (`r sprintf("d-sep *p*=%s, model comparison *χ*^2^(%d)=%.2g, *p*=%s", scinot(tone_pcr_results$med_gender__education$piecewise$linearreg$dsep.p), as.numeric(tone_pcr_results$med_gender__education$piecewise$linearreg$anova[[1]][2,"Df.diff"]), as.numeric(tone_pcr_results$med_gender__education$piecewise$linearreg$anova[[1]][2,"Chisq.diff"]), scinot(as.numeric(tone_pcr_results$med_gender__education$piecewise$linearreg$anova[[1]][2,"P.value"])))`).

#### Mediation: age → education → tone

Likewise, fitting a mediation model where *age* influences *tone* through *years of education* finds a significant positive direct (ADE) and a significant negative mediated effect (ACME); as fitted using `mediation::mediate()` (with linear regression):
```{r}
tone_pcr_results$med_age__education$mediation;
```
and as fitted using `piecewiseSEM::psem()` (using linear regression):
```{r}
summary(tone_pcr_results$med_age__education$piecewise$linearreg$model, .progressBar = FALSE);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model of *age*, *years of education* and *pcr* showing the *un*standardized coefficients (fitted using `piecewiseSEM::psem()`).")}
plot(tone_pcr_results$med_age__education$piecewise$linearreg$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=8, width=1), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```
Moreover, the effect of *age* is split between the direct positive effect and the mediated (negative effect on *years of education*) effect (`r sprintf("d-sep *p*=%s, model comparison *χ*^2^(%d)=%.1f, *p*=%s", scinot(tone_pcr_results$med_age__education$piecewise$linearreg$dsep.p), as.numeric(tone_pcr_results$med_age__education$piecewise$linearreg$anova[[1]][2,"Df.diff"]), as.numeric(tone_pcr_results$med_age__education$piecewise$linearreg$anova[[1]][2,"Chisq.diff"]), tone_pcr_results$med_age__education$piecewise$linearreg$anova[[1]][2,"P.value"])`).

#### Mediation: location → education → tone

Likewise, fitting a mediation model where *location* influences *tone* through *years of education* finds significant negative (i.e., that Xinming has worse performance than A) direct (ADE) and mediated (ACME) effects; as fitted using `mediation::mediate()` (with linear regression):
```{r}
tone_pcr_results$med_location__education$mediation;
```
and as fitted using `piecewiseSEM::psem()` (using linear regression):
```{r}
summary(tone_pcr_results$med_location__education$piecewise$linearreg$model, .progressBar = FALSE);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model of *age*, *years of education* and *pcr* showing the *un*standardized coefficients (fitted using `piecewiseSEM::psem()`).")}
plot(tone_pcr_results$med_location__education$piecewise$linearreg$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=8, width=1), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```
Moreover, the effect of *location* is split between the direct and the mediated effects (`r sprintf("d-sep *p*=%s, model comparison *χ*^2^(%d)=%.1f, *p*=%s", scinot(tone_pcr_results$med_location__education$piecewise$linearreg$dsep.p), as.numeric(tone_pcr_results$med_location__education$piecewise$linearreg$anova[[1]][2,"Df.diff"]), as.numeric(tone_pcr_results$med_location__education$piecewise$linearreg$anova[[1]][2,"Chisq.diff"]), tone_pcr_results$med_location__education$piecewise$linearreg$anova[[1]][2,"P.value"])`).

#### Mediation: education, age, gender → working memory → tone

Here we want to check if there is any effect of working memory on tone above and beyond that of education, age and gender, and we found both significant positive direct (ADE) and mediated (ACME) effects; as fitted using `mediation::mediate()` (with linear regression):
```{r}
tone_pcr_results$med_wm__all$mediation;
```
and as fitted using `piecewiseSEM::psem()` (using linear regression):
```{r}
summary(tone_pcr_results$med_wm__all$piecewise$linearreg$model);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model of *education*, *age*, *gender*, *wm* and *pcr* showing the *un*standardized coefficients (fitted using `piecewiseSEM::psem()`).")}
plot(tone_pcr_results$med_wm__all$piecewise$linearreg$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=6, width=0.65), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```
It can be seen that, indeed, *working memory* has a positive effect on *tone* while also mediating some of the effects of *age* and *education*.

#### Complex path model

Finally, we fitted a complex path model involving all these factors using `piecewiseSEM::psem()` (with linear regression):
```{r}
summary(tone_pcr_results$path_model$linearreg$model, .progressBar = FALSE);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("The complex path model for *pcr* showing the *un*standardized coefficients (fitted using `piecewiseSEM::psem()`).")}
plot(tone_pcr_results$path_model$linearreg$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=6, width=0.65), edge_attrs = data.frame(style = "solid", color = "gray80", fontsize=6),digits=2);
```
This confirms and extends the previous simpler mediation models in that *pcr* is directly influenced by *working memory* (positively), *years of education* (more is better), *age* (older is better) and *location* (A is better), but also indirectly by *gender* (mediated through *years of education* and *age*); both *age* and *location* also have effects mediated through *years of education*, and *age* and *years of education* have effects mediated through *working memory*. 

#### Conclusions

The *tone task performance* as measured by the *% correct total responses in the recoded dataset* (aka *pcr*) does somewhat cluster within families (but we ignore this here due to the low sample size and variance explained), but is done better by participants from A, who are older, who have more education and who have better working memory performance, while gender has only an indirect effect.


### d' (recode)

```{r results='hide'}
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/tone_dpr_results.rds") ) # computationally expensive
{
  ## The distribution of d' seems vaguely normal, so let's go for a linear model
  d_all$dpr <- d_all$dprime_recd;
  
  ## Family and location:
  # Does the family structure matter?
  m_dpr__fam <- lmer(dpr ~ 1 + (1 | family), data=d_all); summary(m_dpr__fam);
  (icc_dpr__fam <- performance::icc(m_dpr__fam)); # 4.4%
  # -> nope

  # ... but does the generation matter?
  m_dpr__gen <- lm(dpr ~ generation, data=d_all); summary(m_dpr__gen); # beta=0.12, p=0.62
  # -> nope
  # -> so, family and generation do not seem to matter (at our statistical power)...
  
  # Does location matter (it has missing data so important to know if we include it or not)?
  m_dpr__location <- lm(dpr ~ location_ab, data=d_all); summary(m_dpr__location); # beta=-0.63, p=1.56e-10 ***
  # -> YES (A is better)

  # Keep only the relevant non-missing data:
  d <- na.omit(d_all[,c("dpr", "age", "gender", "gender_n", "education_years", "location_ab", "location_bin", "wm_norm")]);
  
  ## Multiple regression:
  # Manual simplification starting with the full model:
  m_dpr_full <- lm(dpr ~ age * gender * education_years * location_ab * wm_norm, data=d); summary(m_dpr_full); par(mfrow=c(2,2)); plot(m_dpr_full); par(mfrow=c(1,1)); # diagnostics are decent so indeed linear is fine
  m_dpr <- update(m_dpr_full, . ~ . - 
                    age:gender:education_years:location_ab:wm_norm - # 5-way interaction
                    age:gender:education_years:wm_norm - age:gender:location_mxXinmin:wm_norm - gender:education_years:location_ab:wm_norm - age:gender:location_ab:wm_norm - 
                    age:education_years:location_ab:wm_norm - age:gender:education_years:location_ab - # 4-way interactions
                    genderM:location_ab:wm_norm - gender:location_ab:wm_norm - age:education_years:wm_norm - age:gender:wm_norm - education_years:location_ab:wm_norm - age:gender:location_ab - 
                    gender:education_years:location_ab - gender:education_years:wm_norm - age:gender:education_years - age:location_ab:wm_norm - age:education_years:location_ab - # 3-way interactions
                    age:wm_norm - gender:wm_norm - age:location_ab - location_ab:wm_norm - age:gender - gender:location_ab - education_years:wm_norm - education_years:location_ab # 2-way interactions
  ); summary(m_dpr); (anova_m_dpr = anova(m_dpr, m_dpr_full));
  sjPlot::plot_model(m_dpr, transform=NULL); 
  sjPlot::plot_model(m_dpr, type="pred", terms=c("education_years", "gender")); 
  sjPlot::plot_model(m_dpr, type="pred", terms=c("education_years", "age"));

  
  ## Mediation models:
  # gender is a proxy for education:
  med_dpr__mediator <-  glm(education_years ~ gender_n, family=poisson(), data=d); summary(med_dpr__mediator); # poisson regression as education_years is a count
  med_dpr__outcome  <- lm(dpr ~ gender_n + education_years, data=d); summary(med_dpr__outcome);
  med_dpr_gender <- mediation::mediate(med_dpr__mediator, med_dpr__outcome, mediator="education_years", treat="gender_n", outcome="dpr", sims=10000, robustSE=TRUE); summary(med_dpr_gender); plot(med_dpr_gender); # no direct effect
  
  psem_dpr_gender <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ gender_n, family=poisson(), data=d),
    lm(dpr ~ gender_n + education_years, data=d),
    data=d);
  (summary_psem_dpr_gender <- summary(psem_dpr_gender, .progressBar = FALSE));
  plot(psem_dpr_gender, show="unstd"); # standardized not available for education_years
  psem_dpr_gender_full <- piecewiseSEM::psem( # just indirect effect (aka full mediation)
    glm(education_years ~ gender_n, family=poisson(), data=d),
    lm(dpr ~ education_years, data=d),
    data=d);
  (summary_psem_dpr_gender_full <- summary(psem_dpr_gender_full, .progressBar = FALSE));
  piecewiseSEM::dSep(psem_dpr_gender_full, .progressBar = FALSE); # p = 0.91
  plot(psem_dpr_gender_full, show="unstd"); # standardized not available for education_years
  (psem_anova_dpr_gender <- anova(psem_dpr_gender, psem_dpr_gender_full)); # p = 0.91
  # -> no direct effect of gender, but all is mediated through education
  
  # age has an independent effect from education:
  med_dpr__mediator <-  glm(education_years ~ age, family=poisson(), data=d); summary(med_dpr__mediator); # poisson regression as education_years is a count
  med_dpr__outcome  <- lm(dpr ~ age + education_years, data=d); summary(med_dpr__outcome); # mediation::mediate cannot do beta regression so let's use linear regression instead...
  med_dpr_age <- mediation::mediate(med_dpr__mediator, med_dpr__outcome, mediator="education_years", treat="age", outcome="dpr", sims=10000, robustSE=TRUE); summary(med_dpr_age); plot(med_dpr_age); # positive direct effect and negative indirect effect
  
  psem_dpr_age <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ age, family=poisson(), data=d),
    lm(dpr ~ age + education_years, data=d),
    data=d);
  (summary_psem_dpr_age <- summary(psem_dpr_age, .progressBar = FALSE));
  plot(psem_dpr_age, show="unstd"); # standardized not available for education_years
  psem_dpr_age_full <- piecewiseSEM::psem( # just indirect effect (aka full mediation)
    glm(education_years ~ age, family=poisson(), data=d),
    lm(dpr ~ education_years, data=d),
    data=d);
  (summary_psem_dpr_age_full <- summary(psem_dpr_age_full, .progressBar = FALSE));
  piecewiseSEM::dSep(psem_dpr_age_full, .progressBar = FALSE); # p = 0.00067 ***
  plot(psem_dpr_age_full, show="unstd"); # standardized not available for education_years
  (psem_anova_dpr_age <- anova(psem_dpr_age, psem_dpr_age_full)); # p = 6e-04 ***
  # -> there is a positive direct effect of age besides the mediated effect (older participants have less education)
  
  # location has an independent effect from education:
  med_dpr__mediator <-  glm(education_years ~ location_bin, family=poisson(), data=d); summary(med_dpr__mediator); # poisson regression as education_years is a count
  med_dpr__outcome  <- lm(dpr ~ location_bin + education_years, data=d); summary(med_dpr__outcome); # mediation::mediate cannot do beta regression so let's use linear regression instead...
  med_dpr_location <- mediation::mediate(med_dpr__mediator, med_dpr__outcome, mediator="education_years", treat="location_bin", outcome="dpr", sims=10000, robustSE=TRUE); summary(med_dpr_location); plot(med_dpr_location); # negative direct and indirect effects
  
  psem_dpr_location <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ location_bin, family=poisson(), data=d),
    lm(dpr ~ location_bin + education_years, data=d),
    data=d);
  (summary_psem_dpr_location <- summary(psem_dpr_location, .progressBar = FALSE));
  plot(psem_dpr_location, show="unstd"); # standardized not available for education_years
  psem_dpr_location_full <- piecewiseSEM::psem( # just indirect effect (aka full mediation)
    glm(education_years ~ location_bin, family=poisson(), data=d),
    lm(dpr ~ education_years, data=d),
    data=d);
  (summary_psem_dpr_location_full <- summary(psem_dpr_location_full, .progressBar = FALSE));
  piecewiseSEM::dSep(psem_dpr_location_full, .progressBar = FALSE); # p = 1.99e-07 ***
  plot(psem_dpr_location_full, show="unstd"); # standardized not available for education_years
  (psem_anova_dpr_location <- anova(psem_dpr_location, psem_dpr_location_full)); # p = 0 ***
  # -> the direct effect is needed
  
  ## working memory has a direct effect besides education, age and gender would predict:
  med_dpr__mediator <-  lm(wm_norm ~ education_years + gender_n + age, data=d); summary(med_dpr__mediator);
  med_dpr__outcome  <- lm(dpr ~ wm_norm + education_years + gender_n + age, data=d); summary(med_dpr__outcome);
  med_dpr_wm <- mediation::mediate(med_dpr__mediator, med_dpr__outcome, mediator="wm_norm", treat="education_years", outcome="dpr", sims=10000, robustSE=TRUE); summary(med_dpr_wm); plot(med_dpr_wm);
  
  # use linear regression:
  psem_dpr_wm <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    lm(wm_norm ~ education_years + gender_n + age, data=d),
    lm(dpr ~ wm_norm + education_years + gender_n + age, data=d),
    data=d);
  (summary_psem_dpr_wm <- summary(psem_dpr_wm, .progressBar = FALSE));
  plot(psem_dpr_wm, show="unstd"); # standardized not available
  
  ## More complex path model:
  psem_dpr_path <- piecewiseSEM::psem(
    glm(education_years ~ age + gender_n + location_bin, family=poisson(), data=d), # age + gender -> education
    lm(age ~ location_bin + gender_n, data=d), # location + gender -> age
    glm(gender_n ~ location_bin, family=binomial(), data=d), # location -> gender
    lm(wm_norm ~ education_years + gender_n + age, data=d), # age + gender + education -> wm
    lm(dpr ~ wm_norm + age + gender_n + education_years + location_bin, data=d), # all -> dpr
    data=d);
  (summary_psem_dpr_path <- summary(psem_dpr_path, .progressBar = FALSE));
  plot(psem_dpr_path, show="unstd"); # standardized not available
  
  # save them to file:
  tone_dpr_results <- list("icc_fam"=icc_dpr__fam, "m_gen"=summary(m_dpr__gen), "m_location"=summary(m_dpr__location), # checks
                           "multreg"=m_dpr, # multiple regression model
                           "med_gender__education"=list("mediation"=summary(med_dpr_gender),
                                                     "piecewise"=list("model"=psem_dpr_gender, "dsep.p"=piecewiseSEM::dSep(psem_dpr_gender_full, .progressBar = FALSE)$P.Value, "anova"=psem_anova_dpr_gender)),
                           "med_age__education"=list("mediation"=summary(med_dpr_age),
                                                     "piecewise"=list("model"=psem_dpr_age, "dsep.p"=piecewiseSEM::dSep(psem_dpr_age_full, .progressBar = FALSE)$P.Value, "anova"=psem_anova_dpr_age)),
                           "med_location__education"=list("mediation"=summary(med_dpr_location),
                                                     "piecewise"=list("model"=psem_dpr_location, "dsep.p"=piecewiseSEM::dSep(psem_dpr_location_full, .progressBar = FALSE)$P.Value, "anova"=psem_anova_dpr_location)),
                           "med_wm__all"=list("mediation"=summary(med_dpr_wm),
                                              "piecewise"=list("model"=psem_dpr_wm, "summary"=summary_psem_dpr_wm)),
                           "path_model"=list("model"=psem_dpr_path, "summary"=summary_psem_dpr_path),
                           "data"=d);
  saveRDS(tone_dpr_results, file=xzfile("./cached_results/tone_dpr_results.rds", compression=9));
} else
{
  tone_dpr_results <- readRDS(xzfile("./cached_results/tone_dpr_results.rds"));
}
```

We focus here on the *d'* estimate (*dpr* = *d* *p*rime *r*ecoded).
Its distribution seems relatively normal and diagnostics (not shown) suggest that, indeed, a linear model fits this variable quite well.
However, `piecewiseSEM` has troubles estimating (some of) the standardized path coefficients, so we report here the *un*standardized ones.

#### Location and family

There is a highly significant difference between the two main locations (A has higher overall performance than B; `r sprintf("*β~B-A~*=%.2g, *p*=%s", tone_dpr_results$m_location$coefficients[2,"Estimate"], scinot(tone_dpr_results$m_location$coefficients[2,"Pr(>|t|)"]))`).
For those `r sum(!is.na(d_all$generation))` participants with information about family relationships, the generation they belong to has no effect (`r sprintf("*β~old-young~*=%.2g, *p*=%s", tone_dpr_results$m_gen$coefficients[2,"Estimate"], scinot(tone_dpr_results$m_gen$coefficients[2,"Pr(>|t|)"]))`), and, moreover, there is no clustering within families (the beta model with family as a random effect has an ICC of `r round(100*tone_dpr_results$icc_fam$ICC_adjusted,1)`%), suggesting that we need not model these factors here.

#### Multiple regression

A beta multiple linear regression of the *working memory task performance* on *age*, *gender*, *years of education* and *location* and all their interactions simplifies (using manual simplification based on F-test's *p* value) to a model with main effects and one interaction only:
```{r}
summary(tone_dpr_results$multreg);
```
```{r fig.width=4, fig.height=4, fig.cap=capFig("Untransformed slopes with standard errors.")}
sjPlot::plot_model(tone_dpr_results$multreg, show.values=TRUE, value.offset=.2, vline.color="gray20", transform=NULL);
```
```{r fig.width=4, fig.height=4, fig.cap=capFig("Predicted values of dpr showing the interaction of *education_years* and *gender*.")}
sjPlot::plot_model(tone_dpr_results$multreg, type="pred", terms=c("education_years", "gender")) + ylim(0,6);
```
```{r fig.width=4, fig.height=4, fig.cap=capFig("Predicted values of dpr showing the interaction of *education_years* and *age*.")}
sjPlot::plot_model(tone_dpr_results$multreg, type="pred", terms=c("education_years", "age")) + ylim(0,6);
```
suggesting that *age* and *years of education* have no main effects, that *working memory* has a main positive effect, that males have better performance than the females but there is an interaction between education and age, and between education and gender, and that participants from A have better performance than those from B.
As above, we test more complex of causality using mediation and path analysis.

#### Mediation: gender → education → tone

The mediation model where *gender* influences *tone* through *years of education* finds a highly significant positive indirect effect (ACME) and no direct effect (ADE); as fitted using `mediation::mediate()` (with linear regression):
```{r}
tone_dpr_results$med_gender__education$mediation;
```
and as fitted using `piecewiseSEM::psem()`:
```{r}
summary(tone_dpr_results$med_gender__education$piecewise$model);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model of *gender*, *years of education* and *dpr* showing the *un*standardized coefficients (fitted using `piecewiseSEM::psem()`); dotted arrows are not significant.")}
plot(tone_dpr_results$med_gender__education$piecewise$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=8, width=1), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```
Moreover, testing this partial mediation model against the full mediation model using d-separation and model comparison finds that there is no direct effect of *gender*, but that it is entirely mediated through *years of education* (`r sprintf("d-sep *p*=%s, model comparison *χ*^2^(%d)=%.2g, *p*=%s", scinot(tone_dpr_results$med_gender__education$piecewise$dsep.p), as.numeric(tone_dpr_results$med_gender__education$piecewise$anova[[1]][2,"Df.diff"]), as.numeric(tone_dpr_results$med_gender__education$piecewise$anova[[1]][2,"Chisq.diff"]), scinot(as.numeric(tone_dpr_results$med_gender__education$piecewise$anova[[1]][2,"P.value"])))`).

#### Mediation: age → education → tone

Likewise, fitting a mediation model where *age* influences *tone* through *years of education* finds a significant positive direct (ADE) and a significant negative mediated effect (ACME); as fitted using `mediation::mediate()` (with linear regression):
```{r}
tone_dpr_results$med_age__education$mediation;
```
and as fitted using `piecewiseSEM::psem()`:
```{r}
summary(tone_dpr_results$med_age__education$piecewise$model);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model of *age*, *years of education* and *dpr* showing the *un*standardized coefficients (fitted using `piecewiseSEM::psem()`).")}
plot(tone_dpr_results$med_age__education$piecewise$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=8, width=1), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```
Moreover, the effect of *age* is split between the direct positive effect and the mediated (negative effect on *years of education*) effect (`r sprintf("d-sep *p*=%s, model comparison *χ*^2^(%d)=%.1f, *p*=%s", scinot(tone_dpr_results$med_age__education$piecewise$dsep.p), as.numeric(tone_dpr_results$med_age__education$piecewise$anova[[1]][2,"Df.diff"]), as.numeric(tone_dpr_results$med_age__education$piecewise$anova[[1]][2,"Chisq.diff"]), tone_dpr_results$med_age__education$piecewise$anova[[1]][2,"P.value"])`).

#### Mediation: location → education → tone

Likewise, fitting a mediation model where *location* influences *tone* through *years of education* finds significant negative (i.e., that Xinming has worse performance than A) direct (ADE) and mediated (ACME) effects; as fitted using `mediation::mediate()` (with linear regression):
```{r}
tone_dpr_results$med_location__education$mediation;
```
and as fitted using `piecewiseSEM::psem()`:
```{r}
summary(tone_dpr_results$med_location__education$piecewise$model);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model of *age*, *years of education* and *dpr* showing the *un*standardized coefficients (fitted using `piecewiseSEM::psem()`).")}
plot(tone_dpr_results$med_location__education$piecewise$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=8, width=1), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```
Moreover, the effect of *location* is split between the direct and the mediated effects (`r sprintf("d-sep *p*=%s, model comparison *χ*^2^(%d)=%.1f, *p*=%s", scinot(tone_dpr_results$med_location__education$piecewise$dsep.p), as.numeric(tone_dpr_results$med_location__education$piecewise$anova[[1]][2,"Df.diff"]), as.numeric(tone_dpr_results$med_location__education$piecewise$anova[[1]][2,"Chisq.diff"]), tone_dpr_results$med_location__education$piecewise$anova[[1]][2,"P.value"])`).

#### Mediation: education, age, gender → working memory → tone

Here we want to check if there is any effect of working memory on tone above and beyond that of education, age and gender, and we found both significant positive direct (ADE) and mediated (ACME) effects; as fitted using `mediation::mediate()` (with linear regression):
```{r}
tone_dpr_results$med_wm__all$mediation;
```
and as fitted using `piecewiseSEM::psem()`:
```{r}
tone_dpr_results$med_wm__all$piecewise$summary;
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model of *education*, *age*, *gender*, *wm* and *dpr* showing the *un*standardized coefficients (fitted using `piecewiseSEM::psem()`).")}
plot(tone_dpr_results$med_wm__all$piecewise$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=6, width=0.65), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```
It can be seen that, indeed, *working memory* has a positive effect on *tone* while also mediating some of the effects of *age* and *education*.

#### Complex path model

Finally, we fitted a complex path model involving all these factors using `piecewiseSEM::psem()`:
```{r}
tone_dpr_results$path_model$summary;
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("The complex path model for *dpr* showing the *un*standardized coefficients (fitted using `piecewiseSEM::psem()`).")}
plot(tone_dpr_results$path_model$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=6, width=0.65), edge_attrs = data.frame(style = "solid", color = "gray80", fontsize=6),digits=2);
```
This confirms and extends the previous simpler mediation models in that *dpr* is directly influenced by *working memory* (positively), *years of education* (more is better), *age* (older is better) and *location* (A is better), but also indirectly by *gender* (mediated through *years of education* and *age*); both *age* and *location* also have effects mediated through *years of education*, and *age* and *years of education* have effects mediated through *working memory*.

#### Conclusions

As expected, *d'* and *% correct total responses in the recoded dataset* behave very similarly, down to very similar coefficient estimates and associated *p*-values.
We compared (using AIC) the two measures (a rather "apples to pears" comparison in this case) using the multiple regression model `r sprintf("ΔAIC(%%cr, d') = %.1f", AIC(tone_pcr_results$multreg) - AIC(tone_dpr_results$multreg))`, and the complex path model  `r sprintf("ΔAIC(%%cr, d') = %.1f", AIC(tone_pcr_results$path_model$linearreg$model)$AIC - AIC(tone_dpr_results$path_model$model)$AIC)`, both suggesting that %cr fits the data much better than d'.


### Secondary measures

Here we look at secondary measures.

#### % correct total (original)

```{r results='hide'}
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/tone_pco_results.rds") ) # computationally expensive
{
  ## Prepare for beta regression:
  d_all$pco <- prepare_for_beta_regression(d_all$pc_tot_orig);
  
  ## Family and location:
  # Does the family structure matter?
  m_pco__fam <- glmmTMB(pco ~ 1 + (1 | family), family=beta_family(), control=beta_ctrl, data=d_all); summary(m_pco__fam); DHARMa::testDispersion(m_pco__fam, plot=FALSE);
  (icc_pco__fam <- performance::icc(m_pco__fam)); # -0.19/100 !!! (but with warning "Results are not reliable")
  # so let's try a linear model:
  m_pco__fam <- lmer(pco ~ 1 + (1 | family), data=d_all); summary(m_pco__fam); DHARMa::testDispersion(m_pco__fam, plot=FALSE);
  (icc_pco__fam <- performance::icc(m_pco__fam)); # 9.8%
  # and also a beta regression but with family as fixed effect:
  m_pco__fam <- glmmTMB(pco ~ family, family=beta_family(), control=beta_ctrl, data=d_all[ !is.na(d_all$family), ]); summary(m_pco__fam); DHARMa::testDispersion(m_pco__fam, plot=FALSE);
  (anova_pco__fam <- anova(m_pco__fam, update(m_pco__fam, . ~ . -family, data=d_all[ !is.na(d_all$family), ]))); # p=0.2951
  # -> no efect really
  
  # ... but does the generation matter?
  m_pco__gen <- glmmTMB(pco ~ generation, family=beta_family(), control=beta_ctrl, data=d_all); summary(m_pco__gen); # beta=0.069, p=0.62
  # -> nope
  # -> so, we will not model family and generation...
  
  # Does location matter (it has missing data so important to know if we include it or not)?
  m_pco__location <- glmmTMB(pco ~ location_ab, family=beta_family(), control=beta_ctrl, data=d_all); summary(m_pco__location); # beta=-0.30, p=2.73e-07 ***
  # -> YES (A is better)
  
  # Keep only the relevant non-missing data:
  d <- na.omit(d_all[,c("pco", "age", "gender", "gender_n", "education_years", "location_ab", "location_bin", "wm_norm")]);
  
  ## Multiple regression:
  # Manual simplification starting with the full model:
  m_pco_full <- glmmTMB(pco ~ age * gender * education_years * location_ab * wm_norm, family=beta_family(), control=beta_ctrl, data=d); summary(m_pco_full); # converge issues...
  m_pco <- update(m_pco_full, . ~ . - 
                    age:gender:education_years:location_ab:wm_norm - # 5-way interaction
                    age:education_years:location_ab:wm_norm - gender:education_years:location_ab:wm_norm - age:gender:education_years:wm_norm - age:gender:location_mxXinmin:wm_norm - 
                    age:gender:location_ab:wm_norm - age:gender:education_years:location_ab - # 4-way interactions
                    gender:location_ab:wm_norm - age:location_ab:wm_norm - education_years:location_mxn:wm_norm - education_years:location_ab:wm_norm - age:gender:location_ab - age:gender:wm_norm - 
                    gender:education_years:location_ab - gender:education_years:wm_norm - age:gender:education_years - age:education_years:wm_norm - age:education_years:location_ab # 3-way interactions
  ); summary(m_pco); (anova_m_pco = anova(m_pco, m_pco_full));
  # let's swtich the baseline model to this one (i.a, test the 2-way interactions):
  m_pco_full <- m_pco;
  m_pco <- update(m_pco_full, . ~ . - 
                    location_ab:wm_norm - gender:wm_norm - age:wm_norm - gender:location_ab - age:gender - age:location_ab - age:education_years - education_years:location_ab # 2-way interactions
  ); summary(m_pco); (anova_m_pco = anova(m_pco, m_pco_full));
  sjPlot::plot_model(m_pco, transform=NULL); 
  sjPlot::plot_model(m_pco, type="pred", terms=c("education_years", "gender"));
  sjPlot::plot_model(m_pco, type="pred", terms=c("education_years", "wm_norm"));

  
  ## Mediation models:
  
  ## gender is a proxy for education:
  med_pco__mediator <-  glm(education_years ~ gender_n, family=poisson(), data=d); summary(med_pco__mediator); # poisson regression as education_years is a count
  med_pco__outcome  <- lm(pco ~ gender_n + education_years, data=d); summary(med_pco__outcome); # mediation::mediate cannot do beta regression so let's use linear regression instead...
  summary(glmmTMB(pco ~ gender_n + education_years, family=beta_family(), control=beta_ctrl, data=d)); # ... and the estimates and p-values are very similar between the two
  med_pco_gender <- mediation::mediate(med_pco__mediator, med_pco__outcome, mediator="education_years", treat="gender_n", outcome="pco", sims=10000, robustSE=TRUE); summary(med_pco_gender); plot(med_pco_gender); # no direct effect
  
  # use beta regression:
  psem_pco_gender_beta <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ gender_n, family=poisson(), data=d),
    glmmTMB(pco ~ gender_n + education_years, family=beta_family(), control=beta_ctrl, data=d),
    data=d);
  summary_psem_pco_gender_beta <- NULL; #(summary_psem_pco_gender_beta <- summary(psem_pco_gender_beta, .progressBar = FALSE));
  plot(psem_pco_gender_beta, show="unstd"); # standardized not available
  # the full mediation model does not work...
  
  # use linear regression (very similar estimates and p-values):
  psem_pco_gender <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ gender_n, family=poisson(), data=d),
    lm(pco ~ gender_n + education_years, data=d),
    data=d);
  (summary_psem_pco_gender <- summary(psem_pco_gender, .progressBar = FALSE));
  plot(psem_pco_gender, show="unstd"); # standardized not available for education_years
  psem_pco_gender_full <- piecewiseSEM::psem( # just indirect effect (aka full mediation)
    glm(education_years ~ gender_n, family=poisson(), data=d),
    lm(pco ~ education_years, data=d),
    data=d);
  (summary_psem_pco_gender_full <- summary(psem_pco_gender_full, .progressBar = FALSE));
  piecewiseSEM::dSep(psem_pco_gender_full, .progressBar = FALSE); # p = 0.85
  plot(psem_pco_gender_full, show="unstd"); # standardized not available for education_years
  (psem_anova_pco_gender <- anova(psem_pco_gender, psem_pco_gender_full)); # p = 0.85
  # -> no direct effect of gender, but all is mediated through education
  
  
  ## age has an independent effect from education:
  med_pco__mediator <-  glm(education_years ~ age, family=poisson(), data=d); summary(med_pco__mediator); # poisson regression as education_years is a count
  med_pco__outcome  <- lm(pco ~ age + education_years, data=d); summary(med_pco__outcome); # mediation::mediate cannot do beta regression so let's use linear regression instead...
  summary(glmmTMB(pco ~ age + education_years, family=beta_family(), control=beta_ctrl, data=d)); # ... and the estimates and p-values are relatively (but not very) similar between the two
  med_pco_age <- mediation::mediate(med_pco__mediator, med_pco__outcome, mediator="education_years", treat="age", outcome="pco", sims=10000, robustSE=TRUE); summary(med_pco_age); plot(med_pco_age); # positive direct effect and negative indirect effect
  
  # use beta regression:
  psem_pco_age_beta <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ age, family=poisson(), data=d),
    glmmTMB(pco ~ age + education_years, family=beta_family(), control=beta_ctrl, data=d),
    data=d);
  summary_psem_pco_age_beta <- NULL; #(summary_psem_pco_age_beta <- summary(psem_pco_age_beta, .progressBar = FALSE));
  plot(psem_pco_age_beta, show="unstd"); # standardized not available
  # the full mediation model does not work...
  
  # use linear regression (relatively similar estimates and p-values):
  psem_pco_age <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ age, family=poisson(), data=d),
    lm(pco ~ age + education_years, data=d),
    data=d);
  (summary_psem_pco_age <- summary(psem_pco_age, .progressBar = FALSE));
  plot(psem_pco_age, show="unstd"); # standardized not available for education_years
  psem_pco_age_full <- piecewiseSEM::psem( # just indirect effect (aka full mediation)
    glm(education_years ~ age, family=poisson(), data=d),
    lm(pco ~ education_years, data=d),
    data=d);
  (summary_psem_pco_age_full <- summary(psem_pco_age_full, .progressBar = FALSE));
  piecewiseSEM::dSep(psem_pco_age_full, .progressBar = FALSE); # p = 0.0029 **
  plot(psem_pco_age_full, show="unstd"); # standardized not available for education_years
  (psem_anova_pco_age <- anova(psem_pco_age, psem_pco_age_full)); # p = 0.0028 **
  # -> there is a positive direct effect of age besides the mediated effect (older participants have less education)
  
  
  ## location has an independent effect from education:
  med_pco__mediator <-  glm(education_years ~ location_bin, family=poisson(), data=d); summary(med_pco__mediator); # poisson regression as education_years is a count
  med_pco__outcome  <- lm(pco ~ location_bin + education_years, data=d); summary(med_pco__outcome); # mediation::mediate cannot do beta regression so let's use linear regression instead...
  summary(glmmTMB(pco ~ location_bin + education_years, family=beta_family(), control=beta_ctrl, data=d)); # ... and the estimates and p-values are relatively (but not very) similar between the two
  med_pco_location <- mediation::mediate(med_pco__mediator, med_pco__outcome, mediator="education_years", treat="location_bin", outcome="pco", sims=10000, robustSE=TRUE); summary(med_pco_location); plot(med_pco_location); # negative direct and indirect effects
  
  # use beta regression:
  psem_pco_location_beta <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ location_bin, family=poisson(), data=d),
    glmmTMB(pco ~ location_bin + education_years, family=beta_family(), control=beta_ctrl, data=d),
    data=d);
  summary_psem_pco_location_beta <- NULL; #(summary_psem_pco_location_beta <- summary(psem_pco_location_beta, .progressBar = FALSE));
  plot(psem_pco_location_beta, show="unstd"); # standardized not available
  # the full mediation model does not work...
  
  # use linear regression (relatively similar estimates and p-values):
  psem_pco_location <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ location_bin, family=poisson(), data=d),
    lm(pco ~ location_bin + education_years, data=d),
    data=d);
  (summary_psem_pco_location <- summary(psem_pco_location, .progressBar = FALSE));
  plot(psem_pco_location, show="unstd"); # standardized not available for education_years
  psem_pco_location_full <- piecewiseSEM::psem( # just indirect effect (aka full mediation)
    glm(education_years ~ location_bin, family=poisson(), data=d),
    lm(pco ~ education_years, data=d),
    data=d);
  (summary_psem_pco_location_full <- summary(psem_pco_location_full, .progressBar = FALSE));
  piecewiseSEM::dSep(psem_pco_location_full, .progressBar = FALSE); # p = 0.00016 ***
  plot(psem_pco_location_full, show="unstd"); # standardized not available for education_years
  (psem_anova_pco_location <- anova(psem_pco_location, psem_pco_location_full)); # p = 2e-04 ***
  # -> the direct effect is needed
  
  ## working memory has a direct effect besides education, age and gender would predict:
  med_pco__mediator <-  lm(wm_norm ~ education_years + gender_n + age, data=d); summary(med_pco__mediator);
  med_pco__outcome  <- lm(pco ~ wm_norm + education_years + gender_n + age, data=d); summary(med_pco__outcome); # mediation::mediate cannot do beta regression so let's use linear regression instead...
  summary(glmmTMB(pco ~ wm_norm + education_years + gender_n + age, family=beta_family(), control=beta_ctrl, data=d)); # ... and the estimates and p-values are very similar between the two
  med_pco_wm <- mediation::mediate(med_pco__mediator, med_pco__outcome, mediator="wm_norm", treat="education_years", outcome="pco", sims=10000, robustSE=TRUE); summary(med_pco_wm); plot(med_pco_wm);
  
  # use beta regression:
  psem_pco_wm_beta <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    lm(wm_norm ~ education_years + gender_n + age, data=d),
    glmmTMB(pco ~ wm_norm + education_years + gender_n + age, family=beta_family(), control=beta_ctrl, data=d),
    data=d);
  summary_psem_pco_wm_beta <- NULL; #(summary_psem_pco_wm_beta <- summary(psem_pco_wm_beta, .progressBar = FALSE));
  plot(psem_pco_wm_beta, show="unstd"); # standardized not available
  
  # use linear regression:
  psem_pco_wm <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    lm(wm_norm ~ education_years + gender_n + age, data=d),
    lm(pco ~ wm_norm + education_years + gender_n + age, data=d),
    data=d);
  (summary_psem_pco_wm <- summary(psem_pco_wm, .progressBar = FALSE));
  plot(psem_pco_wm, show="unstd"); # standardized not available

  
  ## More complex path model:
  psem_pco_path_beta <- piecewiseSEM::psem(
    glm(education_years ~ age + gender_n + location_bin, family=poisson(), data=d), # age + gender -> education
    lm(age ~ location_bin + gender_n, data=d), # location + gender -> age
    glm(gender_n ~ location_bin, family=binomial(), data=d), # location -> gender
    lm(wm_norm ~ education_years + gender_n + age, data=d), # age + gender + education -> wm
    glmmTMB(pco ~ wm_norm + age + gender_n + education_years + location_bin, family=beta_family(), control=beta_ctrl, data=d), # all -> pco
    data=d);
  summary_psem_pco_path_beta <- NULL; #(summary_psem_pco_path_beta <- summary(psem_pco_path_beta, .progressBar = FALSE));
  plot(psem_pco_path_beta, show="unstd"); # standardized not available

  # use linear regression:
  psem_pco_path <- piecewiseSEM::psem(
    glm(education_years ~ age + gender_n + location_bin, family=poisson(), data=d), # age + gender -> education
    lm(age ~ location_bin + gender_n, data=d), # location + gender -> age
    glm(gender_n ~ location_bin, family=binomial(), data=d), # location -> gender
    lm(wm_norm ~ education_years + gender_n + age, data=d), # age + gender + education -> wm
    lm(pco ~ wm_norm + age + gender_n + education_years + location_bin, data=d), # all -> pco
    data=d);
  (summary_psem_pco_path <- summary(psem_pco_path, .progressBar = FALSE));
  plot(psem_pco_path, show="unstd"); # standardized not available

  
  # save them to file:
  tone_pco_results <- list("icc_fam"=icc_pco__fam ,"anova_fam"=anova_pco__fam, "m_gen"=summary(m_pco__gen), "m_location"=summary(m_pco__location), # checks
                           "multreg"=m_pco, # multiple regression model
                           "med_gender__education"=list("mediation"=summary(med_pco_gender),
                                                     "piecewise"=list("betareg"=list("model"=psem_pco_gender_beta, "summary"=summary_psem_pco_gender_beta),
                                                                      "linearreg"=list("model"=psem_pco_gender, "dsep.p"=piecewiseSEM::dSep(psem_pco_gender_full, .progressBar = FALSE)$P.Value, "anova"=psem_anova_pco_gender))),
                           "med_age__education"=list("mediation"=summary(med_pco_age),
                                                     "piecewise"=list("betareg"=list("model"=psem_pco_age_beta, "summary"=summary_psem_pco_age_beta),
                                                                      "linearreg"=list("model"=psem_pco_age, "dsep.p"=piecewiseSEM::dSep(psem_pco_age_full, .progressBar = FALSE)$P.Value, "anova"=psem_anova_pco_age))),
                           "med_location__education"=list("mediation"=summary(med_pco_location),
                                                     "piecewise"=list("betareg"=list("model"=psem_pco_location_beta, "summary"=summary_psem_pco_location_beta),
                                                                      "linearreg"=list("model"=psem_pco_location, "dsep.p"=piecewiseSEM::dSep(psem_pco_location_full, .progressBar = FALSE)$P.Value, "anova"=psem_anova_pco_location))),
                           "med_wm__all"=list("mediation"=summary(med_pco_wm),
                                              "piecewise"=list("betareg"=list("model"=psem_pco_wm_beta, "summary"=summary_psem_pco_wm_beta),
                                                               "linearreg"=list("model"=psem_pco_wm))),
                           "path_model"=list("betareg"=list("model"=psem_pco_path_beta, "summary"=summary_psem_pco_path_beta),
                                             "linearreg"=list("model"=psem_pco_path, "summary"=summary_psem_pco_path)),
                           "data"=d);
  saveRDS(tone_pco_results, file=xzfile("./cached_results/tone_pco_results.rds", compression=9));
} else
{
  tone_pco_results <- readRDS(xzfile("./cached_results/tone_pco_results.rds"));
}
```

We focus here on the *% total correct responses on the 'original' dataset* estimate (*pco* = *p*ercent *c*orrect *o*riginal) -- see the comments above for pcr.

##### Location and family

There is a highly significant difference between the two main locations (A has higher overall performance than B; beta regression `r sprintf("*β~B-A~*=%.2g, *p*=%s", tone_pco_results$m_location$coefficients$cond[2,"Estimate"], scinot(tone_pco_results$m_location$coefficients$cond[2,"Pr(>|z|)"]))`).
For those `r sum(!is.na(d_all$generation))` participants with information about family relationships, the generation they belong to has no effect (beta regression `r sprintf("*β~old-young~*=%.2g, *p*=%s", tone_pco_results$m_gen$coefficients$cond[2,"Estimate"], scinot(tone_pco_results$m_gen$coefficients$cond[2,"Pr(>|z|)"]))`) and there is no clustering within families (the linear model with family as a random effect has an ICC of `r round(100*tone_pco_results$icc_fam$ICC_adjusted,1)`%, and including family as a fixed effect in a "flat" beta regression vs excluding it results in a *p*=`r scinot(tone_pco_results$anova_fam[2,"Pr(>Chisq)"])`).

##### Multiple regression

```{r}
summary(tone_pco_results$multreg);
```
```{r fig.width=4, fig.height=4, fig.cap=capFig("Untransformed slopes with standard errors.")}
sjPlot::plot_model(tone_pco_results$multreg, show.values=TRUE, value.offset=.2, vline.color="gray20", transform=NULL);
```
```{r fig.width=4, fig.height=4, fig.cap=capFig("Predicted values of pco showing the interaction of*gender* and *age*.")}
sjPlot::plot_model(tone_pco_results$multreg, type="pred", terms=c("age", "gender")) + ylim(0,1);
```
```{r fig.width=4, fig.height=4, fig.cap=capFig("Predicted values of pco showing the interaction of *education_years* and *age*.")}
sjPlot::plot_model(tone_pco_results$multreg, type="pred", terms=c("education_years", "age")) + ylim(0,1);
```

##### Mediation: gender → education → tone

```{r}
tone_pco_results$med_gender__education$mediation;
summary(tone_pco_results$med_gender__education$piecewise$linearreg$model);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model.")}
plot(tone_pco_results$med_gender__education$piecewise$linearreg$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=8, width=1), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```
`r sprintf("d-sep *p*=%s, model comparison *χ*^2^(%d)=%.2g, *p*=%s", scinot(tone_pco_results$med_gender__education$piecewise$linearreg$dsep.p), as.numeric(tone_pco_results$med_gender__education$piecewise$linearreg$anova[[1]][2,"Df.diff"]), as.numeric(tone_pco_results$med_gender__education$piecewise$linearreg$anova[[1]][2,"Chisq.diff"]), scinot(as.numeric(tone_pco_results$med_gender__education$piecewise$linearreg$anova[[1]][2,"P.value"])))`

##### Mediation: age → education → tone

```{r}
tone_pco_results$med_age__education$mediation;
summary(tone_pco_results$med_age__education$piecewise$linearreg$model);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model.")}
plot(tone_pco_results$med_age__education$piecewise$linearreg$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=8, width=1), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```
`r sprintf("d-sep *p*=%s, model comparison *χ*^2^(%d)=%.1f, *p*=%s", scinot(tone_pco_results$med_age__education$piecewise$linearreg$dsep.p), as.numeric(tone_pco_results$med_age__education$piecewise$linearreg$anova[[1]][2,"Df.diff"]), as.numeric(tone_pco_results$med_age__education$piecewise$linearreg$anova[[1]][2,"Chisq.diff"]), tone_pco_results$med_age__education$piecewise$linearreg$anova[[1]][2,"P.value"])`.

##### Mediation: location → education → tone

```{r}
tone_pco_results$med_location__education$mediation;
summary(tone_pco_results$med_location__education$piecewise$linearreg$model);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model.")}
plot(tone_pco_results$med_location__education$piecewise$linearreg$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=8, width=1), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```
`r sprintf("d-sep *p*=%s, model comparison *χ*^2^(%d)=%.1f, *p*=%s", scinot(tone_pco_results$med_location__education$piecewise$linearreg$dsep.p), as.numeric(tone_pco_results$med_location__education$piecewise$linearreg$anova[[1]][2,"Df.diff"]), as.numeric(tone_pco_results$med_location__education$piecewise$linearreg$anova[[1]][2,"Chisq.diff"]), tone_pco_results$med_location__education$piecewise$linearreg$anova[[1]][2,"P.value"])`

##### Mediation: education, age, gender → working memory → tone

```{r}
tone_pco_results$med_wm__all$mediation;
summary(tone_pco_results$med_wm__all$piecewise$linearreg$model);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model.")}
plot(tone_pco_results$med_wm__all$piecewise$linearreg$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=6, width=0.65), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```

##### Complex path model

```{r}
summary(tone_pco_results$path_model$linearreg$model, .progressBar = FALSE);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("The complex path model.")}
plot(tone_pco_results$path_model$linearreg$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=6, width=0.65), edge_attrs = data.frame(style = "solid", color = "gray80", fontsize=6),digits=2);
```

##### Conclusions

*pco* behaves relatively similarly to *pcr*, and we compared (using AIC) the two (more meaningful here) using the multiple regression model `r sprintf("ΔAIC(pcr, pco) = %.1f", AIC(tone_pcr_results$multreg) - AIC(tone_pco_results$multreg))`, and the complex path model  `r sprintf("ΔAIC(pcr, pco) = %.1f", AIC(tone_pcr_results$path_model$linearreg$model)$AIC - AIC(tone_pco_results$path_model$linearreg$model)$AIC)`, both suggesting that pcr (the 'reduced' dataset) fits the data better than pco (the 'original' dataset).


#### d' (original)

```{r results='hide'}
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/tone_dpo_results.rds") ) # computationally expensive
{
  ## The distribution of d' seems vaguely normal, so let's go for a linear model
  d_all$dpo <- d_all$dprime_orig;
  
  ## Family and location:
  # Does the family structure matter?
  m_dpo__fam <- lmer(dpo ~ 1 + (1 | family), data=d_all); summary(m_dpo__fam);
  (icc_dpo__fam <- performance::icc(m_dpo__fam)); # 8.6%
  # -> nope

  # ... but does the generation matter?
  m_dpo__gen <- lm(dpo ~ generation, data=d_all); summary(m_dpo__gen); # beta=0.15, p=0.47
  # -> nope
  # -> so, family and generation do not seem to matter (at our statistical power)...
  
  # Does location matter (it has missing data so important to know if we include it or not)?
  m_dpo__location <- lm(dpo ~ location_ab, data=d_all); summary(m_dpo__location); # beta=-0.49, p=1.83e-08 ***
  # -> YES (A is better)

  # Keep only the relevant non-missing data:
  d <- na.omit(d_all[,c("dpo", "age", "gender", "gender_n", "education_years", "location_ab", "location_bin", "wm_norm")]);
  
  ## Multiple regression:
  # Manual simplification starting with the full model:
  m_dpo_full <- lm(dpo ~ age * gender * education_years * location_ab * wm_norm, data=d); summary(m_dpo_full); par(mfrow=c(2,2)); plot(m_dpo_full); par(mfrow=c(1,1)); # diagnostics are decent so indeed linear is fine
  m_dpo <- update(m_dpo_full, . ~ . - 
                    age:gender:education_years:location_ab:wm_norm - # 5-way interaction
                    age:gender:education_years:wm_norm - age:gender:location_mxXinmin:wm_norm - gender:education_years:location_ab:wm_norm - age:gender:location_ab:wm_norm - 
                    age:education_years:location_ab:wm_norm - age:gender:education_years:location_ab - # 4-way interactions
                    genderM:location_ab:wm_norm - gender:location_ab:wm_norm - age:education_years:wm_norm - age:gender:wm_norm - education_years:location_ab:wm_norm - age:gender:location_ab - 
                    gender:education_years:location_ab - gender:education_years:wm_norm - age:gender:education_years - age:location_ab:wm_norm - age:education_years:location_ab - # 3-way interactions
                    age:location_ab - age:wm_norm - location_ab:wm_norm - gender:wm_norm - age:gender - gender:location_ab - education_years:wm_norm - education_years:location_ab # 2-way interactions
  ); summary(m_dpo); (anova_m_dpo = anova(m_dpo, m_dpo_full));
  sjPlot::plot_model(m_dpo, transform=NULL); 
  sjPlot::plot_model(m_dpo, type="pred", terms=c("education_years", "gender")); 
  sjPlot::plot_model(m_dpo, type="pred", terms=c("education_years", "age"));

  
  ## Mediation models:
  # gender is a proxy for education:
  med_dpo__mediator <-  glm(education_years ~ gender_n, family=poisson(), data=d); summary(med_dpo__mediator); # poisson regression as education_years is a count
  med_dpo__outcome  <- lm(dpo ~ gender_n + education_years, data=d); summary(med_dpo__outcome);
  med_dpo_gender <- mediation::mediate(med_dpo__mediator, med_dpo__outcome, mediator="education_years", treat="gender_n", outcome="dpo", sims=10000, robustSE=TRUE); summary(med_dpo_gender); plot(med_dpo_gender); # no direct effect
  
  psem_dpo_gender <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ gender_n, family=poisson(), data=d),
    lm(dpo ~ gender_n + education_years, data=d),
    data=d);
  (summary_psem_dpo_gender <- summary(psem_dpo_gender, .progressBar = FALSE));
  plot(psem_dpo_gender, show="unstd"); # standardized not available for education_years
  psem_dpo_gender_full <- piecewiseSEM::psem( # just indirect effect (aka full mediation)
    glm(education_years ~ gender_n, family=poisson(), data=d),
    lm(dpo ~ education_years, data=d),
    data=d);
  (summary_psem_dpo_gender_full <- summary(psem_dpo_gender_full, .progressBar = FALSE));
  piecewiseSEM::dSep(psem_dpo_gender_full, .progressBar = FALSE); # p = 0.61
  plot(psem_dpo_gender_full, show="unstd"); # standardized not available for education_years
  (psem_anova_dpo_gender <- anova(psem_dpo_gender, psem_dpo_gender_full)); # p = 0.62
  # -> no direct effect of gender, but all is mediated through education
  
  # age has an independent effect from education:
  med_dpo__mediator <-  glm(education_years ~ age, family=poisson(), data=d); summary(med_dpo__mediator); # poisson regression as education_years is a count
  med_dpo__outcome  <- lm(dpo ~ age + education_years, data=d); summary(med_dpo__outcome); # mediation::mediate cannot do beta regression so let's use linear regression instead...
  med_dpo_age <- mediation::mediate(med_dpo__mediator, med_dpo__outcome, mediator="education_years", treat="age", outcome="dpo", sims=10000, robustSE=TRUE); summary(med_dpo_age); plot(med_dpo_age); # positive direct effect and negative indirect effect
  
  psem_dpo_age <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ age, family=poisson(), data=d),
    lm(dpo ~ age + education_years, data=d),
    data=d);
  (summary_psem_dpo_age <- summary(psem_dpo_age, .progressBar = FALSE));
  plot(psem_dpo_age, show="unstd"); # standardized not available for education_years
  psem_dpo_age_full <- piecewiseSEM::psem( # just indirect effect (aka full mediation)
    glm(education_years ~ age, family=poisson(), data=d),
    lm(dpo ~ education_years, data=d),
    data=d);
  (summary_psem_dpo_age_full <- summary(psem_dpo_age_full, .progressBar = FALSE));
  piecewiseSEM::dSep(psem_dpo_age_full, .progressBar = FALSE); # p = 0.00069 ***
  plot(psem_dpo_age_full, show="unstd"); # standardized not available for education_years
  (psem_anova_dpo_age <- anova(psem_dpo_age, psem_dpo_age_full)); # p = 7e-04 ***
  # -> there is a positive direct effect of age besides the mediated effect (older participants have less education)
  
  # location has an independent effect from education:
  med_dpo__mediator <-  glm(education_years ~ location_bin, family=poisson(), data=d); summary(med_dpo__mediator); # poisson regression as education_years is a count
  med_dpo__outcome  <- lm(dpo ~ location_bin + education_years, data=d); summary(med_dpo__outcome); # mediation::mediate cannot do beta regression so let's use linear regression instead...
  med_dpo_location <- mediation::mediate(med_dpo__mediator, med_dpo__outcome, mediator="education_years", treat="location_bin", outcome="dpo", sims=10000, robustSE=TRUE); summary(med_dpo_location); plot(med_dpo_location); # negative direct and indirect effects
  
  psem_dpo_location <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    glm(education_years ~ location_bin, family=poisson(), data=d),
    lm(dpo ~ location_bin + education_years, data=d),
    data=d);
  (summary_psem_dpo_location <- summary(psem_dpo_location, .progressBar = FALSE));
  plot(psem_dpo_location, show="unstd"); # standardized not available for education_years
  psem_dpo_location_full <- piecewiseSEM::psem( # just indirect effect (aka full mediation)
    glm(education_years ~ location_bin, family=poisson(), data=d),
    lm(dpo ~ education_years, data=d),
    data=d);
  (summary_psem_dpo_location_full <- summary(psem_dpo_location_full, .progressBar = FALSE));
  piecewiseSEM::dSep(psem_dpo_location_full, .progressBar = FALSE); # p = 1.63e-05 ***
  plot(psem_dpo_location_full, show="unstd"); # standardized not available for education_years
  (psem_anova_dpo_location <- anova(psem_dpo_location, psem_dpo_location_full)); # p = 0 ***
  # -> the direct effect is needed
  
  ## working memory has a direct effect besides education, age and gender would predict:
  med_dpo__mediator <-  lm(wm_norm ~ education_years + gender_n + age, data=d); summary(med_dpo__mediator);
  med_dpo__outcome  <- lm(dpo ~ wm_norm + education_years + gender_n + age, data=d); summary(med_dpo__outcome);
  med_dpo_wm <- mediation::mediate(med_dpo__mediator, med_dpo__outcome, mediator="wm_norm", treat="education_years", outcome="dpo", sims=10000, robustSE=TRUE); summary(med_dpo_wm); plot(med_dpo_wm);
  
  # use beta regression:
  psem_dpo_wm <- piecewiseSEM::psem( # with indirect effect (aka partial mediation)
    lm(wm_norm ~ education_years + gender_n + age, data=d),
    lm(dpo ~ wm_norm + education_years + gender_n + age, data=d),
    data=d);
  (summary_psem_dpo_wm <- summary(psem_dpo_wm, .progressBar = FALSE));
  plot(psem_dpo_wm, show="unstd"); # standardized not available
  
  ## More complex path model:
  psem_dpo_path <- piecewiseSEM::psem(
    glm(education_years ~ age + gender_n + location_bin, family=poisson(), data=d), # age + gender -> education
    lm(age ~ location_bin + gender_n, data=d), # location + gender -> age
    glm(gender_n ~ location_bin, family=binomial(), data=d), # location -> gender
    lm(wm_norm ~ education_years + gender_n + age, data=d), # age + gender + education -> wm
    lm(dpo ~ wm_norm + age + gender_n + education_years + location_bin, data=d), # all -> dpo
    data=d);
  (summary_psem_dpo_path <- summary(psem_dpo_path, .progressBar = FALSE));
  plot(psem_dpo_path, show="unstd"); # standardized not available
  
  # save them to file:
  tone_dpo_results <- list("icc_fam"=icc_dpo__fam, "m_gen"=summary(m_dpo__gen), "m_location"=summary(m_dpo__location), # checks
                           "multreg"=m_dpo, # multiple regression model
                           "med_gender__education"=list("mediation"=summary(med_dpo_gender),
                                                     "piecewise"=list("model"=psem_dpo_gender, "dsep.p"=piecewiseSEM::dSep(psem_dpo_gender_full, .progressBar = FALSE)$P.Value, "anova"=psem_anova_dpo_gender)),
                           "med_age__education"=list("mediation"=summary(med_dpo_age),
                                                     "piecewise"=list("model"=psem_dpo_age, "dsep.p"=piecewiseSEM::dSep(psem_dpo_age_full, .progressBar = FALSE)$P.Value, "anova"=psem_anova_dpo_age)),
                           "med_location__education"=list("mediation"=summary(med_dpo_location),
                                                     "piecewise"=list("model"=psem_dpo_location, "dsep.p"=piecewiseSEM::dSep(psem_dpo_location_full, .progressBar = FALSE)$P.Value, "anova"=psem_anova_dpo_location)),
                           "med_wm__all"=list("mediation"=summary(med_dpo_wm),
                                              "piecewise"=list("model"=psem_dpo_wm, "summary"=summary_psem_dpo_wm)),
                           "path_model"=list("model"=psem_dpo_path, "summary"=summary_psem_dpo_path),
                           "data"=d);
  saveRDS(tone_dpo_results, file=xzfile("./cached_results/tone_dpo_results.rds", compression=9));
} else
{
  tone_dpo_results <- readRDS(xzfile("./cached_results/tone_dpo_results.rds"));
}
```

We focus here on *d' on the 'original' dataset* estimate (*dpo* = *d* *p*rime *o*riginal) -- see the comments above for dpr.

##### location and family

There is a highly significant difference between the two main locations (A has higher overall performance than B; `r sprintf("*β~B-A~*=%.2g, *p*=%s", tone_dpr_results$m_location$coefficients[2,"Estimate"], scinot(tone_dpr_results$m_location$coefficients[2,"Pr(>|t|)"]))`).
For those `r sum(!is.na(d_all$generation))` participants with information about family relationships, the generation they belong to has no effect (`r sprintf("*β~old-young~*=%.2g, *p*=%s", tone_dpr_results$m_gen$coefficients[2,"Estimate"], scinot(tone_dpr_results$m_gen$coefficients[2,"Pr(>|t|)"]))`), and, moreover, there is no clustering within families (the beta model with family as a random effect has an ICC of `r round(100*tone_dpr_results$icc_fam$ICC_adjusted,1)`%), suggesting that we need not model these factors here.

##### Multiple regression

```{r}
summary(tone_dpo_results$multreg);
```
```{r fig.width=4, fig.height=4, fig.cap=capFig("Untransformed slopes with standard errors.")}
sjPlot::plot_model(tone_dpo_results$multreg, show.values=TRUE, value.offset=.2, vline.color="gray20", transform=NULL);
```
```{r fig.width=4, fig.height=4, fig.cap=capFig("Predicted values of dpo showing the interaction of*gender* and *age*.")}
sjPlot::plot_model(tone_dpo_results$multreg, type="pred", terms=c("education_years", "gender")) + ylim(0,6);
```
```{r fig.width=4, fig.height=4, fig.cap=capFig("Predicted values of dpo showing the interaction of *education_years* and *age*.")}
sjPlot::plot_model(tone_dpo_results$multreg, type="pred", terms=c("education_years", "age")) + ylim(0,6);
```

##### Mediation gender → education → tone

```{r}
tone_dpo_results$med_gender__education$mediation;
summary(tone_dpo_results$med_gender__education$piecewise$model);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model.")}
plot(tone_dpo_results$med_gender__education$piecewise$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=8, width=1), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```
`r sprintf("d-sep *p*=%s, model comparison *χ*^2^(%d)=%.2g, *p*=%s", scinot(tone_dpo_results$med_gender__education$piecewise$dsep.p), as.numeric(tone_dpo_results$med_gender__education$piecewise$anova[[1]][2,"Df.diff"]), as.numeric(tone_dpo_results$med_gender__education$piecewise$anova[[1]][2,"Chisq.diff"]), scinot(as.numeric(tone_dpo_results$med_gender__education$piecewise$anova[[1]][2,"P.value"])))`

##### Mediation age → education → tone

```{r}
tone_dpo_results$med_age__education$mediation;
summary(tone_dpo_results$med_age__education$piecewise);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model.")}
plot(tone_dpo_results$med_age__education$piecewise$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=8, width=1), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```
`r sprintf("d-sep *p*=%s, model comparison *χ*^2^(%d)=%.1f, *p*=%s", scinot(tone_dpo_results$med_age__education$piecewise$dsep.p), as.numeric(tone_dpo_results$med_age__education$piecewise$anova[[1]][2,"Df.diff"]), as.numeric(tone_dpo_results$med_age__education$piecewise$anova[[1]][2,"Chisq.diff"]), tone_dpo_results$med_age__education$piecewise$anova[[1]][2,"P.value"])`.

##### Mediation location → education → tone

```{r}
tone_dpo_results$med_location__education$mediation;
summary(tone_dpo_results$med_location__education$piecewise);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model.")}
plot(tone_dpo_results$med_location__education$piecewise$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=8, width=1), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```
`r sprintf("d-sep *p*=%s, model comparison *χ*^2^(%d)=%.1f, *p*=%s", scinot(tone_dpo_results$med_location__education$piecewise$dsep.p), as.numeric(tone_dpo_results$med_location__education$piecewise$anova[[1]][2,"Df.diff"]), as.numeric(tone_dpo_results$med_location__education$piecewise$anova[[1]][2,"Chisq.diff"]), tone_dpo_results$med_location__education$piecewise$anova[[1]][2,"P.value"])`

##### Mediation education, age, gender → working memory → tone

```{r}
tone_dpo_results$med_wm__all$mediation;
summary(tone_dpo_results$med_wm__all$piecewise);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Mediation model.")}
plot(tone_dpo_results$med_wm__all$piecewise$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=6, width=0.65), edge_attrs = data.frame(style = "solid", color = "gray80"),digits=2);
```

##### Complex path model

```{r}
tone_dpo_results$path_model$summary;
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("The complex path model.")}
plot(tone_dpo_results$path_model$model, show="unstd", node_attrs = data.frame(shape = "rectangle", color = "black", fillcolor = "white", fontsize=6, width=0.65), edge_attrs = data.frame(style = "solid", color = "gray80", fontsize=6),digits=2);
```

##### Conclusions

*dpo* behaves relatively similarly to *dpr*, and we compared (using AIC) the two (more meaningful here) using the multiple regression model `r sprintf("ΔAIC(dpr, dpo) = %.1f", AIC(tone_dpr_results$multreg) - AIC(tone_dpo_results$multreg))`, and the complex path model  `r sprintf("ΔAIC(dpr, dpo) = %.1f", AIC(tone_dpr_results$path_model$model)$AIC - AIC(tone_dpo_results$path_model$model)$AIC)`, both suggesting that dpr (the 'reduced' dataset) fits the data worse than dpo (the 'original' dataset).
We also compared *dpo* and *pco* using the multiple regression model `r sprintf("ΔAIC(%%cr, d') = %.1f", AIC(tone_pco_results$multreg) - AIC(tone_dpo_results$multreg))`, and the complex path model  `r sprintf("ΔAIC(%%cr, d') = %.1f", AIC(tone_pco_results$path_model$linearreg$model)$AIC - AIC(tone_dpo_results$path_model$model)$AIC)`, both suggesting that %cr fits the data much better than d'.

Finally, let's compare both measures on both datasets:
```{r}
kable(data.frame("measure.1"=c("pcr", 
                               "pcr",
                               "pcr",
                               "dpr",
                               "dpr",
                               "pco"),
                 "measure.2"=c("dpr", 
                               "pco",
                               "dpo",
                               "pco",
                               "dpo",
                               "dpo"),
                 "delta.AIC.multreg"=c(AIC(tone_pcr_results$multreg) - AIC(tone_dpr_results$multreg),
                                       AIC(tone_pcr_results$multreg) - AIC(tone_pco_results$multreg),
                                       AIC(tone_pcr_results$multreg) - AIC(tone_dpo_results$multreg),
                                       AIC(tone_dpr_results$multreg) - AIC(tone_pco_results$multreg),
                                       AIC(tone_dpr_results$multreg) - AIC(tone_dpo_results$multreg),
                                       AIC(tone_pco_results$multreg) - AIC(tone_dpo_results$multreg)),
                 "delta.AIC.path"   =c(AIC(tone_pcr_results$path_model$linearreg$model)$AIC - AIC(tone_dpr_results$path_model$model)$AIC,
                                       AIC(tone_pcr_results$path_model$linearreg$model)$AIC - AIC(tone_pco_results$path_model$linearreg$model)$AIC,
                                       AIC(tone_pcr_results$path_model$linearreg$model)$AIC - AIC(tone_dpo_results$path_model$model)$AIC,
                                       AIC(tone_dpr_results$path_model$model)$AIC - AIC(tone_pco_results$path_model$linearreg$model)$AIC,
                                       AIC(tone_dpr_results$path_model$model)$AIC - AIC(tone_dpo_results$path_model$model)$AIC,
                                       AIC(tone_pco_results$path_model$linearreg$model)$AIC - AIC(tone_dpo_results$path_model$model)$AIC)),
      col.names=c("Measure 1", "Measure 2", "ΔAIC(multiple regression)", "ΔAIC(complex path model)"), row.names=FALSE,
      digits=c(NA, NA, 1, 1), caption=capTab("Model comparison (using AIC) between both measures on both datasets."));
```

It can be seen that the ordering (taking into account various caveats) is: pcr > pco >> dpo > dpr, suggesting that, indeed, the % correct responses on the 'recoded' dataset might be the best choice.


#### Bias c (recoded)

```{r results='hide'}
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/tone_cbr_results.rds") ) # computationally expensive
{
  ## c is normally distributed with mean 0 and sd 1, so linear regression is fine:
  d_all$cbr <- d_all$c_recd;
  
  ## Family and location:
  # Does the family structure matter?
  m_cbr__fam <- lmer(cbr ~ 1 + (1 | family), data=d_all); summary(m_cbr__fam);
  (icc_cbr__fam <- performance::icc(m_cbr__fam)); # 18.2%
  # -> probably yes but too little data really

  # ... but does the generation matter?
  m_cbr__gen <- lm(cbr ~ generation, data=d_all); summary(m_cbr__gen); # beta=0.08, p=0.38
  # -> nope
  # -> so, family and generation do not seem to matter (at our statistical power)...
  
  # Does location matter (it has missing data so important to know if we include it or not)?
  m_cbr__location <- lm(cbr ~ location_ab, data=d_all); summary(m_cbr__location); # beta=0.062, p=0.073
  # -> nope

  # Keep only the relevant non-missing data:
  d <- na.omit(d_all[,c("cbr", "age", "gender", "gender_n", "education_years", "location_ab", "location_bin", "wm_norm")]);
  
  ## Multiple regression:
  # Manual simplification starting with the full model:
  m_cbr_full <- lm(cbr ~ age * gender * education_years * location_ab * wm_norm, data=d); summary(m_cbr_full); par(mfrow=c(2,2)); plot(m_cbr_full); par(mfrow=c(1,1)); # diagnostics are decent so indeed linear is fine
  m_cbr <- update(m_cbr_full, . ~ . - 
                    age:gender:education_years:location_ab:wm_norm - # 5-way interaction
                    age:gender:location_ab:wm_norm - age:education_years:location_ab:wm_norm - gender:education_years:location_ab:wm_norm - age:gender:education_years:wm_norm - age:gender:education_years:location_ab - # 4-way interactions
                    age:location_ab:wm_norm - age:education_years:location_ab - age:education_years:wm_norm - age:gender:wm_norm - age:gender:location_ab - gender:education_years:wm_norm - gender:location_mxXinmin:wm_norm -
                    gender:location_ab:wm_norm - education_years:location_ab:wm_norm - gender:education_years:location_ab - age:gender:education_years - # 3-way interactions
                    gender:education_years - education_years:location_ab - education_years:wm_norm - age:education_years - location_ab:wm_norm - age:gender - age:location_ab - gender:location_ab - gender:wm_norm - age:wm_norm - # 2-way interactions
                    education_years - gender - wm_norm - age - location_ab # main effects
  ); summary(m_cbr); (anova_m_cbr = anova(m_cbr, m_cbr_full));
  # -> no predictor, so it seems like c really is idiosyncratic

  # save them to file:
  tone_cbr_results <- list("icc_fam"=icc_cbr__fam, "m_gen"=summary(m_cbr__gen), "m_location"=summary(m_cbr__location), # checks
                           "multreg"=m_cbr, "anova_0"=anova_m_cbr); # multiple regression model
  saveRDS(tone_cbr_results, file=xzfile("./cached_results/tone_cbr_results.rds", compression=9));
} else
{
  tone_cbr_results <- readRDS(xzfile("./cached_results/tone_cbr_results.rds"));
}
```

The bias *c* on the 'recoded' dataset (*cbr* from *c* *b*ias *r*ecoded) shows some clustering within families (`r round(100*tone_cbr_results$icc_fam$ICC_adjusted,1)`%), but no effects of generation nor of location.
While the clustering within families is potentially interesting (and might suggest that the bias has a familial component, which may have shared environmental and/or genetic components) we cannot really model it here due to the massive loss of data (dropping from `r nrow(d_all)` to `r sum(!is.na(d_all$family))` participants).

Interestingly, including all the potential predictors (and their interactions) in a multiple linear regression results in a model that is not better than the null model with no predictor (`r sprintf("*F*(%d)=%.2g, *p*=%s", tone_cbr_results$anova_0$Df[2], tone_cbr_results$anova_0$F[2], scinot(tone_cbr_results$anova_0[2,"Pr(>F)"]))`), suggesting that the bias *c* is idiosyncratic, probably influenced by personality, genetic and/or cultural variables that we did not measure here.


#### Bias c (original)

```{r results='hide'}
if( FORCE_COMPUTE_ALL || 
    !file.exists("./cached_results/tone_cbo_results.rds") ) # computationally expensive
{
  ## c is normally distributed with mean 0 and sd 1, so linear regression is fine:
  d_all$cbo <- d_all$c_orig;
  
  ## Family and location:
  # Does the family structure matter?
  m_cbo__fam <- lmer(cbo ~ 1 + (1 | family), data=d_all); summary(m_cbo__fam);
  (icc_cbo__fam <- performance::icc(m_cbo__fam)); # 12.3%
  # -> probably yes but too little data really

  # ... but does the generation matter?
  m_cbo__gen <- lm(cbo ~ generation, data=d_all); summary(m_cbo__gen); # beta=0.074, p=0.39
  # -> nope
  # -> so, family and generation do not seem to matter (at our statistical power)...
  
  # Does location matter (it has missing data so important to know if we include it or not)?
  m_cbo__location <- lm(cbo ~ location_ab, data=d_all); summary(m_cbo__location); # beta=-0.064, p=0.06
  # -> nope

  # Keep only the relevant non-missing data:
  d <- na.omit(d_all[,c("cbo", "age", "gender", "gender_n", "education_years", "location_ab", "location_bin", "wm_norm")]);
  
  ## Multiple regression:
  # Manual simplification starting with the full model:
  m_cbo_full <- lm(cbo ~ age * gender * education_years * location_ab * wm_norm, data=d); summary(m_cbo_full); par(mfrow=c(2,2)); plot(m_cbo_full); par(mfrow=c(1,1)); # diagnostics are decent so indeed linear is fine
  m_cbo <- update(m_cbo_full, . ~ . - 
                    age:gender:education_years:location_ab:wm_norm - # 5-way interaction
                    age:gender:location_ab:wm_norm - age:education_years:location_ab:wm_norm - gender:education_years:location_ab:wm_norm - age:gender:education_years:location_ab - age:gender:education_years:wm_norm - # 4-way interactions
                    age:education_years:wm_norm - gender:education_years:wm_norm - age:gender:wm_norm - gender:location_ab:wm_norm - education_years:location_ab:wm_norm - 
                    age:education_years:location_ab - age:gender:location_ab - age:location_ab:wm_norm - age:gender:education_years - gender:education_years:location_ab - # 3-way interactions
                    location_ab:wm_norm - education_years:wm_norm - age:gender - education_years:location_ab - age:location_ab - gender:location_ab - gender:wm_norm - 
                    gender:education_years - age:wm_norm - age:education_years - # 2-way interactions
                    gender - location_ab - age - wm_norm # main effects
  ); summary(m_cbo); (anova_m_cbo = anova(m_cbo, m_cbo_full));
  # -> education_years (beta=0.021, p=1.43e-07 ***)

  # save them to file:
  tone_cbo_results <- list("icc_fam"=icc_cbo__fam, "m_gen"=summary(m_cbo__gen), "m_location"=summary(m_cbo__location), # checks
                           "multreg"=m_cbo); # multiple regression model
  saveRDS(tone_cbo_results, file=xzfile("./cached_results/tone_cbo_results.rds", compression=9));
} else
{
  tone_cbo_results <- readRDS(xzfile("./cached_results/tone_cbo_results.rds"));
}
```

The bias *c* on the 'original' dataset (*cbo* from *c* *b*ias *o*riginal) shows some clustering within families (`r round(100*tone_cbo_results$icc_fam$ICC_adjusted,1)`%), but no effects of generation nor of location.
As above, while the clustering within families is potentially interesting we cannot really model it here due to the massive loss of data.

Including all the potential predictors (and their interactions) in a multiple linear regression model results, after manual simplification, in only *years of education* having a positive significant effect (i.e., participants with more years of education have a tendency to answer 'same' which might simply say something about how they treat the "weird" items):

```{r}
summary(tone_cbo_results$multreg);
```



### Conclusions

Putting everything together, it seems that:

- on the **'recoded'** dataset (i.e., with the "weird" items recoded as 'same' items), both measures of performance, the *% total correct responses* and the bias-free *d'*, behave in very similar ways, namely:
    + there is *no family clustering* and *no effect of generation* (but there is very little data to draw solid conclusions),
    + multiple regression suggests that participants from *A* perform better across the board, as do *males* and participants with a higher *working memory*, while *age* and *years of education* have no main effects but are involved in interactions,
    + mediation analyses dissect these, showing that:
      - *gender* has no direct effect but is fully mediated by education, resulting in a positive total effect,
      - *age* has both a positive direct effect, and a negative indirect effect (mediated through education), resulting in a very weak negative (formally ns) total effect,
      - *A*'s better overall performance than that of B (total effect) is due both to a mediated effect (through education, higher by about 1.5 years) and to a strong direct effect (participants from A do the task better),
      - *working memory* has an own positive effect and also mediates those of *age* and *education*,
    + the complex path model confirms these and suggests that the performance on the tone task is simultaneously influenced by multiple factors, directly and indirectly, in particular being increased for the participants with a higher working memory, from A, more educated and/or older.

- on the **'original'** dataset, the two measure also behave similarly

- the *% total correct responses on the 'reduced' dataset* fits the data best (in terms of AIC), so, all in all, we should probably take it as our primary measure.


# Appendices

## Appendix I

Here we give some technical details, also citing the most important methodological packages we use in this paper.

***PCA:*** we used `prcomp(...)` in package `stats` to estimate the PCs (which uses the singular value decomposition method), and `fviz_eig(...)` and `fviz_pca_var(...)` in package `factoextra` [@factoextra_2020].

***EFA:*** we implemented the model fit using `cfa(...)` and the fit measures using `fitMeasures(...)` from package `lavaan` [@lavaan_2012], and we plotted the fitted models using `lavaanPlot(...)` from package `lavaanPlot` [@lavaanPlot_2021].
The preliminary EFA tests use `KMO(...)` in package `psych` [@psych_2023], `cortest.bartlett(...)` also in package `psych`, and `det(cor(...))`; the most likely number of latent factors uses several methods implemented by `fa.parallel(...)` and `nfactors(...)` in package `psych`.

***CFA:*** uses `factanal(...)` from package `stats` (with the "promax" rotation) and the resulting model was plotted using `fa.diagram(...)` in package `psych`.

***Mokken:*** we estimated the Guttman errors using `check.errors (...)`, the scalability coefficients *H* with `coefH(...)` and aisp with `aisp(...)`, all in the package `mokken` [@ark_mokken_2007].

***SDT:*** the cumulative density function (cdf) of the normal distribution was estimated using `qnorm(...)` in package `stats`; we estimated *d*', *β*, *c*, *A*' and *B*''~*D*~ using `dprime(..., adjusted=TRUE)` in package `psycho` [@psycho_2018] with the adjustment for extreme values.
Please see [here](https://bookdown.org/danbarch/psy_207_advanced_stats_I/signal-detection-theory.html) and [here](https://www.birmingham.ac.uk/Documents/college-les/psych/vision-laboratory/sdtintro.pdf) for visual, non-technical explanations of STD.

***Regression:*** we tested the clustering by *family* using mixed-effects models as implemented by `lmer(...)` in package `lme4` [@lme4_2015] with *p*-values as implemented in package `lmerTest` [@lmerTest_2017], and the intra-class correlation coefficient (ICC) was estimated using `icc(...)` in  package `performance` [@performance_2021].
Logistic regression is implemented by `glm(..., family=binomial("logit"))` or `glmer(..., family=binomial("logit"))` (as appropriate), linear regression by `lm(...)` or `lmer(...)`, Poisson regression by `glm(..., family=poisson())`, and Beta regression bu `glmmTMB(..., family=beta_family())` from package `glmmTMB` [@glmmTMB_2017].

***Mediation***: we used `mediate(...)` in package `mediation` [@mediation_2014] with 10,000 simulations and heteroskedasticity-consistent standard errors for "classic" mediation modeling, and `psem(...)` and `dSep(...)` from package `piecewiseSEM` [@lefcheck_piecewisesem_2016] for modeling the mediation through a piecewise Structural Equation Modelling approach with *d*-separation.

***Path models:*** we used the piecewise Structural Equation Modelling approach as implemented by `psem(...)` in package `piecewiseSEM` [@lefcheck_piecewisesem_2016] and `lavaan` [@lavaan_2012].




## Appendix II

Here we present some examples of responses on an AX task (such as the tone task here) where *pcr* fails to distinguish clearly different response patterns, but the SDT-derived discrimination and bias do.

Consider an AX task with an equal numbers of signal and non-signal items, #(A ≠ X) = #(A = X), and several types of participant response strategies as given in **Table 6**.
It can be seen that the first 6 strategies, some of which are clearly very different, have very similar % of correct responses (50% or very close to it), showing that relaying on this measure only might hide important inter-individual variation.
Second, it can be seen that for the three extreme strategies 1, 3 and 5, the % correct response is exactly 50% (as expected), but also that the sensitivity *d*' and bias *b* are identically 0.0 and -1.0, respectively, while the non-parametric estimates completely fail for strategies 1 and 3; however, criterion location *c* does differentiate between these three strategies in the correct direction and strength.
Slightly relaxing the extreme strategies 1 and 3 and arguably making them more realistic (by allowing a small probability of just 1% of "error") is enough to make all five measures of sensitivity and bias informative.
The 6^th^ strategy is also highly artificial but correctly diagnosed by all 5 estimates.
Finally, the last 2 strategies, while also extreme, might reflect actual participant behavior and are correctly diagnosed by the % correct responses and by the parametric estimates (but not by the non-parametric ones, whose estimation mostly fails).
Therefore, this suggests that, in general, (a) the % correct responses by itself fails to disambiguate between clearly different strategies, but that (b) the parametric estimates *d'* and *c*, when used together, do capture such differences correctly (the non-parametric estimates should work as well in non-extreme cases).

```{r}
# Simulate participants with the same d' but different beta for a task with 100 items, half different and half same:
n.items <- 2*500; # needs to be even
p.almost.const <- 0.01; # almost constant
set.seed(42); # for replicability
d.sim <- data.frame("item"              =1:n.items, # item number
                    "type"              =c(rep("same",n.items/2), rep("different",n.items/2)), # item type (half:half)
                    "resp.const.diff"   =rep("different",n.items), # always respond "different"
                    "resp.bias.diff"    =sample(c("same", "different"), size=n.items, replace=TRUE, prob=c(p.almost.const, 1-p.almost.const)), # almost always respond "different"
                    "resp.const.same"   =rep("same",n.items),      # always respond "same"
                    "resp.bias.same"    =sample(c("same", "different"), size=n.items, replace=TRUE, prob=c(1-p.almost.const, p.almost.const)), # almost always respond "same"
                    "resp.inverse"      =rep(c(rep("different",n.items/4), rep("same",n.items/4)),2), # respond exactly inverse half the time
                    "resp.random"       =sample(c("same", "different"), size=n.items, replace=TRUE, prob=c(0.5, 1-0.5)), # random response with equal probabilities
                    "resp.correct"      =c(rep("same",n.items/2), rep("different",n.items/2)), # perfectly correct
                    "resp.incorrect"    =c(rep("different",n.items/2), rep("same",n.items/2))  # perfectly incorrect
);

sdt.examples <- do.call(rbind, lapply( 3:ncol(d.sim), function(i)
  {
    n_hit  <- sum(d.sim$type == "different" & d.sim[,i] == "different"); # number of hits = 'different' items with response 'different'
    n_fa   <- sum(d.sim$type == "same"      & d.sim[,i] == "different"); # number of false alarms = 'same' items with response 'different'
    n_miss <- sum(d.sim$type == "different" & d.sim[,i] == "same"     ); # number of misses = 'different' items with response 'same'
    n_cr   <- sum(d.sim$type == "same"      & d.sim[,i] == "same"     ); # number of correct rejections = 'same' items with response 'same'
    res <- psycho::dprime(n_hit, n_fa, n_miss, n_cr, adjusted=TRUE);
    return (data.frame("pattern"=c('always respond "different"', 
                                   paste0((1-p.almost.const)*100,'% of the time respond  "different"'), 
                                   'always respond "same"',      
                                   paste0((1-p.almost.const)*100,'% of the time respond "same"'), 
                                   'respond correctly to exactly half and wrongly to the other half',
                                   'respond randomly throwing a fair coin (i.e., 50%:50% chance)',
                                   'respond corectly to all items',
                                   'respond incorectly to all items')[i-2], # pretty names 
                       "n_hit"=n_hit, "n_fa"=n_fa, "n_miss"=n_miss, "n_cr"=n_cr,
                       "pcr"=(n_hit + n_cr)/n.items,
                       "dprime"=res$dprime, "beta"=res$beta, "c"=res$c, "aprime"=res$aprime, "bppd"=ifelse(length(res$bppd)==0,NaN,res$bppd)));
  }));
kable(sdt.examples %>%
        dplyr::mutate("counts"=paste(n_hit,n_fa,n_miss,n_cr,sep=":"),
                      "pcr"=sprintf("%.1f%%",100*pcr),
                      "dprime_beta_c"=sprintf("%.2 f (%.2 f, %.2 f)", dprime, beta, c),
                      "aprime_bppd"=paste0(ifelse(!is.na(aprime),sprintf("%.2 f",aprime),"-")," (",ifelse(!is.na(bppd),sprintf("%.2 f",bppd),"-"),")")) %>%
        dplyr::select(pattern, counts, pcr, dprime_beta_c, aprime_bppd), 
      row.names=FALSE, 
      col.names=c("Response pattern", "Counts", "%", "*d*' (*β*, *c*)", "*A*' (*B*''~*D*~)"), 
      align=c("r", "r", "r", "r"),
      caption=capTab(paste0("Some examples of reponse patterns (1^st^ column) for a total of ",n.items," items equally distributed between \"signal\" (A ≠ X, ",n.items/2," items) and \"non-signal\" (A = X, ",n.items/2," items) that result in very similar % correct responses (3^rd^ column) but that may be disambiguated by SDT-derived measures of discrimination and bias: 4^th^ column gives the parametric estimates *d*', *β* and *c*, while the last column the non-parametric ones *A*' and *B*''~*D*~, showing a dash if the estimation results in not-a-number (`NaN`). Please see text for details about these estimates. The 2^nd^ column gives the counts in the order hits:false alarms:misses:correct rejections.")));
```




# Session information

```{r warning=FALSE, results='asis'}
if( require(benchmarkme) )
{
  # CPU:
  cpu_info <- benchmarkme::get_cpu();
  if( is.null(cpu_info) )
  {
    cat("**CPU:** unknown.\n\n");
  } else
  {
    if( !is.null(cpu_info$model_name) && !is.na(cpu_info$model_name) )
    {
      cat(paste0("**CPU:** ",cpu_info$model_name));
      if( !is.null(cpu_info$no_of_cores) && !is.na(cpu_info$no_of_cores) )
      {
        cat(paste0(" (",cpu_info$no_of_cores," threads)"));
      }
      cat("\n\n");
    } else
    {
      cat("**CPU:** unknown.\n\n");
    }
  }
  
  # RAM:
  ram_info <- benchmarkme::get_ram();
  if( is.null(ram_info) || is.na(ram_info) )
  {
    cat("**RAM (memory):** unknown.\n\n");
  } else
  {
    cat("**RAM (memory):** "); print(ram_info); cat("\n");
  }
} else
{
  cat("**RAM (memory):** cannot get info (try installing package 'benchmarkme').\n\n");
}
```

```{r}
pander::pander(sessionInfo());
```

```{r}
if( exists("start_time_force_compute_all") )
{
  cat(paste0("\n\n**Note: the full computation took ", round(difftime(Sys.time(), start_time_force_compute_all, units="mins"), 2), " minutes on this machine.**\n\n"))
}

```


```{r}
# Save the input files checksums:
md5_checksums <- data.frame("file"=setdiff(list.files("./input_files/", all.files=FALSE, full.names=FALSE, recursive=FALSE, include.dirs=FALSE, no.. = TRUE), "md5_checksums.csv"),
                            "md5"=NA);
md5_checksums$md5 <- tools::md5sum(paste0("./input_files/", md5_checksums$file));
if( any(is.na(md5_checksums$md5)) )
{
  warning("Could not compute the MD5 checksum for all the input files: not saving them...\n");
  unlink("./input_files/md5_checksums.txt");
} else
{
  write.csv(md5_checksums, file="./input_files/md5_checksums.csv", row.names=FALSE);
}
```


# References




